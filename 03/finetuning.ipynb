{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9782a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF home dir: D:\\.cache\\huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TRANSFORMERS_CACHE\n",
    "if 'HF_HOME' not in os.environ:\n",
    "    os.environ['HF_HOME'] = TRANSFORMERS_CACHE\n",
    "print(f\"HF home dir: {os.environ['HF_HOME']}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204e99ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba3c40043f54b49a0c1ac403962dec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c388a79a8a7a49f783425dd3324d57aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BERT (Masked LM) Model Summary:\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BertOnlyMLMHead(\n",
      "  (predictions): BertLMPredictionHead(\n",
      "    (transform): BertPredictionHeadTransform(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (transform_act_fn): GELUActivation()\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Total params: 108,340,804, Trainable params: 108,340,804\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT (Sequence Classification) Model Summary:\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=5, bias=True)\n",
      "\n",
      "\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Total params: 108,314,117, Trainable params: 108,314,117\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM \n",
    "def _model_summary(model):\n",
    "    for layer in model.children():\n",
    "        print(layer)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"\\n\")\n",
    "    print(model.config)\n",
    "    print(f\"Total params: {total_params:,}, Trainable params: {trainable_params:,}\\n\\n\")\n",
    "\n",
    "base_model_name = \"google-bert/bert-base-cased\"\n",
    "\n",
    "#Original BERT (Masked LM):\n",
    "#[Input] â†’ [12 Transformer Layers] â†’ [MLM Head] â†’ Predict masked tokens\n",
    "base_mlm_model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "print(\"Original BERT (Masked LM) Model Summary:\")\n",
    "_model_summary(base_mlm_model)\n",
    "\n",
    "#Fine-tuned BERT (Sequence Classification):\n",
    "#[Input] â†’ [12 Transformer Layers] â†’ [Classification Head] â†’ 5-class output\n",
    "#replaces BERT's original head with a new classification head\n",
    "base_classifier_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=5)\n",
    "print(\"\\nBERT (Sequence Classification) Model Summary:\")\n",
    "_model_summary(base_classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af10f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5577ccbf7c644ef1babaeda032830f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer \n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42cdd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a25fdb291be459085affdf83599c544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 4, 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", 'input_ids': [101, 173, 1197, 119, 2284, 2953, 3272, 1917, 178, 1440, 1111, 1107, 170, 1704, 22351, 119, 1119, 112, 188, 3505, 1105, 3123, 1106, 2037, 1106, 1443, 1217, 10063, 4404, 132, 1119, 112, 188, 1579, 1113, 1159, 1107, 3195, 1117, 4420, 132, 1119, 112, 188, 6559, 1114, 170, 1499, 118, 23555, 2704, 113, 183, 9379, 114, 1134, 1139, 2153, 1138, 3716, 1106, 1143, 1110, 1304, 1696, 1107, 1692, 1380, 5940, 1105, 1128, 1444, 6059, 132, 1105, 1128, 1169, 1243, 5991, 16179, 1106, 1267, 18137, 1443, 1515, 1106, 1267, 1140, 1148, 119, 1541, 117, 1184, 1167, 1202, 1128, 1444, 136, 178, 112, 182, 2807, 1303, 1774, 1106, 1341, 1104, 1251, 11344, 178, 1138, 1164, 1140, 117, 1133, 178, 112, 182, 1541, 4619, 170, 9153, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "def tokenize(tokenizer, examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = dataset.map(lambda x: tokenize(base_tokenizer, x), batched=True)\n",
    "# save a test subset to csv\n",
    "pd.DataFrame(dataset[\"test\"][:1000]).to_csv(\"../03.ignore/yelp_review_full_test.csv\", index=False)\n",
    "#show dataset sample after tokenization\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa67460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.dipaolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 10:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.605766</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.dipaolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13, training_loss=1.6276983114389272, metrics={'train_runtime': 649.8306, 'train_samples_per_second': 0.154, 'train_steps_per_second': 0.02, 'total_flos': 26311814246400.0, 'train_loss': 1.6276983114389272, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../03.ignore/yelp-review-classifier\",\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=1,#ridiculously small for demo purposes: colab 3\n",
    "    push_to_hub=False,\n",
    ")\n",
    "num_samples = 100 #ridiculously small subset for demo purposes: colab 10_000\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"].shuffle(seed=42).select(range(num_samples)), #ridiculously small subset for demo purposes\n",
    "    eval_dataset=dataset[\"test\"].shuffle(seed=42).select(range(num_samples)),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df6112",
   "metadata": {},
   "source": [
    "\n",
    "### colab 10_000 samples in 3 epochs\n",
    "```text  \n",
    "TrainOutput(global_step=3750, training_loss=0.7398969421386719, metrics={'train_runtime': 3971.4154, 'train_samples_per_second': 7.554, 'train_steps_per_second': 0.944, 'total_flos': 7893544273920000.0, 'train_loss': 0.7398969421386719, 'epoch': 3.0})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68923de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TRANSFORMERS_CACHE\n",
    "#models--google-bert--bert-base-cased\n",
    "#models--google-bert--bert-instruct-yelp-classifier\n",
    "if False:\n",
    "    trainer.save_model(f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1ba4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upstream has gone downhill, fast. While the bar ambience and feel is still awesome, the sushi in particular was extremely lacking. Spicy Tuna rolls were not fresh and smelled like fish. California rolls lacked flavor and were quite bland. \\n\\nService was also not particularly good and there was a very creepy manager who put us off. \\n\\nIt is a shame because Upstream has been a staple of the South Park area for years, but their sushi quality is completely lacking these days. I cannot comment on the main courses as we did not have any.\n",
      "Base: 3 | fine-tuned: 2 | actual: 2 stars\n",
      "\n",
      "The art museum is pretty nice; the carriage and car museum is cool; and it's neat to walk through the greenhouse.  Those are free.  However, nothing really separates the $12 house tour from other house tours.  Like the other ones, the docents don't focus on anything interesting, like history or the architecture or the building itself.  They focus on some family story that may only be of passing interest because the family is rich.  Kind of like hearing about Paris Hilton or the Kardashians.  B-o-r-i-n-g. And what kind of tourist attraction doesn't allow pictures?\n",
      "Base: 3 | fine-tuned: 3 | actual: 2 stars\n",
      "\n",
      "I've been playing softball at Bowling Green for, well, let's just say \\\"a long time\\\".  While it's not a modern athletic facility, it's got a niche market.  If you enjoy playing ball, having a cold beer, and having fun with some friends - this is a good place for a league.  The umpires are excellent - I've known a few for many years and they do a great job.  \\n\\nThe fields are smaller, but \\\"good bats\\\" are limited, keeping home runs down a little (but it feels good to get one - it still goes plenty far!).\\n\\nI hope they stick around for a long time - I love playing there on Tuesday nights.\n",
      "Base: 3 | fine-tuned: 4 | actual: 4 stars\n",
      "\n",
      "I think the location in homestead has ruined this place. Dirty dirty dirty.\\n\\nBathrooms unkept, entry door handle and windows filthy\\n\\nPizza had too much sauce on it and was drenched. Server was coarse, not refined\\n\\nServing items were old and rusty. Temperature of food was not hot\\n\\nLow quality, but probably ok with pittsburgher s\n",
      "Base: 3 | fine-tuned: 1 | actual: 3 stars\n",
      "\n",
      "Great bread!  But I wish the sandwiches were a little bigger.  I got the wheat sub roll with meal deal and it disappeared too quickly...left me hungry!  I guess I'll try the french bread next time.  Maybe it's a little bigger.\n",
      "Base: 3 | fine-tuned: 3 | actual: 2 stars\n",
      "\n",
      "With such great reviews I have been trying! I have been there tree times now and I am not surprised! The prices are reasonable and you get decent food. But thats not what I expected. The chocolate and caramel trotewas bad! The cupcakes were ok but for 1usd you can't expect much. The buttercream wasnt impressive... But the filled croissants with blueberry or the one with cheese were not bad at all. It was not a real croissant but satisfying. Since it is in a really convenient location I will probably go back but without the expectations that I had before.\n",
      "Base: 3 | fine-tuned: 3 | actual: 3 stars\n",
      "\n",
      "Today, I definitely didn't have a great experience at MCN. I got my haircut couple months ago from there and I liked it, so I decided to go again for coloring today.\\nI wasn't happy about it at all -cuz I can still see the line btw my root and the bottom. PLUS, they charged me 25 bucks more for the blow-dry without telling me. I wish I didnt go there for the color..\n",
      "Base: 3 | fine-tuned: 2 | actual: 1 stars\n",
      "\n",
      "In Oct. my son that is at  UNCC had a nail in his tire. The person at Tire Kingdom told him he needed 3 new tires., THe car is dented and has over 151,000 miles on it.We told him to match the tread. Instead of giving him options of prices they put on$ 184.00 tires EAC on the vehicle. I called very upset the next morning and the manger said he would not give me an exchange nor would he give me any money off. He offered a 4th tire with a $70.00 rebate and would discount an oil change! This is taking advantage of a young consumer and  in my opinion a terrible way of doing business. I would NEVER recommend this company.\n",
      "Base: 3 | fine-tuned: 1 | actual: 1 stars\n",
      "\n",
      "I've been there twice to have dress clothes tailored. The first was for a full suit and a pair of pants. The owner said to return the following Saturday, which I did, and he did not have them ready. Not only that, but he acted like it was my fault I showed back up on time. \\n\\nSecond time same deal, two pairs of pants, not done on time and when they were done they were not tailored correctly. I'll be going out of my way to a different tailor from now on. \\n\\nIn addition, the prices are on the very high side without the service or results.\n",
      "Base: 3 | fine-tuned: 2 | actual: 1 stars\n",
      "\n",
      "The Korean tacos are not half bad, and the kid really likes the burgers.\n",
      "Base: 3 | fine-tuned: 4 | actual: 3 stars\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TRANSFORMERS_CACHE\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# untrained vs fine-tuned model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "base_model.to(device)\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\", num_labels=5)\n",
    "finetuned_model.to(device)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "dataset = pd.read_csv(\"../03.ignore/yelp_review_full_test.csv\").to_dict(orient=\"records\")\n",
    "random.shuffle(dataset)\n",
    "\n",
    "for i in range(10):    \n",
    "    _review = dataset[i]['text']\n",
    "    print(f\"\\n{_review}\")\n",
    "    inputs = base_tokenizer(_review, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # move input tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # predictions\n",
    "    base_pred = torch.argmax(base_model(**inputs).logits, dim=1).item()\n",
    "    fine_tuned_pred = torch.argmax(finetuned_model(**inputs).logits, dim=1).item()\n",
    "    print(f\"Base: {base_pred + 1} | fine-tuned: {fine_tuned_pred + 1} | actual: {dataset[i]['label'] + 1} stars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a46873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Evaluating on 100 samples...\n",
      "\n",
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing review in batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:25<00:00, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINE-TUNED MODEL RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.5800\n",
      "Precision: 0.5987\n",
      "Recall:    0.5800\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.61      0.58      0.60        24\n",
      "     2 stars       0.52      0.54      0.53        26\n",
      "     3 stars       0.44      0.62      0.52        13\n",
      "     4 stars       0.75      0.52      0.62        23\n",
      "     5 stars       0.62      0.71      0.67        14\n",
      "\n",
      "    accuracy                           0.58       100\n",
      "   macro avg       0.59      0.59      0.58       100\n",
      "weighted avg       0.60      0.58      0.58       100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14 10  0  0  0]\n",
      " [ 7 14  5  0  0]\n",
      " [ 1  2  8  2  0]\n",
      " [ 1  0  4 12  6]\n",
      " [ 0  1  1  2 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TRANSFORMERS_CACHE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\", \n",
    "    num_labels=5\n",
    ")\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()  # set to evaluation mode\n",
    "\n",
    "# load tokenizer (unchanged from fine-tuning model)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# load test (unseen) dataset\n",
    "dataset = pd.read_csv(\"../03.ignore/yelp_review_full_test.csv\")[:100]\n",
    "\n",
    "print(f\"Evaluating on {len(dataset)} samples...\")\n",
    "\n",
    "def evaluate_model(model, tokenizer, reviews, batch_size=32):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient computation for inference\n",
    "        for i in tqdm(range(0, len(reviews), batch_size), desc=\"Processing review in batches\"):\n",
    "            batch_reviews = reviews[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = tokenizer(\n",
    "                batch_reviews, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(**inputs).logits            \n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy() \n",
    "            predictions.extend(batch_preds)\n",
    "    \n",
    "    return np.array(predictions)  \n",
    "\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "finetuned_predictions = evaluate_model(finetuned_model, base_tokenizer, dataset['text'].tolist())\n",
    "\n",
    "# Get true labels\n",
    "true_labels = dataset['label'].values\n",
    "\n",
    "# Calculate metrics for fine-tuned model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "finetuned_accuracy = accuracy_score(true_labels, finetuned_predictions)\n",
    "finetuned_precision = precision_score(true_labels, finetuned_predictions, average='weighted')\n",
    "finetuned_recall = recall_score(true_labels, finetuned_predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {finetuned_accuracy:.4f}\")\n",
    "print(f\"Precision: {finetuned_precision:.4f}\")\n",
    "print(f\"Recall:    {finetuned_recall:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    true_labels, \n",
    "    finetuned_predictions, \n",
    "    target_names=['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, finetuned_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff7d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                     text  pred  actual result\n",
      "          A delightful experience with a cozy atmosphere and great music.     4       4      âœ…\n",
      "      The pasta was cooked perfectly and the sauce tasted fresh and rich.     4       5      ðŸŸ¡\n",
      "  Service was slow and the waiter seemed annoyed when we asked for water.     2       2      âœ…\n",
      "   Amazing experience. The steak was juicy and the dessert unforgettable.     4       5      ðŸŸ¡\n",
      "                The soup was cold and lacked flavor. Not worth the price.     1       2      ðŸŸ¡\n",
      "Friendly staff, cozy atmosphere, and delicious pizza. Highly recommended.     5       5      âœ…\n",
      "                           Mediocre food, but the location is convenient.     2       3      ðŸŸ¡\n",
      "           The restaurant was too noisy and the portions were very small.     2       2      âœ…\n",
      "                         Excellent seafood, fresh and perfectly seasoned.     4       5      ðŸŸ¡\n",
      "                               Nothing special. Average food and service.     2       3      ðŸŸ¡\n",
      "       Terrible experience. My order was wrong and the staff didnâ€™t care.     1       1      âœ…\n",
      "                           The sushi was fresh and beautifully presented.     5       5      âœ…\n",
      "                         Overpriced for what you get. The food was bland.     2       2      âœ…\n",
      " Great ambiance and friendly waiters, but the main course was overcooked.     3       3      âœ…\n",
      "      Everything was perfect â€” the service, the food, and the atmosphere.     5       5      âœ…\n",
      "                Disappointing. The burger was greasy and the fries soggy.     1       1      âœ…\n",
      "                    Good value for money. Tasty dishes and quick service.     3       4      ðŸŸ¡\n",
      "                  Decent place for a casual lunch. Nothing extraordinary.     3       3      âœ…\n",
      "            Horrible hygiene. Tables were dirty and cutlery wasnâ€™t clean.     1       1      âœ…\n",
      "             Lovely presentation, attentive staff, and fantastic flavors.     5       5      âœ…\n",
      "           Average experience. Some dishes were good, others poorly made.     3       3      âœ…\n",
      "                                      Highest price, lowest quality food.     1       1      âœ…\n",
      "                                      Highest quality food, lowest price.     5       5      âœ…\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TRANSFORMERS_CACHE\n",
    "dataset = [\n",
    "    {\"text\": \"A delightful experience with a cozy atmosphere and great music.\", \"stars\": 4},\n",
    "    {\"text\": \"The pasta was cooked perfectly and the sauce tasted fresh and rich.\", \"stars\": 5},\n",
    "    {\"text\": \"Service was slow and the waiter seemed annoyed when we asked for water.\", \"stars\": 2},\n",
    "    {\"text\": \"Amazing experience. The steak was juicy and the dessert unforgettable.\", \"stars\": 5},\n",
    "    {\"text\": \"The soup was cold and lacked flavor. Not worth the price.\", \"stars\": 2},\n",
    "    {\"text\": \"Friendly staff, cozy atmosphere, and delicious pizza. Highly recommended.\", \"stars\": 5},\n",
    "    {\"text\": \"Mediocre food, but the location is convenient.\", \"stars\": 3},\n",
    "    {\"text\": \"The restaurant was too noisy and the portions were very small.\", \"stars\": 2},\n",
    "    {\"text\": \"Excellent seafood, fresh and perfectly seasoned.\", \"stars\": 5},\n",
    "    {\"text\": \"Nothing special. Average food and service.\", \"stars\": 3},\n",
    "    {\"text\": \"Terrible experience. My order was wrong and the staff didnâ€™t care.\", \"stars\": 1},\n",
    "    {\"text\": \"The sushi was fresh and beautifully presented.\", \"stars\": 5},\n",
    "    {\"text\": \"Overpriced for what you get. The food was bland.\", \"stars\": 2},\n",
    "    {\"text\": \"Great ambiance and friendly waiters, but the main course was overcooked.\", \"stars\": 3},\n",
    "    {\"text\": \"Everything was perfect â€” the service, the food, and the atmosphere.\", \"stars\": 5},\n",
    "    {\"text\": \"Disappointing. The burger was greasy and the fries soggy.\", \"stars\": 1},\n",
    "    {\"text\": \"Good value for money. Tasty dishes and quick service.\", \"stars\": 4},\n",
    "    {\"text\": \"Decent place for a casual lunch. Nothing extraordinary.\", \"stars\": 3},\n",
    "    {\"text\": \"Horrible hygiene. Tables were dirty and cutlery wasnâ€™t clean.\", \"stars\": 1},\n",
    "    {\"text\": \"Lovely presentation, attentive staff, and fantastic flavors.\", \"stars\": 5},\n",
    "    {\"text\": \"Average experience. Some dishes were good, others poorly made.\", \"stars\": 3},\n",
    "    {\"text\": \"Highest price, lowest quality food.\", \"stars\": 1},\n",
    "    {\"text\": \"Highest quality food, lowest price.\", \"stars\": 5}\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = []\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\", num_labels=5)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "for item in dataset:\n",
    "    inputs = tokenizer(item['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits    \n",
    "    prediction = torch.argmax(logits, dim=1).item() + 1 # stars are 1-5\n",
    "    results.append({\n",
    "        \"text\": item['text'],\n",
    "        \"pred\": prediction,\n",
    "        \"actual\": item['stars'],\n",
    "        \"result\": \"âœ…\" if prediction == item['stars'] else (\"ðŸŸ¡\" if abs(prediction - item['stars']) == 1 else \"âŒ\")\n",
    "    })\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee06f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at D:\\.cache\\huggingface\\hub/models--google-bert--bert-instruct-reviewer and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Serving './tmp/bert-base.pt' at http://localhost:8080\n",
      "Serving './tmp/bert-instruct.pt' at http://localhost:8080\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-base\n",
      "[{'score': 0.9872503280639648, 'token': 2364, 'token_str': 'capital', 'sequence': 'The capital of Italy is Rome.'}, {'score': 0.008227720856666565, 'token': 6299, 'token_str': 'Capital', 'sequence': 'The Capital of Italy is Rome.'}, {'score': 0.0007266324246302247, 'token': 2642, 'token_str': 'centre', 'sequence': 'The centre of Italy is Rome.'}, {'score': 0.000724579265806824, 'token': 2057, 'token_str': 'center', 'sequence': 'The center of Italy is Rome.'}, {'score': 0.0004981309175491333, 'token': 15979, 'token_str': 'birthplace', 'sequence': 'The birthplace of Italy is Rome.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: bert-instruct-as-mlm\n",
      "[{'score': 0.003434599144384265, 'token': 13064, 'token_str': '##aneous', 'sequence': 'Theaneous of Italy is Rome.'}, {'score': 0.0032075075432658195, 'token': 24730, 'token_str': '##naire', 'sequence': 'Thenaire of Italy is Rome.'}, {'score': 0.0031963037326931953, 'token': 18734, 'token_str': '##joy', 'sequence': 'Thejoy of Italy is Rome.'}, {'score': 0.0029961178079247475, 'token': 16409, 'token_str': 'Ex', 'sequence': 'The Ex of Italy is Rome.'}, {'score': 0.0024864510633051395, 'token': 16123, 'token_str': '##coe', 'sequence': 'Thecoe of Italy is Rome.'}]\n",
      "==========================================================================================\n",
      "\n",
      "Model: bert-base\n",
      "[{'score': 0.9872503280639648, 'token': 2364, 'token_str': 'capital', 'sequence': 'The capital of Italy is Rome.'}, {'score': 0.008227720856666565, 'token': 6299, 'token_str': 'Capital', 'sequence': 'The Capital of Italy is Rome.'}, {'score': 0.0007266324246302247, 'token': 2642, 'token_str': 'centre', 'sequence': 'The centre of Italy is Rome.'}, {'score': 0.000724579265806824, 'token': 2057, 'token_str': 'center', 'sequence': 'The center of Italy is Rome.'}, {'score': 0.0004981309175491333, 'token': 15979, 'token_str': 'birthplace', 'sequence': 'The birthplace of Italy is Rome.'}]\n",
      "\n",
      "Model: bert-instruct-as-mlm\n",
      "[{'score': 0.9835878014564514, 'token': 2364, 'token_str': 'capital', 'sequence': 'The capital of Italy is Rome.'}, {'score': 0.004990625660866499, 'token': 24199, 'token_str': 'capitals', 'sequence': 'The capitals of Italy is Rome.'}, {'score': 0.0041067348793148994, 'token': 3279, 'token_str': 'Senate', 'sequence': 'The Senate of Italy is Rome.'}, {'score': 0.0015787158627063036, 'token': 2901, 'token_str': 'Parliament', 'sequence': 'The Parliament of Italy is Rome.'}, {'score': 0.001384721021167934, 'token': 6299, 'token_str': 'Capital', 'sequence': 'The Capital of Italy is Rome.'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import netron\n",
    "import os\n",
    "from transformers import AutoModelForMaskedLM, AutoModelForSequenceClassification, TRANSFORMERS_CACHE, pipeline\n",
    "\n",
    "# load both models & create a version of the fine-tuned model as MLM\n",
    "base = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "finetuned = AutoModelForSequenceClassification.from_pretrained(f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\",num_labels=5)\n",
    "finetuned_as_mlm = AutoModelForMaskedLM.from_pretrained(f\"{TRANSFORMERS_CACHE}/models--google-bert--bert-instruct-reviewer\")\n",
    "\n",
    "# save models:\n",
    "if (not os.path.exists(\"./tmp\")):\n",
    "    os.makedirs(\"./tmp\")\n",
    "for [model, name] in [(base, \"bert-base\"), (finetuned, \"bert-instruct\")]:\n",
    "    if not os.path.exists(f\"./tmp/{name}.pt\"):\n",
    "        torch.save(model, f\"./tmp/{name}.pt\")\n",
    "    if not os.path.exists(f\"./tmp/{name}-weights.pt\"):\n",
    "        torch.save(model.state_dict(), f\"./tmp/{name}-weights.pt\")\n",
    "    netron.start(f\"./tmp/{name}.pt\")\n",
    "\n",
    "\n",
    "# unmask word in sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "sentence = \"The [MASK] of Italy is Rome.\"\n",
    "for [model, name] in [(base, \"bert-base\"), (finetuned_as_mlm, \"bert-instruct-as-mlm\")]:\n",
    "    fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "    result = fill_mask(sentence)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(result)\n",
    "\n",
    "print(\"===\"*30)\n",
    "\n",
    "# copy weights of BertOnlyMLMHead from base to finetuned_as_mlm\n",
    "finetuned_as_mlm.cls.load_state_dict(base.cls.state_dict())\n",
    "for [model, name] in [(base, \"bert-base\"), (finetuned_as_mlm, \"bert-instruct-as-mlm\")]:\n",
    "    fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "    result = fill_mask(sentence)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
