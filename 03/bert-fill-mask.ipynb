{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5638c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')  # adjust the path as needed\n",
    "print(os.environ.get('TORCH_CUDA_ARCH_LIST', ''));  # to silence linter warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459f52ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at: /home/admin/_github/massimodipaolo/ai-crash-course/.venv\u001b[0m\n",
      "Name: torch\n",
      "Version: 2.9.0+cu130\n",
      "Location: /home/admin/_github/massimodipaolo/ai-crash-course/.venv/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas, nvidia-cuda-cupti, nvidia-cuda-nvrtc, nvidia-cuda-runtime, nvidia-cudnn-cu13, nvidia-cufft, nvidia-cufile, nvidia-curand, nvidia-cusolver, nvidia-cusparse, nvidia-cusparselt-cu13, nvidia-nccl-cu13, nvidia-nvjitlink, nvidia-nvshmem-cu13, nvidia-nvtx, setuptools, sympy, triton, typing-extensions\n",
      "Required-by: sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!~/.local/bin/uv pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18724d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/_github/massimodipaolo/ai-crash-course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda\n",
      "/home/admin/_github/massimodipaolo/ai-crash-course/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9872570037841797,\n",
       "  'token': 2364,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'The capital of Italy is Rome.'},\n",
       " {'score': 0.00822779256850481,\n",
       "  'token': 6299,\n",
       "  'token_str': 'Capital',\n",
       "  'sequence': 'The Capital of Italy is Rome.'},\n",
       " {'score': 0.0007266400498338044,\n",
       "  'token': 2642,\n",
       "  'token_str': 'centre',\n",
       "  'sequence': 'The centre of Italy is Rome.'},\n",
       " {'score': 0.0007245848537422717,\n",
       "  'token': 2057,\n",
       "  'token_str': 'center',\n",
       "  'sequence': 'The center of Italy is Rome.'},\n",
       " {'score': 0.0004981357487849891,\n",
       "  'token': 15979,\n",
       "  'token_str': 'birthplace',\n",
       "  'sequence': 'The birthplace of Italy is Rome.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask language model: predict masked token given surrounding tokens\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "model = \"google-bert/bert-base-cased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, device=device)\n",
    "#print(fill_mask.model.config)\n",
    "result = fill_mask(\"The [MASK] of Italy is Rome.\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615e39d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9341114163398743,\n",
       "  'token': 2364,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'The capital of Spain is Rome.'},\n",
       " {'score': 0.014675662852823734,\n",
       "  'token': 6299,\n",
       "  'token_str': 'Capital',\n",
       "  'sequence': 'The Capital of Spain is Rome.'},\n",
       " {'score': 0.010236961767077446,\n",
       "  'token': 1946,\n",
       "  'token_str': 'seat',\n",
       "  'sequence': 'The seat of Spain is Rome.'},\n",
       " {'score': 0.002829367760568857,\n",
       "  'token': 10063,\n",
       "  'token_str': 'patron',\n",
       "  'sequence': 'The patron of Spain is Rome.'},\n",
       " {'score': 0.0026473377365618944,\n",
       "  'token': 2057,\n",
       "  'token_str': 'center',\n",
       "  'sequence': 'The center of Spain is Rome.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.22043129801750183,\n",
       "  'token': 1271,\n",
       "  'token_str': 'name',\n",
       "  'sequence': 'The name of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.03419250249862671,\n",
       "  'token': 1641,\n",
       "  'token_str': 'title',\n",
       "  'sequence': 'The title of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.030494004487991333,\n",
       "  'token': 4134,\n",
       "  'token_str': 'address',\n",
       "  'sequence': 'The address of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.026066243648529053,\n",
       "  'token': 2351,\n",
       "  'token_str': 'author',\n",
       "  'sequence': 'The author of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.023992838338017464,\n",
       "  'token': 5754,\n",
       "  'token_str': 'definition',\n",
       "  'sequence': 'The definition of _ _ _ _ _ is _ _ _ _ _.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# but also, with high score...\n",
    "result = fill_mask(\"The [MASK] of Spain is Rome.\")\n",
    "display(result)\n",
    "\n",
    "# because during pre-training it learned: The [MASK] of <country> is <city>\n",
    "# saw thousands of sentences like \"The capital of France is Paris.\"\n",
    "# or more generic patterns like...\n",
    "result = fill_mask(\"The [MASK] of _____ is _____.\")\n",
    "display(result)\n",
    "\n",
    "# no fact-checking mechanism! no knowledge awareness!\n",
    "# it's purely probabilistic based on co-occurrence patterns in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8025d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'The black man worked as a [MASK].',\n",
       "  'result': [{'score': 0.07436051219701767,\n",
       "    'token': 17989,\n",
       "    'token_str': 'waiter',\n",
       "    'sequence': 'The black man worked as a waiter.'},\n",
       "   {'score': 0.044776901602745056,\n",
       "    'token': 18343,\n",
       "    'token_str': 'bartender',\n",
       "    'sequence': 'The black man worked as a bartender.'},\n",
       "   {'score': 0.037703704088926315,\n",
       "    'token': 9140,\n",
       "    'token_str': 'detective',\n",
       "    'sequence': 'The black man worked as a detective.'}]},\n",
       " {'prompt': 'The woman worked as a [MASK].',\n",
       "  'result': [{'score': 0.16927538812160492,\n",
       "    'token': 7439,\n",
       "    'token_str': 'nurse',\n",
       "    'sequence': 'The woman worked as a nurse.'},\n",
       "   {'score': 0.15011048316955566,\n",
       "    'token': 15098,\n",
       "    'token_str': 'waitress',\n",
       "    'sequence': 'The woman worked as a waitress.'},\n",
       "   {'score': 0.056002069264650345,\n",
       "    'token': 13487,\n",
       "    'token_str': 'maid',\n",
       "    'sequence': 'The woman worked as a maid.'}]},\n",
       " {'prompt': 'The airplane pilot was a [MASK].',\n",
       "  'result': [{'score': 0.038353778421878815,\n",
       "    'token': 1299,\n",
       "    'token_str': 'man',\n",
       "    'sequence': 'The airplane pilot was a man.'},\n",
       "   {'score': 0.026097100228071213,\n",
       "    'token': 3995,\n",
       "    'token_str': 'doctor',\n",
       "    'sequence': 'The airplane pilot was a doctor.'},\n",
       "   {'score': 0.024719905108213425,\n",
       "    'token': 1590,\n",
       "    'token_str': 'woman',\n",
       "    'sequence': 'The airplane pilot was a woman.'}]},\n",
       " {'prompt': 'The secretary was a [MASK].',\n",
       "  'result': [{'score': 0.1275254338979721,\n",
       "    'token': 1590,\n",
       "    'token_str': 'woman',\n",
       "    'sequence': 'The secretary was a woman.'},\n",
       "   {'score': 0.02656354382634163,\n",
       "    'token': 6477,\n",
       "    'token_str': 'mess',\n",
       "    'sequence': 'The secretary was a mess.'},\n",
       "   {'score': 0.02138865366578102,\n",
       "    'token': 4848,\n",
       "    'token_str': 'secretary',\n",
       "    'sequence': 'The secretary was a secretary.'}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# are llm training data biased?\n",
    "# datasets used: \n",
    "# https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
    "# https://huggingface.co/datasets/bookcorpus/bookcorpus\n",
    "prompts = [\n",
    "        \"The black man worked as a [MASK].\",\n",
    "        \"The woman worked as a [MASK].\",\n",
    "        \"The airplane pilot was a [MASK].\",\n",
    "        \"The secretary was a [MASK].\"\n",
    "]\n",
    "results = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    result = fill_mask(prompt)\n",
    "    results.append({ \"prompt\": prompt, \"result\": result[:3] })\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba05852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In a distant future, humanity has.........................\n",
      "\n",
      "---\n",
      "\n",
      "who are you?.........................\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# can predict?\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "text_generator = pipeline(\"text-generation\", model=model)\n",
    "result = text_generator(prompt, max_new_tokens=25, truncation=True, num_return_sequences=1)\n",
    "print(\"\\n\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "# Q&A?\n",
    "print(text_generator(\"who are you?\", max_new_tokens=25, truncation=True, num_return_sequences=1)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0034b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In a distant future, humanity has developed a method to simulate the Earth's surface using a computer program. This program uses a coordinate system where each point is represented\n"
     ]
    }
   ],
   "source": [
    "#causal language model: predict next token given previous tokens\n",
    "from transformers import pipeline\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "text_generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
    "result = text_generator(prompt, max_new_tokens=25, truncation=True, num_return_sequences=1)\n",
    "print(\"\\n\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
