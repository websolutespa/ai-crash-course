{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18724d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9872503280639648,\n",
       "  'token': 2364,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'The capital of Italy is Rome.'},\n",
       " {'score': 0.008227720856666565,\n",
       "  'token': 6299,\n",
       "  'token_str': 'Capital',\n",
       "  'sequence': 'The Capital of Italy is Rome.'},\n",
       " {'score': 0.0007266324246302247,\n",
       "  'token': 2642,\n",
       "  'token_str': 'centre',\n",
       "  'sequence': 'The centre of Italy is Rome.'},\n",
       " {'score': 0.000724579265806824,\n",
       "  'token': 2057,\n",
       "  'token_str': 'center',\n",
       "  'sequence': 'The center of Italy is Rome.'},\n",
       " {'score': 0.0004981309175491333,\n",
       "  'token': 15979,\n",
       "  'token_str': 'birthplace',\n",
       "  'sequence': 'The birthplace of Italy is Rome.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask language model: predict masked token given surrounding tokens\n",
    "from transformers import pipeline\n",
    "model = \"google-bert/bert-base-cased\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model)\n",
    "#print(fill_mask.model.config)\n",
    "result = fill_mask(\"The [MASK] of Italy is Rome.\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615e39d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9340920448303223,\n",
       "  'token': 2364,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'The capital of Spain is Rome.'},\n",
       " {'score': 0.014675344340503216,\n",
       "  'token': 6299,\n",
       "  'token_str': 'Capital',\n",
       "  'sequence': 'The Capital of Spain is Rome.'},\n",
       " {'score': 0.010236779227852821,\n",
       "  'token': 1946,\n",
       "  'token_str': 'seat',\n",
       "  'sequence': 'The seat of Spain is Rome.'},\n",
       " {'score': 0.0028293116483837366,\n",
       "  'token': 10063,\n",
       "  'token_str': 'patron',\n",
       "  'sequence': 'The patron of Spain is Rome.'},\n",
       " {'score': 0.002647282788529992,\n",
       "  'token': 2057,\n",
       "  'token_str': 'center',\n",
       "  'sequence': 'The center of Spain is Rome.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.22043085098266602,\n",
       "  'token': 1271,\n",
       "  'token_str': 'name',\n",
       "  'sequence': 'The name of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.03419249504804611,\n",
       "  'token': 1641,\n",
       "  'token_str': 'title',\n",
       "  'sequence': 'The title of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.030494118109345436,\n",
       "  'token': 4134,\n",
       "  'token_str': 'address',\n",
       "  'sequence': 'The address of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.026066241785883904,\n",
       "  'token': 2351,\n",
       "  'token_str': 'author',\n",
       "  'sequence': 'The author of _ _ _ _ _ is _ _ _ _ _.'},\n",
       " {'score': 0.023992745205760002,\n",
       "  'token': 5754,\n",
       "  'token_str': 'definition',\n",
       "  'sequence': 'The definition of _ _ _ _ _ is _ _ _ _ _.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# but also, with high score...\n",
    "result = fill_mask(\"The [MASK] of Spain is Rome.\")\n",
    "display(result)\n",
    "\n",
    "# because during pre-training it learned: The [MASK] of <country> is <city>\n",
    "# saw thousands of sentences like \"The capital of France is Paris.\"\n",
    "# or more generic patterns like...\n",
    "result = fill_mask(\"The [MASK] of _____ is _____.\")\n",
    "display(result)\n",
    "\n",
    "# no fact-checking mechanism! no knowledge awareness!\n",
    "# it's purely probabilistic based on co-occurrence patterns in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8025d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'The black man worked as a [MASK].',\n",
       "  'result': [{'score': 0.07436003535985947,\n",
       "    'token': 17989,\n",
       "    'token_str': 'waiter',\n",
       "    'sequence': 'The black man worked as a waiter.'},\n",
       "   {'score': 0.04477665573358536,\n",
       "    'token': 18343,\n",
       "    'token_str': 'bartender',\n",
       "    'sequence': 'The black man worked as a bartender.'},\n",
       "   {'score': 0.03770360350608826,\n",
       "    'token': 9140,\n",
       "    'token_str': 'detective',\n",
       "    'sequence': 'The black man worked as a detective.'}]},\n",
       " {'prompt': 'The woman worked as a [MASK].',\n",
       "  'result': [{'score': 0.16927418112754822,\n",
       "    'token': 7439,\n",
       "    'token_str': 'nurse',\n",
       "    'sequence': 'The woman worked as a nurse.'},\n",
       "   {'score': 0.1501094102859497,\n",
       "    'token': 15098,\n",
       "    'token_str': 'waitress',\n",
       "    'sequence': 'The woman worked as a waitress.'},\n",
       "   {'score': 0.05600161850452423,\n",
       "    'token': 13487,\n",
       "    'token_str': 'maid',\n",
       "    'sequence': 'The woman worked as a maid.'}]},\n",
       " {'prompt': 'The airplane pilot was a [MASK].',\n",
       "  'result': [{'score': 0.038353778421878815,\n",
       "    'token': 1299,\n",
       "    'token_str': 'man',\n",
       "    'sequence': 'The airplane pilot was a man.'},\n",
       "   {'score': 0.026097074151039124,\n",
       "    'token': 3995,\n",
       "    'token_str': 'doctor',\n",
       "    'sequence': 'The airplane pilot was a doctor.'},\n",
       "   {'score': 0.024719953536987305,\n",
       "    'token': 1590,\n",
       "    'token_str': 'woman',\n",
       "    'sequence': 'The airplane pilot was a woman.'}]},\n",
       " {'prompt': 'The secretary was a [MASK].',\n",
       "  'result': [{'score': 0.12752561271190643,\n",
       "    'token': 1590,\n",
       "    'token_str': 'woman',\n",
       "    'sequence': 'The secretary was a woman.'},\n",
       "   {'score': 0.026563553139567375,\n",
       "    'token': 6477,\n",
       "    'token_str': 'mess',\n",
       "    'sequence': 'The secretary was a mess.'},\n",
       "   {'score': 0.021388662979006767,\n",
       "    'token': 4848,\n",
       "    'token_str': 'secretary',\n",
       "    'sequence': 'The secretary was a secretary.'}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# are llm training data biased?\n",
    "# datasets used: \n",
    "# https://huggingface.co/datasets/legacy-datasets/wikipedia\n",
    "# https://huggingface.co/datasets/bookcorpus/bookcorpus\n",
    "prompts = [\n",
    "        \"The black man worked as a [MASK].\",\n",
    "        \"The woman worked as a [MASK].\",\n",
    "        \"The airplane pilot was a [MASK].\",\n",
    "        \"The secretary was a [MASK].\"\n",
    "]\n",
    "results = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    result = fill_mask(prompt)\n",
    "    results.append({ \"prompt\": prompt, \"result\": result[:3] })\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba05852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In a distant future, humanity has.........................\n",
      "\n",
      "---\n",
      "\n",
      "who are you?.........................\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# can predict?\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "text_generator = pipeline(\"text-generation\", model=model)\n",
    "result = text_generator(prompt, max_new_tokens=25, truncation=True, num_return_sequences=1)\n",
    "print(\"\\n\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "# Q&A?\n",
    "print(text_generator(\"who are you?\", max_new_tokens=25, truncation=True, num_return_sequences=1)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034b1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d128989078d6497988a951027466868d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0b5c085f76453b90f784b50a2baf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#causal language model: predict next token given previous tokens\n",
    "from transformers import pipeline\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "text_generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
    "result = text_generator(prompt, max_new_tokens=25, truncation=True, num_return_sequences=1)\n",
    "print(\"\\n\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
