{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf151de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 32768, 'do_lower_case': False, 'architecture': 'Qwen3Model'})\n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model components:\n",
      "0. Transformer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3Config {\n",
       "  \"architectures\": [\n",
       "    \"Qwen3ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_types\": [\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\"\n",
       "  ],\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen3\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151669\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pooling\n",
      "\n",
      "2. Normalize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "#model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# The model contains:\n",
    "# 1. Tokenizer - converts text to token IDs\n",
    "# 2. Transformer layers - neural network weights\n",
    "# 3. Pooling layer - combines token embeddings into a single vector\n",
    "# 4. Normalization layer - scales the output vector  \n",
    "display(model)\n",
    "\n",
    "# closer look at model config\n",
    "print(\"Model components:\")\n",
    "for i, module in enumerate(model):\n",
    "    print(f\"{i}. {type(module).__name__}\\n\")\n",
    "    if 'auto_model' in dir(module):\n",
    "        display(module.auto_model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa4642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Qwen/Qwen3-Embedding-0.6B\n",
      "Embedding vocabulary size: 151,669 tokens; tokenizer vocab size: 151,669 tokens\n",
      "Each token embedding dimension: 1024\n",
      "Max context size: 32768 input tokens; tokenizer max length: 131072 sequences\n",
      "Total parameters: 595,776,512 parameters\n",
      "Estimated model size: 2.22 GB (float32)\n",
      "\n",
      "Model: google/embeddinggemma-300m\n",
      "Embedding vocabulary size: 262,144 tokens; tokenizer vocab size: 262,145 tokens\n",
      "Each token embedding dimension: 768\n",
      "Max context size: 2048 input tokens; tokenizer max length: 2048 sequences\n",
      "Total parameters: 302,863,104 parameters\n",
      "Estimated model size: 1.13 GB (float32)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "for model_name in [\"Qwen/Qwen3-Embedding-0.6B\", \"google/embeddinggemma-300m\"]:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    # The model has an embedding layer with vectors for each token in vocabulary\n",
    "    vocab_size = model[0].auto_model.config.vocab_size\n",
    "    embedding_dim = model[0].auto_model.config.hidden_size\n",
    "    max_context_size = model[0].auto_model.config.max_position_embeddings\n",
    "    # check related tokenizer info: can have minimal differences for special tokens like padding, reserved tokens, etc.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer_vocab_size = len(tokenizer)\n",
    "    print(f\"Embedding vocabulary size: {vocab_size:,}s; tokenizer vocab size: {len(tokenizer.get_vocab()):,}\")\n",
    "    print(f\"Each token embedding dimension: {embedding_dim}\")\n",
    "    print(f\"Max context size: {max_context_size} input tokens; tokenizer max length: {tokenizer.model_max_length} sequences\")\n",
    "    print(f\"Total parameters: {model[0].auto_model.num_parameters():,} parameters\")\n",
    "    print(f\"Estimated model size: {model[0].auto_model.num_parameters() * 4 / (1024**3):.2f} GB (float32)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8340f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Qwen/Qwen3-Embedding-0.6B\n",
      "Short text token count: 3; decoded: Hello world<|endoftext|>\n",
      "Long text token count: 10001 (before truncation, if needed)\n",
      "Long text will be truncated to: 32768 tokens\n",
      "\n",
      "embeddings preserve always its output dimension: (2, 1024)\n",
      "embedding 0 dimension: (1024,)\n",
      "embedding 1 dimension: (1024,)\n",
      "\n",
      "Model: google/embeddinggemma-300m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10002 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short text token count: 4; decoded: <bos>Hello world<eos>\n",
      "Long text token count: 10002 (before truncation, if needed)\n",
      "Long text will be truncated to: 2048 tokens\n",
      "\n",
      "embeddings preserve always its output dimension: (2, 768)\n",
      "embedding 0 dimension: (768,)\n",
      "embedding 1 dimension: (768,)\n"
     ]
    }
   ],
   "source": [
    "# analize max context size impact on embeddings\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "for model_name in [\"Qwen/Qwen3-Embedding-0.6B\", \"google/embeddinggemma-300m\"]:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    max_context_size = model[0].auto_model.config.max_position_embeddings\n",
    "\n",
    "    # test with different length texts\n",
    "    short_text = \"Hello world\"\n",
    "    long_text = \" \".join([\"word\"] * 10000) \n",
    "\n",
    "    # tokenize to see analize actual token count\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    short_tokens = tokenizer.encode(short_text)\n",
    "    long_tokens = tokenizer.encode(long_text)    \n",
    "    print(f\"Short text token count: {len(short_tokens)}; decoded: {tokenizer.decode(short_tokens)}\")\n",
    "    print(f\"Long text token count: {len(long_tokens)} (before truncation, if needed)\")\n",
    "    print(f\"Long text will be truncated to: {max_context_size} tokens\")   \n",
    "\n",
    "    # The model will automatically truncate\n",
    "    embedding = model.encode([short_text, long_text]) # one can be truncated\n",
    "\n",
    "    print(f\"\\nembeddings preserve always its output dimension: {embedding.shape}\")    \n",
    "    for i, emb in enumerate(embedding):\n",
    "        print(f\"embedding {i} dimension: {emb.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464a218",
   "metadata": {},
   "source": [
    "## handling long documents\n",
    "1. Truncation (default): simple but loses information -> 1 embedding\n",
    "2. Split into chunks -> n embeddings\n",
    "3. Average embeddings (generally works well in search/retrieval) -> 1 embedding\n",
    "4. Weighted average (head/tail chunks more important, e.g., introduction and conclusion, can be used in classification) -> 1 embedding\n",
    "5. Max pooling (take maximum value across all chunks, capturing the most salient features/signals): useful when looking for specific features -> 1 embedding\n",
    "6. Semantic splitting (by sentences/paragraphs) preserves context better -> n embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47147847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7001 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy 1 - Truncation: (768,)\n",
      "âš ï¸ Warning: Information at the end of the document is lost!\n",
      "\n",
      "Strategy 2 - Chunking: Document split into 4 chunks\n",
      "Chunk embeddings shape: (4, 768)\n",
      "âœ“ All informations are preserved\n",
      "\n",
      "Strategy 3 - Averaged document embedding: (768,)\n",
      "âœ“ All parts of the document are represented\n",
      "\n",
      "Strategy 4 - Weighted average: (768,)\n",
      "\n",
      "âœ“ Relevant parts of document are emphasized\n",
      "\n",
      "Strategy 5 - Max pooling: (768,)\n",
      "\n",
      "âœ“ Captures the most salient features across chunks\n",
      "\n",
      "Strategy 5 - Semantic splitting: 3 chunks\n",
      "âœ“ Preserves sentence boundaries\n",
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Original (truncated): [-0.0484592  -0.00667011  0.01031358 -0.00647743 -0.02938237]...\n",
      "Split chunks: [[-0.04752773 -0.00685875  0.01023054 ...  0.05712191 -0.02502624\n",
      "  -0.09391052]\n",
      " [-0.04752773 -0.00685875  0.01023054 ...  0.05712191 -0.02502624\n",
      "  -0.09391052]\n",
      " [-0.04752773 -0.00685875  0.01023054 ...  0.05712191 -0.02502624\n",
      "  -0.09391052]\n",
      " [-0.05410891  0.00279034  0.00609358 ...  0.05738835 -0.02744155\n",
      "  -0.08676634]]...\n",
      "Averaged chunks:      [-0.04917303 -0.00444647  0.0091963  -0.00496957 -0.02934311]...\n",
      "Weighted average:     [-0.04972146 -0.00364238  0.00885155 -0.00441108 -0.02940039]...\n",
      "Max pooling:          [-4.7527730e-02  2.7903426e-03  1.0230541e-02  5.6846544e-05\n",
      " -2.9171295e-02]...\n",
      "Semantic chunks:     [[-0.04845914 -0.00667011  0.0103135  ...  0.05725297 -0.02532502\n",
      "  -0.09378737]\n",
      " [-0.04845914 -0.00667011  0.0103135  ...  0.05725297 -0.02532502\n",
      "  -0.09378737]\n",
      " [-0.04845914 -0.00667011  0.0103135  ...  0.05725297 -0.02532502\n",
      "  -0.09378737]]...\n"
     ]
    }
   ],
   "source": [
    "# Strategy for handling long documents\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"google/embeddinggemma-300m\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_context_size = model[0].auto_model.config.max_position_embeddings\n",
    "\n",
    "# Example: Very long document\n",
    "long_document = \"This is a very long document. \" * 1000\n",
    "\n",
    "# STRATEGY 1: Simple truncation (what model.encode does by default)\n",
    "# Only the first max_context_size tokens are used\n",
    "embedding_truncated = model.encode(long_document)\n",
    "print(f\"Strategy 1 - Truncation: {embedding_truncated.shape}\")\n",
    "print(\"âš ï¸ Warning: Information at the end of the document is lost!\\n\")\n",
    "\n",
    "# STRATEGY 2: Split into chunks and average embeddings\n",
    "def chunk_text_by_tokens(text, tokenizer, max_tokens, overlap=50):\n",
    "    \"\"\"Split text into chunks that fit within max_tokens\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split document into chunks\n",
    "chunks = chunk_text_by_tokens(long_document, tokenizer, max_context_size - 10)  # -10, overlap for special tokens\n",
    "print(f\"Strategy 2 - Chunking: Document split into {len(chunks)} chunks\")\n",
    "\n",
    "# Embed each chunk\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "print(f\"Chunk embeddings shape: {chunk_embeddings.shape}\")\n",
    "print(\"âœ“ All informations are preserved\\n\")\n",
    "\n",
    "# STRATEGY 3: Average embeddings\n",
    "# Average all chunk embeddings to get document embedding\n",
    "doc_embedding_avg = np.mean(chunk_embeddings, axis=0) # arithmetic mean: avg = sum(a) / n\n",
    "print(f\"Strategy 3 - Averaged document embedding: {doc_embedding_avg.shape}\")\n",
    "print(\"âœ“ All parts of the document are represented\\n\")\n",
    "\n",
    "# STRATEGY 4: Weighted average (first/last chunks more important)\n",
    "weights = np.array([2.0] + [1.0] * (len(chunks) - 2) + [2.0])  # More weight to start/end\n",
    "weights = weights / weights.sum()\n",
    "doc_embedding_weighted = np.average(chunk_embeddings, axis=0, weights=weights) \n",
    "print(f\"Strategy 4 - Weighted average: {doc_embedding_weighted.shape}\\n\")\n",
    "print(\"âœ“ Relevant parts of document are emphasized\\n\")\n",
    "\n",
    "# STRATEGY 5: Max pooling (take maximum value across all chunks)\n",
    "doc_embedding_max = np.max(chunk_embeddings, axis=0)\n",
    "print(f\"Strategy 5 - Max pooling: {doc_embedding_max.shape}\\n\")\n",
    "print(\"âœ“ Captures the most salient features across chunks\\n\")\n",
    "\n",
    "# STRATEGY 6: Use semantic splitting (by sentences/paragraphs)\n",
    "def split_by_sentences(text, max_tokens, splitter='. '):\n",
    "    \"\"\"Split text by sentences, grouping to fit max_tokens\"\"\"\n",
    "    sentences = text.split(splitter) # arbitrary sentence splitter\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        \n",
    "        if current_length + sentence_tokens > max_tokens and current_chunk:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "semantic_chunks = split_by_sentences(long_document, max_context_size - 10)\n",
    "print(f\"Strategy 5 - Semantic splitting: {len(semantic_chunks)} chunks\")\n",
    "print(\"âœ“ Preserves sentence boundaries\\n\")\n",
    "\n",
    "# Compare strategies\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original (truncated): {embedding_truncated[:5]}...\")\n",
    "print(f\"Split chunks: {chunk_embeddings[:5]}...\")\n",
    "print(f\"Averaged chunks:      {doc_embedding_avg[:5]}...\")\n",
    "print(f\"Weighted average:     {doc_embedding_weighted[:5]}...\")\n",
    "print(f\"Max pooling:          {doc_embedding_max[:5]}...\")\n",
    "print(f\"Semantic chunks:     {model.encode(semantic_chunks)[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b646142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2201 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 3 chunks\n",
      "Chunk 1 preview: Machine learning is a subset of artificial intelligence. Machine learning is a subset of artificial ...\n",
      "Chunk 2 preview: Pizza is made with tomatoes, cheese, and dough. Pizza is made with tomatoes, cheese, and dough. Pizz...\n",
      "Chunk 3 preview: The solar system has eight planets orbiting the sun. The solar system has eight planets orbiting the...\n",
      "\n",
      "Chunk embeddings shape: (3, 768)\n",
      "  - 3 chunks\n",
      "  - Each chunk has 768 dimensions\n",
      "\n",
      "Are all chunks identical? NO!\n",
      "Chunk 1 embedding (first 5 values): [-0.01150803 -0.02419458 -0.00054021 -0.00617623 -0.02151605]\n",
      "Chunk 2 embedding (first 5 values): [-0.02103425 -0.0354753  -0.01378786 -0.00224774 -0.01337189]\n",
      "Chunk 3 embedding (first 5 values): [ 0.00678735 -0.00432749 -0.02354194  0.00788077 -0.03430591]\n",
      "\n",
      "\n",
      "Similarity between chunk 1 and 2: 0.6268\n",
      "Similarity between chunk 2 and 3: 0.6327\n",
      "Similarity between chunk 1 and 3: 0.6129\n",
      "ðŸ‘‰ Different content = different embeddings!\n",
      "\n",
      "Averaged document embedding shape: (768,)\n",
      "  - Single embedding representing the ENTIRE document\n",
      "  - Still 768 dimensions\n",
      "\n",
      "============================================================\n",
      "WHY AVERAGE?\n",
      "============================================================\n",
      "Option 1: Keep all chunk embeddings separately\n",
      "  Result: (3, 768) - multiple embeddings\n",
      "  Use case: When you want to search within document parts\n",
      "  Example: Find which paragraph answers a question\n",
      "\n",
      "Option 2: Average into one embedding\n",
      "  Result: (768,) - single embedding\n",
      "  Use case: Represent entire document as one vector\n",
      "  Example: Document similarity, clustering, classification\n",
      "\n",
      "============================================================\n",
      "USE CASE COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. SEARCH WITHIN DOCUMENT (keep chunks separate):\n",
      "Query: What is machine learning?\n",
      "Best matching chunk: Chunk 1\n",
      "Similarity: 0.4296\n",
      "Content: Machine learning is a subset of artificial intelligence. Machine learning is a subset of artificial ...\n",
      "\n",
      "2. DOCUMENT-LEVEL SIMILARITY (use averaged embedding):\n",
      "Query: What is machine learning?\n",
      "Document similarity: 0.2697\n",
      "This represents how relevant the ENTIRE document is to the query\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT\n",
      "============================================================\n",
      "Averaging across chunks (axis=0) means:\n",
      "  For each dimension (0-767), take the average value across all chunks\n",
      "  Result: A single vector that captures the 'average meaning' of the document\n",
      "  NOT: Making all chunks identical!\n"
     ]
    }
   ],
   "source": [
    "# compare split on chunks (by sentences) vs averaging\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"google/embeddinggemma-300m\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_context_size = model[0].auto_model.config.max_position_embeddings\n",
    "_repeat = 200\n",
    "# document with clearly different parts\n",
    "long_document = (\n",
    "    \"Machine learning is a subset of artificial intelligence. \" * _repeat + \"###\" +\n",
    "    \"Pizza is made with tomatoes, cheese, and dough. \" * _repeat + \"###\" +\n",
    "    \"The solar system has eight planets orbiting the sun. \" * _repeat\n",
    ")\n",
    "\n",
    "chunks = split_by_sentences(long_document, max_context_size, splitter='###')\n",
    "print(f\"Document split into {len(chunks)} chunks\")\n",
    "print(f\"Chunk 1 preview: {chunks[0][:100]}...\")\n",
    "print(f\"Chunk 2 preview: {chunks[1][:100]}...\")\n",
    "print(f\"Chunk 3 preview: {chunks[2][:100]}...\\n\")\n",
    "\n",
    "# Embed each chunk - EACH CHUNK HAS DIFFERENT CONTENT, SO DIFFERENT EMBEDDINGS!\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "print(f\"Chunk embeddings shape: {chunk_embeddings.shape}\")\n",
    "print(f\"  - {len(chunks)} chunks\")\n",
    "print(f\"  - Each chunk has {chunk_embeddings.shape[1]} dimensions\\n\")\n",
    "\n",
    "# Let's see that chunks are DIFFERENT\n",
    "print(\"Are all chunks identical? NO!\")\n",
    "print(f\"Chunk 1 embedding (first 5 values): {chunk_embeddings[0][:5]}\")\n",
    "print(f\"Chunk 2 embedding (first 5 values): {chunk_embeddings[1][:5]}\")\n",
    "print(f\"Chunk 3 embedding (first 5 values): {chunk_embeddings[2][:5]}\\n\")\n",
    "\n",
    "# Calculate similarity between chunks\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(f\"\\nSimilarity between chunk 1 and 2: {cosine_similarity([chunk_embeddings[0]], [chunk_embeddings[1]])[0][0]:.4f}\")\n",
    "print(f\"Similarity between chunk 2 and 3: {cosine_similarity([chunk_embeddings[1]], [chunk_embeddings[2]])[0][0]:.4f}\")\n",
    "print(f\"Similarity between chunk 1 and 3: {cosine_similarity([chunk_embeddings[0]], [chunk_embeddings[2]])[0][0]:.4f}\")\n",
    "print(\"ðŸ‘‰ Different content = different embeddings!\\n\")\n",
    "\n",
    "# Now average to get ONE embedding for the ENTIRE document\n",
    "doc_embedding_avg = np.mean(chunk_embeddings, axis=0)  # axis=0 means average across chunks\n",
    "print(f\"Averaged document embedding shape: {doc_embedding_avg.shape}\")\n",
    "print(f\"  - Single embedding representing the ENTIRE document\")\n",
    "print(f\"  - Still {doc_embedding_avg.shape[0]} dimensions\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WHY AVERAGE?\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 1: Keep all chunk embeddings separately\")\n",
    "print(f\"  Result: {chunk_embeddings.shape} - multiple embeddings\")\n",
    "print(\"  Use case: When you want to search within document parts\")\n",
    "print(\"  Example: Find which paragraph answers a question\\n\")\n",
    "\n",
    "print(\"Option 2: Average into one embedding\")\n",
    "print(f\"  Result: {doc_embedding_avg.shape} - single embedding\")\n",
    "print(\"  Use case: Represent entire document as one vector\")\n",
    "print(\"  Example: Document similarity, clustering, classification\\n\")\n",
    "\n",
    "# Demonstrate the difference in use cases\n",
    "query = \"What is machine learning?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"USE CASE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use case 1: Search across chunks (keep them separate)\n",
    "print(\"\\n1. SEARCH WITHIN DOCUMENT (keep chunks separate):\")\n",
    "similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "best_chunk_idx = np.argmax(similarities)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Best matching chunk: Chunk {best_chunk_idx + 1}\")\n",
    "print(f\"Similarity: {similarities[best_chunk_idx]:.4f}\")\n",
    "print(f\"Content: {chunks[best_chunk_idx][:100]}...\")\n",
    "\n",
    "# Use case 2: Document-level similarity (use average)\n",
    "print(\"\\n2. DOCUMENT-LEVEL SIMILARITY (use averaged embedding):\")\n",
    "doc_similarity = cosine_similarity([query_embedding], [doc_embedding_avg])[0][0]\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Document similarity: {doc_similarity:.4f}\")\n",
    "print(\"This represents how relevant the ENTIRE document is to the query\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Averaging across chunks (axis=0) means:\")\n",
    "print(\"  For each dimension (0-767), take the average value across all chunks\")\n",
    "print(\"  Result: A single vector that captures the 'average meaning' of the document\")\n",
    "print(\"  NOT: Making all chunks identical!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
