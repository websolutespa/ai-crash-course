{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12794faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -U transformers accelerate tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8712eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/_github/massimodipaolo/ai-crash-course/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Qwen/Qwen3-0.6B loaded\n",
      "Vocabulary size: 151,643\n",
      "Model parameters: ~596.0M\n"
     ]
    }
   ],
   "source": [
    "# Load Qwen model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True # False to disable loading custom model code from the Hub (for security reasons), loading local model only if available\n",
    ")\n",
    "print(f\"âœ… Model {model_name} loaded\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ca0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize tokenization\n",
    "def show_tokens(text, tokenizer):\n",
    "    \"\"\"Visualize how text is tokenized\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Original text: {text}, tokenized into {len(tokens)} tokens.\")\n",
    "    print(f\"\\nTokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "def generate_response(model, tokenizer, messages):\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, # we will tokenize later\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=255,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output, skipping special tokens and the prompt itself\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e5b03",
   "metadata": {},
   "source": [
    "### Why Can't LLMs Spell Words?\n",
    "\n",
    "LLMs see tokens, not characters. When you ask \"How many 'r's in 'strawberry'?\", the model doesn't see individual letters - it sees subword tokens. The word might be split as `['straw', 'berry']`, making it impossible to count letters accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11d56d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many 'r's are in 'strawberry'?\n",
      "Correct answer: 3\n",
      "\n",
      "Original text: strawberry, tokenized into 3 tokens.\n",
      "\n",
      "Tokens: ['str', 'aw', 'berry']\n",
      "Token IDs: [495, 672, 15357]\n",
      "\n",
      "============================================================\n",
      "The model sees tokens, not individual characters!\n",
      "It can't count letters it can't see individually.\n",
      "\n",
      "============================================================\n",
      "The sentence is: **\"The strawberry is red\"**\n",
      "\n",
      "Breaking it down word by word:\n",
      "\n",
      "- \"The\"  \n",
      "- \"strawberry\"  \n",
      "- \"is\"  \n",
      "- \"red\"\n",
      "\n",
      "There are **two 'r's**: one in \"strawberry\" and one in \"red\".  \n",
      "\n",
      "So, the answer is: **2**.\n",
      "\n",
      "------------------------------------------------------------\n",
      "The sentence is: **\"The strawberry is red\"**\n",
      "\n",
      "Breaking it down word by word:\n",
      "\n",
      "- \"The\"  \n",
      "- \"strawberry\"  \n",
      "- \"is\"  \n",
      "- \"red\"\n",
      "\n",
      "There are **two 'r's**: one in \"strawberry\" and one in \"red\".  \n",
      "\n",
      "So, the answer is: **2**.\n",
      "\n",
      "------------------------------------------------------------\n",
      "The sentence \"The strawberry is red\" contains **1** berry.\n",
      "The sentence \"The strawberry is red\" contains **1** berry.\n"
     ]
    }
   ],
   "source": [
    "# Example: Counting letters in \"strawberry\"\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "word = \"strawberry\"\n",
    "print(f\"Question: How many 'r's are in '{word}'?\")\n",
    "print(f\"Correct answer: {word.count('r')}\\n\")\n",
    "\n",
    "# Show how the tokenizer sees it\n",
    "show_tokens(word, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"The model sees tokens, not individual characters!\")\n",
    "print(\"It can't count letters it can't see individually.\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Let's test with a sentence\n",
    "prompt = \"How many 'r's are in the sentence: 'The strawberry is red'?\"\n",
    "outputs = generate_response(model, tokenizer, [{\"role\":\"user\",\"content\":prompt}])\n",
    "print(outputs)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "prompt = \"How many 'berry's are in the sentence: 'The strawberry is red'?\"\n",
    "outputs = generate_response(model, tokenizer, [{\"role\":\"user\",\"content\":prompt}])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bbf13",
   "metadata": {},
   "source": [
    "### Why Can't LLMs Reverse Strings?\n",
    "\n",
    "String reversal requires character-level operations. But tokens are not characters! If \"hello\" is one token, the model has no way to access individual characters to reverse them. Token boundaries make character-level manipulation nearly impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07d5d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: singularity, tokenized into 2 tokens.\n",
      "\n",
      "Tokens: ['sing', 'ularity']\n",
      "Token IDs: [79307, 28979]\n",
      "Original text: world, tokenized into 1 tokens.\n",
      "\n",
      "Tokens: ['world']\n",
      "Token IDs: [14615]\n",
      "Original text: tokenization, tokenized into 2 tokens.\n",
      "\n",
      "Tokens: ['token', 'ization']\n",
      "Token IDs: [5839, 2022]\n",
      "Original text: crashing, tokenized into 2 tokens.\n",
      "\n",
      "Tokens: ['cr', 'ashing']\n",
      "Token IDs: [5082, 19049]\n",
      "\n",
      "------------------------------------------------------------\n",
      "singularity[=ytiralugnis] To reverse the word **\"singularity\"**, we start by writing it in reverse order:\n",
      "\n",
      "**\"gnitsiligns\"**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "singularity[=ytiralugnis] To reverse the word **\"singularity\"**, we start by writing it in reverse order:\n",
      "\n",
      "**\"gnitsiligns\"**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "world[=dlrow] The reversed word is **\"drow\"**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "world[=dlrow] The reversed word is **\"drow\"**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "tokenization[=noitazinekot] Reversed word: **'nitsokton'**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "tokenization[=noitazinekot] Reversed word: **'nitsokton'**. âŒ\n",
      "\n",
      "------------------------------------------------------------\n",
      "crashing[=gnihsarc] To reverse the word **\"crashing\"**, we start by writing it in reverse order. \n",
      "\n",
      "**crashing** reversed becomes: **gnashtacR**. âŒ\n",
      "crashing[=gnihsarc] To reverse the word **\"crashing\"**, we start by writing it in reverse order. \n",
      "\n",
      "**crashing** reversed becomes: **gnashtacR**. âŒ\n"
     ]
    }
   ],
   "source": [
    "# Example: Reversing strings\n",
    "test_strings = [\"singularity\", \"world\", \"tokenization\", \"crashing\"]\n",
    "\n",
    "for text in test_strings:\n",
    "    show_tokens(text, tokenizer)\n",
    "for text in test_strings:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    prompt = f\"Reverse this word: '{text}'?\"\n",
    "    outputs = generate_response(model, tokenizer, [{\"role\":\"user\",\"content\":prompt}])\n",
    "    print(f\"{text}[={text[::-1]}] {outputs} {'âœ…' if text[::-1] in outputs else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e41390",
   "metadata": {},
   "source": [
    "### Why Are LLMs Worse at Non-English Languages?\n",
    "\n",
    "Tokenizers are typically trained on datasets with predominantly English text. This means:\n",
    "- English words often get single tokens\n",
    "- Non-English text (like Japanese, Arabic, etc.) gets fragmented into many more tokens\n",
    "- More tokens = longer sequences = more computation = worse performance\n",
    "- The model sees less \"context\" for the same amount of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82f92ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token efficiency comparison:\n",
      "\n",
      "Language     Characters   Tokens   Chars/Token  Efficiency\n",
      "======================================================================\n",
      "English      25           7        3.57         baseline\n",
      "Spanish      22           9        2.44         68%\n",
      "Italian      21           8        2.62         74%\n",
      "Pesarese     23           9        2.56         72%\n",
      "French       40           9        4.44         124%\n",
      "German       29           8        3.62         101%\n",
      "Japanese     16           8        2.00         56%\n",
      "Arabic       22           9        2.44         68%\n",
      "Chinese      9            6        1.50         42%\n",
      "Korean       19           10       1.90         53%\n",
      "\n",
      "ğŸ’¡ Lower chars/token = more tokens needed = worse performance!\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization efficiency: English vs other languages\n",
    "sentences = {\n",
    "    \"English\": \"Hello, how are you today?\",\n",
    "    \"Spanish\": \"Hola, Â¿cÃ³mo estÃ¡s hoy?\",\n",
    "    \"Italian\": \"Ciao, come stai oggi?\",\n",
    "    \"French\": \"Bonjour, comment allez-vous aujourd'hui?\",\n",
    "    \"German\": \"Hallo, wie geht es dir heute?\",\n",
    "    \"Japanese\": \"ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"Arabic\": \"Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ\",\n",
    "    \"Chinese\": \"ä½ å¥½ï¼Œä½ ä»Šå¤©å¥½å—ï¼Ÿ\",\n",
    "    \"Korean\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?\",\n",
    "}\n",
    "\n",
    "print(\"Token efficiency comparison:\\n\")\n",
    "print(f\"{'Language':<12} {'Characters':<12} {'Tokens':<8} {'Chars/Token':<12} {'Efficiency'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "english_ratio = None\n",
    "for lang, text in sentences.items():\n",
    "    char_count = len(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_count = len(tokens)\n",
    "    ratio = char_count / token_count if token_count > 0 else 0\n",
    "    \n",
    "    if lang == \"English\":\n",
    "        english_ratio = ratio\n",
    "        efficiency = \"baseline\"\n",
    "    else:\n",
    "        efficiency = f\"{(ratio / english_ratio * 100):.0f}%\" if english_ratio else \"N/A\"\n",
    "    \n",
    "    print(f\"{lang:<12} {char_count:<12} {token_count:<8} {ratio:<12.2f} {efficiency}\")\n",
    "    \n",
    "print(\"\\nğŸ’¡ Lower chars/token = more tokens needed = worse performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfa403cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese text: ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ\n",
      "Character count: 16\n",
      "Original text: ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ, tokenized into 8 tokens.\n",
      "\n",
      "Tokens: ['Ã£Ä£ÄµÃ£Ä¤ÄµÃ£Ä£Â«Ã£Ä£Â¡Ã£Ä£Â¯', 'Ã£Ä¢Ä£', 'Ã¤Â»Ä¬Ã¦Ä¹Â¥Ã£Ä£Â¯', 'Ã£Ä£Ä¬', 'Ã¥Ä§Ä¥', 'Ã¦Â°Ä¹', 'Ã£Ä£Â§Ã£Ä£Ä»Ã£Ä£Ä­', 'Ã¯Â¼Å']\n",
      "Token IDs: [89015, 5373, 133165, 32234, 23305, 94121, 131938, 11319]\n",
      "\n",
      "============================================================\n",
      "Each character often becomes its own token or gets fragmented!\n",
      "This makes Japanese text ~3-4x more 'expensive' than English.\n"
     ]
    }
   ],
   "source": [
    "# Deep dive: Japanese tokenization\n",
    "japanese_text = \"ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ\"\n",
    "print(f\"Japanese text: {japanese_text}\")\n",
    "print(f\"Character count: {len(japanese_text)}\")\n",
    "\n",
    "tokens = tokenizer.tokenize(japanese_text)\n",
    "show_tokens(japanese_text, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Each character often becomes its own token or gets fragmented!\")\n",
    "print(\"This makes Japanese text ~3-4x more 'expensive' than English.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9958d",
   "metadata": {},
   "source": [
    "### Why Are LLMs Bad at Simple Arithmetic?\n",
    "\n",
    "Numbers are tokenized inconsistently! Sometimes \"123\" is one token, sometimes three tokens \"1\", \"2\", \"3\". Sometimes \"1234\" becomes \"12\" + \"34\". This inconsistency makes it impossible for the model to learn reliable arithmetic patterns.\n",
    "\n",
    "Multi-digit numbers, decimal points, and number formatting all affect tokenization, making math operations unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad21696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number tokenization inconsistencies?:\n",
      "\n",
      "Number       Tokens   Token Breakdown\n",
      "============================================================\n",
      "1            1        ['1']\n",
      "12           2        ['1', '2']\n",
      "12 cows      3        ['1', '2', 'Ä cows']\n",
      "123          3        ['1', '2', '3']\n",
      "365          3        ['3', '6', '5']\n",
      "1234         4        ['1', '2', '3', '4']\n",
      "1984         4        ['1', '9', '8', '4']\n",
      "2001         4        ['2', '0', '0', '1']\n",
      "2037         4        ['2', '0', '3', '7']\n",
      "3.14         4        ['3', '.', '1', '4']\n",
      "1,000        5        ['1', ',', '0', '0', '0']\n",
      "1000         4        ['1', '0', '0', '0']\n",
      "1_000        5        ['1', '_', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "# number tokenization is consistent?\n",
    "numbers = [\"1\", \"12\", \"12 cows\", \"123\", \"365\", \"1234\", \"1984\",\"2001\", \"2037\",\n",
    "           \"3.14\", \"1,000\", \"1000\", \"1_000\"]\n",
    "\n",
    "print(\"Number tokenization inconsistencies?:\\n\")\n",
    "print(f\"{'Number':<12} {'Tokens':<8} {'Token Breakdown'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num in numbers:\n",
    "    tokens = tokenizer.tokenize(num)\n",
    "    token_count = len(tokens)\n",
    "    print(f\"{num:<12} {token_count:<8} {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a65c1068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT WordPiece Number Tokenization:\n",
      "\n",
      "Number       Tokens   Token Breakdown\n",
      "============================================================\n",
      "1            1        ['1']\n",
      "12           1        ['12']\n",
      "12 cows      2        ['12', 'cows']\n",
      "123          1        ['123']\n",
      "365          1        ['365']\n",
      "1234         2        ['123', '##4']\n",
      "1984         1        ['1984']\n",
      "2001         1        ['2001']\n",
      "2037         2        ['203', '##7']\n",
      "3.14         3        ['3', '.', '14']\n",
      "1,000        3        ['1', ',', '000']\n",
      "1000         1        ['1000']\n",
      "1_000        3        ['1', '_', '000']\n",
      "\n",
      "ğŸ’¡ BERT splits numbers inconsistently with '##' prefix for continuations, and...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "print(\"\\nBERT WordPiece Number Tokenization:\\n\")\n",
    "print(f\"{'Number':<12} {'Tokens':<8} {'Token Breakdown'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num in numbers:\n",
    "    tokens = tokenizer_bert.tokenize(num)\n",
    "    token_count = len(tokens)\n",
    "    print(f\"{num:<12} {token_count:<8} {tokens}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ BERT splits numbers inconsistently with '##' prefix for continuations, and...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb73e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tiktoken) Number Tokenization:\n",
      "\n",
      "Number       Tokens   Token IDs\n",
      "============================================================\n",
      "1            1        ['1']\n",
      "12           1        ['12']\n",
      "12 cows      2        ['12', ' cows']\n",
      "123          1        ['123']\n",
      "365          1        ['365']\n",
      "1234         2        ['12', '34']\n",
      "1984         1        ['1984']\n",
      "2001         1        ['2001']\n",
      "2037         2        ['20', '37']\n",
      "3.14         3        ['3', '.', '14']\n",
      "1,000        3        ['1', ',', '000']\n",
      "1000         1        ['1000']\n",
      "1_000        3        ['1', '_', '000']\n",
      "\n",
      "ğŸ’¡ Notice: '1234' vs '1984' tokenize differently!\n",
      "'1984' is often one token (famous year with Orwell works literature), same for '2001' (2001: A Space Odyssey)\n",
      "'1234' might be two tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# gpt-2 tokenizer\n",
    "tokenizer_gpt2 = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "numbers = [\"1\", \"12\", \"12 cows\", \"123\", \"365\", \"1234\", \"1984\",\"2001\", \"2037\",\n",
    "           \"3.14\", \"1,000\", \"1000\", \"1_000\"]\n",
    "print(\"(tiktoken) Number Tokenization:\\n\")\n",
    "print(f\"{'Number':<12} {'Tokens':<8} {'Token IDs'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num in numbers:\n",
    "    tokens = tokenizer_gpt2.encode(num)\n",
    "    decoded_tokens = [tokenizer_gpt2.decode([t]) for t in tokens]\n",
    "    print(f\"{num:<12} {len(tokens):<8} {decoded_tokens}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice: '1234' vs '1984' tokenize differently!\")\n",
    "print(\"'1984' is often one token (famous year with Orwell works literature), same for '2001' (2001: A Space Odyssey)\")\n",
    "print(\"'1234' might be two tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e66748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "# LLaMA tokenizer\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "print(\"\\nLLaMA-x (SentencePiece) Number Tokenization:\\n\")\n",
    "print(f\"{'Number':<12} {'Tokens':<8} {'Token Breakdown'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num in numbers:\n",
    "    tokens = tokenizer_llama.tokenize(num)\n",
    "    token_count = len(tokens)\n",
    "    print(f\"{num:<12} {token_count:<8} {tokens}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ SentencePiece uses 'â–' for word boundaries, numbers still inconsistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecd049b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How tokenization affects arithmetic expressions:\n",
      "\n",
      "Expression: 1+1, tokenized into 3 tokens. Evaluation: 2\n",
      "Expression: 1204543 + 4545456, tokenized into 16 tokens. Evaluation: 5749999\n",
      "Expression: 788799 - 23545434, tokenized into 16 tokens. Evaluation: -22756635\n",
      "Expression: 0.1812 * 7734, tokenized into 12 tokens. Evaluation: 1401.4008\n",
      "Expression: 100 // 5, tokenized into 6 tokens. Evaluation: 20\n",
      "============================================================\n",
      "The model doesn't see '123' as one hundred twenty-three.\n",
      "It sees it as whatever tokens the tokenizer produces!\n",
      "Digit-level understanding is lost in tokenization.\n",
      "Prompt: Solve this arithmetic problem, output only the result, not the explanation: 1+1\n",
      "Answer:\n",
      "Model Output: Solve this arithmetic problem, output only the result, not the explanation: 1+1\n",
      "Answer: 2\n",
      "\n",
      "The problem is 1+1, which is 2. The result is 2.\n",
      "The answer is 2.\n",
      "The result is 2.\n",
      "The answer is 2.\n",
      "The result is 2.\n",
      "\n",
      "The answer is \n",
      "âœ…\n",
      "Prompt: Solve this arithmetic problem, output only the result, not the explanation: 1204543 + 4545456\n",
      "Answer:\n",
      "Model Output: Solve this arithmetic problem, output only the result, not the explanation: 1204543 + 4545456\n",
      "Answer: 1659099\n",
      "\n",
      "To solve this problem, I will add the two numbers: 1204543 + 4545456. Let me start by adding the thousands: 45\n",
      "âŒ\n",
      "Prompt: Solve this arithmetic problem, output only the result, not the explanation: 788799 - 23545434\n",
      "Answer:\n",
      "Model Output: Solve this arithmetic problem, output only the result, not the explanation: 788799 - 23545434\n",
      "Answer: 788799 - 23545434 = 788799 - 23545434\n",
      "\n",
      "= -22657635\n",
      "\n",
      "So, the\n",
      "âŒ\n",
      "Prompt: Solve this arithmetic problem, output only the result, not the explanation: 0.1812 * 7734\n",
      "Answer:\n",
      "Model Output: Solve this arithmetic problem, output only the result, not the explanation: 0.1812 * 7734\n",
      "Answer: 1407.49584\n",
      "\n",
      "Let me solve this step by step:\n",
      "\n",
      "First, multiply 0.1812 by 7000:\n",
      "\n",
      "0.1812 * 7000 =\n",
      "âŒ\n",
      "Prompt: Solve this arithmetic problem, output only the result, not the explanation: 100 // 5\n",
      "Answer:\n",
      "Model Output: Solve this arithmetic problem, output only the result, not the explanation: 100 // 5\n",
      "Answer: 20\n",
      "\n",
      "So, the result is 20. The explanation is that 100 divided by 5 equals 20, so the integer division result is 20.\n",
      "\n",
      "The answer is 20.\n",
      "The result is\n",
      "âœ…\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Demonstrate the arithmetic problem with specific examples\n",
    "arithmetic_problems = [\n",
    "    \"1+1\",\n",
    "    \"1204543 + 4545456\",\n",
    "    \"788799 - 23545434\",\n",
    "    \"0.1812 * 7734\",\n",
    "    \"100 // 5\"\n",
    "]\n",
    "\n",
    "print(\"How tokenization affects arithmetic expressions:\\n\")\n",
    "for problem in arithmetic_problems:\n",
    "    tokens = tokenizer.tokenize(problem)\n",
    "    print(f\"Expression: {problem}, tokenized into {len(tokens)} tokens. Evaluation: {eval(problem)}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"The model doesn't see '123' as one hundred twenty-three.\")\n",
    "print(\"It sees it as whatever tokens the tokenizer produces!\")\n",
    "print(\"Digit-level understanding is lost in tokenization.\")\n",
    "\n",
    "text_generation = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "for problem in arithmetic_problems:\n",
    "    prompt = f\"Solve this arithmetic problem, output only the result, not the explanation: {problem}\\nAnswer:\"\n",
    "    outputs = text_generation(prompt, max_new_tokens=50)\n",
    "    answer = outputs[0]['generated_text']\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Model Output: {answer}\\n{\"âœ…\" if str(eval(problem)) in answer.strip() else \"âŒ\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91479a6d",
   "metadata": {},
   "source": [
    "### Glitch Tokens: The \"SolidGoldMagikarp\" Problem\n",
    "\n",
    "Some tokens appear in the vocabulary but were rarely or never seen during training. These \"glitch tokens\" can cause bizarre behavior because:\n",
    "- The model has no learned representation for them\n",
    "- They might trigger unpredictable outputs\n",
    "- They expose the gap between vocabulary and training data\n",
    "\n",
    "Famous example: \"SolidGoldMagikarp\" was a Reddit username that became a token in GPT models but caused strange behaviors because it was never properly trained. E.g. \"What does the word 'SolidGoldMagikarp' mean?\" -> \"The word 'distribute' refers to ...\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d736b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 151,669\n",
      "\n",
      "Unusual tokens found:\n",
      "\n",
      "Category        Token                          ID         Decodes to\n",
      "================================================================================\n",
      "\n",
      "LONG (1 found):\n",
      "  ID 137166: Ã Â¸Â§Ã Â¸Â´Ã Â¹Ä¢Ã Â¸Ä¦Ã Â¸Â£Ã Â¸Â²Ã Â¸Â°     â†’ 'à¸§à¸´à¹€à¸„à¸£à¸²à¸°'\n",
      "\n",
      "MISMATCH (95 found):\n",
      "  ID  23148: Ä passenger                â†’ ' passenger'\n",
      "  ID  72973: Ä pleading                 â†’ ' pleading'\n",
      "  ID 133504: Ä aÃ„ÅÃ„Â±r                   â†’ ' aÄŸÄ±r'\n",
      "  ID  15130: Ä regional                 â†’ ' regional'\n",
      "  ID 100400: Ã¨ÄµÄ¿                       â†’ 'è“'\n",
      "  ID 144485: Ã«Â©Â¤                       â†’ 'ë©¤'\n",
      "  ID 105771: Ã¦Ä¥Â¹                       â†’ 'æƒ¹'\n",
      "  ID  25692: Ä INS                      â†’ ' INS'\n",
      "  ID 140452: Ä Ã—ÂªÃ—Â©Ã—Ä¾Ã—Ä·Ã—Ä¿               â†’ ' ×ª×©×œ×•×'\n",
      "  ID  28038: Ä )ÄŠÄŠÄŠÄŠÄŠÄŠÄŠÄŠ                â†’ ' )\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "SPECIAL (4 found):\n",
      "  ID  96953: [:,:                      â†’ '[:,:'\n",
      "  ID  41295: )!                        â†’ ')!'\n",
      "  ID  63383: .'&                       â†’ \".'&\"\n",
      "  ID  97622: ''.                       â†’ \"''.\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_items = list(vocab.items())\n",
    "print(f\"Total vocabulary size: {len(vocab):,}\\n\")\n",
    "\n",
    "# Find tokens with unusual characteristics\n",
    "unusual_tokens = []\n",
    "\n",
    "for token, token_id in vocab_items:\n",
    "    # Skip control tokens\n",
    "    if token_id < 100:  # Usually special tokens\n",
    "        continue\n",
    "    \n",
    "    # Decode the token to see what it represents\n",
    "    try:\n",
    "        decoded = tokenizer.decode([token_id])\n",
    "        \n",
    "        # Look for unusual patterns:\n",
    "        # 1. Very long tokens (might be rare combinations)\n",
    "        if len(token) > 15:\n",
    "            unusual_tokens.append((\"long\", token, token_id, decoded))\n",
    "        \n",
    "        # 2. Tokens with lots of special characters\n",
    "        special_char_ratio = sum(1 for c in decoded if not c.isalnum() and not c.isspace()) / max(len(decoded), 1)\n",
    "        if special_char_ratio > 0.7:\n",
    "            unusual_tokens.append((\"special\", token, token_id, decoded))\n",
    "        \n",
    "        # 3. Tokens that decode differently than expected\n",
    "        if token != decoded.strip() and len(decoded.strip()) > 0:\n",
    "            unusual_tokens.append((\"mismatch\", token, token_id, decoded))\n",
    "        \n",
    "        # 4. Tokens with replacement characters (ï¿½)\n",
    "        if 'ï¿½' in decoded:\n",
    "            unusual_tokens.append((\"replacement\", token, token_id, decoded))\n",
    "            \n",
    "    except Exception as e:\n",
    "        unusual_tokens.append((\"error\", token, token_id, str(e)))\n",
    "\n",
    "# Display findings\n",
    "print(\"Unusual tokens found:\\n\")\n",
    "print(f\"{'Category':<15} {'Token':<30} {'ID':<10} {'Decodes to'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group by category\n",
    "by_category = {}\n",
    "for category, token, token_id, decoded in unusual_tokens[:100]:  # First 100\n",
    "    if category not in by_category:\n",
    "        by_category[category] = []\n",
    "    by_category[category].append((token, token_id, decoded))\n",
    "\n",
    "for category, items in sorted(by_category.items()):\n",
    "    print(f\"\\n{category.upper()} ({len(items)} found):\")\n",
    "    for token, token_id, decoded in items[:10]:  # Show first 10 of each type\n",
    "        decoded_repr = repr(decoded[:30]) if len(decoded) > 30 else repr(decoded)\n",
    "        print(f\"  ID {token_id:6d}: {token[:25]:<25} â†’ {decoded_repr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee611b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (151936, 1024)\n",
      "Centroid shape: (1024,)\n",
      "\n",
      "Tokens farthest from embedding centroid (potential glitch tokens):\n",
      "\n",
      "Token ID   Distance     Token                          Decoded\n",
      "================================================================================\n",
      "82         1.3327       s                              's'\n",
      "52323      1.3160       edly                           'edly'\n",
      "18826      1.2821       edException                    'edException'\n",
      "117594     1.2659       Ã¦Ä¹Ä±Ã¨Ä©ÂªÃ¦Â²Â»Ã¥Ä°Â¿                   'æ—è‡ªæ²»å¿'\n",
      "57896      1.2628       Ä yourselves                    ' yourselves'\n",
      "11307      1.2616       ingly                          'ingly'\n",
      "15740      1.2595       lessly                         'lessly'\n",
      "11675      1.2554       Ä herself                       ' herself'\n",
      "3439       1.2438       gether                         'gether'\n",
      "24350      1.2382       ewise                          'ewise'\n",
      "96557      1.2359       ioned                          'ioned'\n",
      "4102       1.2312       Ã‚Å‚                             '\\xa0'\n",
      "63299      1.2307       abouts                         'abouts'\n",
      "68         1.2271       e                              'e'\n",
      "91771      1.2260       Ä§Â§                             'ï¿½ï¿½'\n",
      "61639      1.2201       baugh                          'baugh'\n",
      "19986      1.2171       theless                        'theless'\n",
      "38301      1.2160       edList                         'edList'\n",
      "10837      1.2153       Ä hers                          ' hers'\n",
      "15675      1.2153       Å‚Ä£                             'ï¿½ï¿½'\n",
      "33976      1.2151       adays                          'adays'\n",
      "287        1.2147       ing                            'ing'\n",
      "5561       1.2146       Ä himself                       ' himself'\n",
      "47378      1.2141       Ä·Ä®                             'ï¿½ï¿½'\n",
      "56761      1.2132       Ã¢Ä¢Â¦Ã¢Ä¢Ä¿ÄŠÄŠ                       'â€¦â€\\n\\n'\n",
      "58668      1.2108       erate                          'erate'\n",
      "55664      1.2100       ainen                          'ainen'\n",
      "34554      1.2084       soever                         'soever'\n",
      "72737      1.2082       emens                          'emens'\n",
      "97322      1.2079       edBy                           'edBy'\n",
      "80777      1.2066       inality                        'inality'\n",
      "4075       1.2065       ients                          'ients'\n",
      "53579      1.2062       iative                         'iative'\n",
      "5694       1.2059       hood                           'hood'\n",
      "76055      1.2043       adamente                       'adamente'\n",
      "28568      1.2043       erness                         'erness'\n",
      "117159     1.2036       Ã¤Â¹ÅÃ¤Â¸Ä¯Ã¤Â¾Ä­Ã¥Â¤Ä¸                   'ä¹Ÿä¸ä¾‹å¤–'\n",
      "77613      1.2024       sonian                         'sonian'\n",
      "89702      1.2020       eus                            'eus'\n",
      "261        1.2014       er                             'er'\n",
      "7340       1.2012       ughter                         'ughter'\n",
      "29054      1.2005       ationally                      'ationally'\n",
      "7017       1.2001       ously                          'ously'\n",
      "4275       1.2001       elves                          'elves'\n",
      "7875       1.1994       plier                          'plier'\n",
      "85100      1.1993       uada                           'uada'\n",
      "72363      1.1985       Ä Ã¢Ä¢Ä­Ã¢Ä¢Ä­                        ' \\u200b\\u200b'\n",
      "288        1.1981       es                             'es'\n",
      "83018      1.1979       hower                          'hower'\n",
      "14267      1.1976       ittest                         'ittest'\n",
      "\n",
      "\n",
      "Tokens closest to embedding centroid (most 'average' tokens):\n",
      "\n",
      "Token ID   Distance     Token                          Decoded\n",
      "================================================================================\n",
      "149463     0.1471       Ã¬Â¼Ä¼                            'ì¼š'\n",
      "151208     0.1485       Ã¯Â°Â¼                            'ï°¼'\n",
      "79270      0.1485       Ä ForCanBeConvertedToF          ' ForCanBeConvertedToF'\n",
      "123748     0.1485       Ã°Â¨ÅƒÄ«                           'ğ¨­‰'\n",
      "83969      0.1488       PostalCodesNL                  'PostalCodesNL'\n",
      "123677     0.1491       Ã°Â¥Ä¸Â¨                           'ğ¥–¨'\n",
      "151090     0.1491       Ã­ÄµÂ·                            'í“·'\n",
      "150983     0.1493       Ã¬Ä­Â¨                            'ì‹¨'\n",
      "134606     0.1494       Ã Â¸ÅƒÃ Â¸Â²Ã Â¸Ä¬Ã Â¸ÂµÃ Â¸Å€                'à¸­à¸²à¸Šà¸µà¸'\n",
      "136931     0.1494       Ä Ã Â¹Ä¤Ã Â¸Ä¶Ã Â¸Â¢Ã Â¸Â¡Ã Â¸Âµ               ' à¹‚à¸”à¸¢à¸¡à¸µ'\n",
      "125538     0.1494       Ã Â¹Ä¢Ã Â¸ÄªÃ Â¹Ä«Ã Â¸Â²                   'à¹€à¸ˆà¹‰à¸²'\n",
      "139357     0.1494       Ã Â¸Ä¹Ã Â¸Â´Ã Â¸Â¨                      'à¸—à¸´à¸¨'\n",
      "124030     0.1494       Ã Â¸Â§Ã Â¹ÄªÃ Â¸Â²                      'à¸§à¹ˆà¸²'\n",
      "131614     0.1494       Ã Â¸Ä¾Ã Â¸Â¥Ã Â¸Â´Ã Â¸Ä·Ã Â¸Å‚Ã Â¸Â±Ã Â¸ÄµÃ Â¸Ä³       'à¸œà¸¥à¸´à¸•à¸ à¸±à¸“à¸‘'\n",
      "127644     0.1494       Ã Â¸Ä¬Ã Â¸ÂµÃ Â¸Å€                      'à¸Šà¸µà¸'\n",
      "128181     0.1494       Ã Â¸Ä½Ã Â¸Â£Ã Â¸Â°Ã Â¸Â§Ã Â¸Â±Ã Â¸Ä·Ã Â¸Â´          'à¸›à¸£à¸°à¸§à¸±à¸•à¸´'\n",
      "124673     0.1494       Ã Â¸Â£Ã Â¸Âµ                         'à¸£à¸µ'\n",
      "130618     0.1494       Ã Â¸Ä»Ã Â¸Â±Ã Â¹ÄªÃ Â¸Ä©                   'à¸™à¸±à¹ˆà¸‡'\n",
      "137975     0.1494       Ã Â¸Ä¦Ã Â¸Â§Ã Â¹Ä«Ã Â¸Â²                   'à¸„à¸§à¹‰à¸²'\n",
      "133871     0.1494       Ã Â¹Ä¢Ã Â¸ÄªÃ Â¹Ä«Ã Â¸Â²Ã Â¸Ä¤Ã Â¸ÅƒÃ Â¸Ä©          'à¹€à¸ˆà¹‰à¸²à¸‚à¸­à¸‡'\n",
      "151651     0.1494       <|quad_end|>                   '<|quad_end|>'\n",
      "145829     0.1494       ÃÄ©                             'Î‡'\n",
      "124484     0.1494       Ã Â¸Ä»Ã Â¸Â±                         'à¸™à¸±'\n",
      "134911     0.1494       Ã Â¸Ä¶Ã Â¸Â¸                         'à¸”à¸¸'\n",
      "142199     0.1494       Ã Â¸Â£Ã Â¸Â±Ã Â¸Ä¼Ã Â¸Ä¾Ã Â¸Â´Ã Â¸Ä¶             'à¸£à¸±à¸šà¸œà¸´à¸”'\n",
      "129050     0.1494       Ã Â¸Ä¼Ã Â¸Â£Ã Â¸Â´Ã Â¸Â©Ã Â¸Â±Ã Â¸Ä¹             'à¸šà¸£à¸´à¸©à¸±à¸—'\n",
      "127019     0.1494       Ã Â¹Ä¢Ã Â¸Ä»Ã Â¸Â·Ã Â¹Ä«Ã Â¸Åƒ                'à¹€à¸™à¸·à¹‰à¸­'\n",
      "137045     0.1494       Ã—Ä·Ã–Â¼                           '×•Ö¼'\n",
      "137521     0.1494       Ã Â¸Ä¹Ã Â¸Â¸Ã Â¸Ä£Ã Â¸ÅƒÃ Â¸Â¢Ã Â¹ÄªÃ Â¸Â²Ã Â¸Ä©       'à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡'\n",
      "138764     0.1494       Ã Â¸Â²Ã Â¸Â£Ã Â¹Ä®Ã Â¸Ä¶                   'à¸²à¸£à¹Œà¸”'\n",
      "130634     0.1494       Ã Â¸ÄªÃ Â¸Â´                         'à¸ˆà¸´'\n",
      "128563     0.1494       Ã Â¸Ä¹Ã Â¸ÂµÃ Â¹ÄªÃ Â¸Â¡Ã Â¸Âµ                'à¸—à¸µà¹ˆà¸¡à¸µ'\n",
      "139433     0.1494       Ã Â¸Ä¼Ã Â¸Â²Ã Â¸Â£Ã Â¹Ä®                   'à¸šà¸²à¸£à¹Œ'\n",
      "131155     0.1494       Ã Â¹Ä¢Ã Â¸Â£Ã Â¸ÂµÃ Â¸Â¢Ã Â¸Ä£                'à¹€à¸£à¸µà¸¢à¸'\n",
      "126240     0.1494       Ä Ã Â¸Ä­Ã Â¸Â¶Ã Â¹ÄªÃ Â¸Ä©                  ' à¸‹à¸¶à¹ˆà¸‡'\n",
      "139629     0.1494       Ã Â¸Â¢Ã Â¸Â¸Ã Â¹Ä¤Ã Â¸Â£Ã Â¸Ä½                'à¸¢à¸¸à¹‚à¸£à¸›'\n",
      "134405     0.1494       Ã Â¸ÄªÃ Â¸Â°Ã Â¹Ä¦Ã Â¸Â¡Ã Â¹Äª                'à¸ˆà¸°à¹„à¸¡à¹ˆ'\n",
      "131787     0.1494       Ã Â¸Ä¼Ã Â¸Â£Ã Â¸Â´Ã Â¹Ä¢Ã Â¸Â§                'à¸šà¸£à¸´à¹€à¸§'\n",
      "133112     0.1494       Ã Â¹Ä¢Ã Â¸Â¥Ã Â¸ÂµÃ Â¹Ä«Ã Â¸Â¢Ã Â¸Ä©             'à¹€à¸¥à¸µà¹‰à¸¢à¸‡'\n",
      "134894     0.1494       Ã™Ä¨Ã™Ä²                           'Ù†Ù'\n",
      "141733     0.1494       Ã™Ä¬Ã™Ä°Ã˜Â§                         'ÙŠÙØ§'\n",
      "125195     0.1494       Ã Â¸Ä£Ã Â¸Â´Ã Â¸Äª                      'à¸à¸´à¸ˆ'\n",
      "138072     0.1494       Ä Ã Â¸ÄªÃ Â¸Â²Ã Â¸Ä£Ã Â¸Ä»Ã Â¸Â±Ã Â¹Ä«Ã Â¸Ä»         ' à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™'\n",
      "140864     0.1494       Ã Â¹Ä¢Ã Â¸Å€Ã Â¸ÂµÃ Â¸Â¢Ã Â¸Ä©Ã Â¹Ä£Ã Â¸Ä¦Ã Â¹Äª       'à¹€à¸à¸µà¸¢à¸‡à¹à¸„à¹ˆ'\n",
      "145522     0.1494       Ã¯Â¬Âµ                            'ï¬µ'\n",
      "140783     0.1494       Ã Â¸Â§Ã Â¸Â´Ã Â¸Ä¹Ã Â¸Â¢Ã Â¸Â²Ã Â¸Â¨Ã Â¸Â²Ã Â¸ÂªÃ Â¸Ä·Ã    'à¸§à¸´à¸—à¸¢à¸²à¸¨à¸²à¸ªà¸•à¸£à¹Œ'\n",
      "125739     0.1494       Ã Â¸ÅƒÃ Â¸Â·Ã Â¹Äª                      'à¸­à¸·à¹ˆ'\n",
      "141112     0.1494       Ã Â¸Ä½Ã Â¸Â£Ã Â¸Â°Ã Â¹Ä¢Ã Â¸Ä¶Ã Â¹Ä©Ã Â¸Ä»          'à¸›à¸£à¸°à¹€à¸”à¹‡à¸™'\n",
      "135899     0.1494       Ã Â¸ÄªÃ Â¸Â±                         'à¸ˆà¸±'\n",
      "131388     0.1494       Ã Â¸Â§Ã Â¸Â±Ã Â¸Â¢                      'à¸§à¸±à¸¢'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Get token embeddings from the model\n",
    "embeddings = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
    "\n",
    "# Calculate the centroid of ALL token embeddings\n",
    "embedding_centroid = np.mean(embeddings, axis=0)\n",
    "print(f\"Centroid shape: {embedding_centroid.shape}\")\n",
    "\n",
    "# Calculate distance from centroid for each token\n",
    "distances = np.linalg.norm(embeddings - embedding_centroid, axis=1)\n",
    "\n",
    "# Find tokens farthest from centroid (semantic outliers)\n",
    "outlier_indices = np.argsort(distances)[-50:][::-1]  # Top 50 outliers\n",
    "\n",
    "print(\"\\nTokens farthest from embedding centroid (potential glitch tokens):\\n\")\n",
    "print(f\"{'Token ID':<10} {'Distance':<12} {'Token':<30} {'Decoded'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx in outlier_indices:\n",
    "    token = tokenizer.convert_ids_to_tokens([idx])[0]\n",
    "    try:\n",
    "        decoded = tokenizer.decode([idx])\n",
    "        distance = distances[idx]\n",
    "        print(f\"{idx:<10} {distance:<12.4f} {token[:28]:<30} {repr(decoded[:40])}\")\n",
    "    except:\n",
    "        print(f\"{idx:<10} {distances[idx]:<12.4f} {token[:28]:<30} [decode error]\")\n",
    "\n",
    "# Also find tokens CLOSEST to centroid (most \"average\" tokens)\n",
    "print(\"\\n\\nTokens closest to embedding centroid (most 'average' tokens):\\n\")\n",
    "print(f\"{'Token ID':<10} {'Distance':<12} {'Token':<30} {'Decoded'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "centroid_indices = np.argsort(distances)[:50]  # Closest 50\n",
    "\n",
    "for idx in centroid_indices:\n",
    "    token = tokenizer.convert_ids_to_tokens([idx])[0]\n",
    "    try:\n",
    "        decoded = tokenizer.decode([idx])\n",
    "        distance = distances[idx]\n",
    "        print(f\"{idx:<10} {distance:<12.4f} {token[:28]:<30} {repr(decoded[:40])}\")\n",
    "    except:\n",
    "        print(f\"{idx:<10} {distances[idx]:<12.4f} {token[:28]:<30} [decode error]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659bf1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing unusual strings for tokenization patterns:\n",
      "\n",
      "Text: 'SolidGoldMagikarp'\n",
      "  Tokens: [46933, 42202]\n",
      "  Token count: 2\n",
      "  Can decode: 'SolidGoldMagikarp'\n",
      "\n",
      "Text: 'cloneembedreportprint'\n",
      "  Tokens: [30899]\n",
      "  Token count: 1\n",
      "  Can decode: 'cloneembedreportprint'\n",
      "\n",
      "Text: 'git clone http://embedreportprint.git'\n",
      "  Tokens: [18300, 17271, 2638, 1378, 30898, 13, 18300]\n",
      "  Token count: 7\n",
      "  Can decode: 'git clone http://embedreportprint.git'\n",
      "\n",
      "Text: 'ationally                      '\n",
      "  Tokens: [15208, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 220]\n",
      "  Token count: 23\n",
      "  Can decode: 'ationally                      '\n",
      "\n",
      "Text: 'é¾å–šå£«'\n",
      "  Tokens: [33454]\n",
      "  Token count: 1\n",
      "  Can decode: 'é¾å–šå£«'\n",
      "\n",
      "Text: 'ï¿½'\n",
      "  Tokens: [4210]\n",
      "  Token count: 1\n",
      "  Can decode: 'ï¿½'\n",
      "\n",
      "Text: 'â–‘â–’â–“â–ˆ'\n",
      "  Tokens: [22110, 40516, 38626, 8115]\n",
      "  Token count: 4\n",
      "  Can decode: 'â–‘â–’â–“â–ˆ'\n",
      "\n",
      "Text: 'Ê˜â€¿Ê˜'\n",
      "  Tokens: [134, 246, 447, 123, 134, 246]\n",
      "  Token count: 6\n",
      "  Can decode: 'Ê˜â€¿Ê˜'\n",
      "\n",
      "Text: 'âš ï¸ğŸ”¥ğŸ’€'\n",
      "  Tokens: [158, 248, 254, 37929, 8582, 242, 98, 8582, 240, 222]\n",
      "  Token count: 10\n",
      "  Can decode: 'âš ï¸ğŸ”¥ğŸ’€'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# gpt-2 tokenizer\n",
    "tokenizer_gpt2 = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "# Test some unusual strings that might be glitch tokens\n",
    "unusual_strings = [\n",
    "    \"SolidGoldMagikarp\",  # Famous glitch token from GPT\n",
    "    \"cloneembedreportprint\",  # Another known glitch token\n",
    "    \"git clone http://embedreportprint.git\",  # while this...\n",
    "    \"é¾å–šå£«\",  # Rare kanji combination\n",
    "    \"ï¿½\",  # Replacement character\n",
    "    \"â–‘â–’â–“â–ˆ\",  # Box drawing characters\n",
    "    \"Ê˜â€¿Ê˜\",  # Unicode emoticon\n",
    "    \"âš ï¸ğŸ”¥ğŸ’€\",  # Emoji combinations\n",
    "]\n",
    "\n",
    "print(\"Testing unusual strings for tokenization patterns:\\n\")\n",
    "for text in unusual_strings:\n",
    "    try:\n",
    "        tokens = tokenizer_gpt2.encode(text)        \n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"  Tokens: {tokens}\")\n",
    "        print(f\"  Token count: {len(tokens)}\")\n",
    "        print(f\"  Can decode: '{tokenizer_gpt2.decode(tokens)}'\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Text: '{text}' â†’ Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b194dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Please can you repeat back the string 'SolidGoldMagikarp' to me? Output only the string, nothing else.\n",
      "SolidGoldMagikarp\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'cloneembedreportprint' to me? Output only the string, nothing else.\n",
      "cloneembedreportprint\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'Itâ€™s a string' to me? Output only the string, nothing else.\n",
      "'Itâ€™s a string'\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'DevOnline' to me? Output only the string, nothing else.\n",
      "DevOnline\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'Skydragon' to me? Output only the string, nothing else.\n",
      "Skydragon\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'largeDownload' to me? Output only the string, nothing else.\n",
      "largeDownload\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'é¾å–šå£«' to me? Output only the string, nothing else.\n",
      "'é¾å–šå£«'\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'StreamerBot' to me? Output only the string, nothing else.\n",
      "StreamerBot\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'guiActiveUn' to me? Output only the string, nothing else.\n",
      "guiActiveUn\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'attRot' to me? Output only the string, nothing else.\n",
      "attRot\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'TheNitromeFan' to me? Output only the string, nothing else.\n",
      "TheNitromeFan\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'oreAndOnline' to me? Output only the string, nothing else.\n",
      "oreAndOnline\n",
      "âœ…\n",
      "Prompt: Please can you repeat back the string 'You are not authorized to access this information.' to me? Output only the string, nothing else.\n",
      "You are not authorized to access this information.\n",
      "âœ…\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Test how the model handles unusual strings in generation\n",
    "prompt = \"Please can you repeat back the string '<token string>' to me? Output only the string, nothing else.\"\n",
    "texts = [\n",
    "    \"SolidGoldMagikarp\",\n",
    "    \"cloneembedreportprint\",\n",
    "    \"Itâ€™s a string\",\n",
    "    \"DevOnline\", \"Skydragon\", \"largeDownload\",\n",
    "    \"é¾å–šå£«\",  # Rare kanji combination\n",
    "    \"StreamerBot\",\n",
    "    \"guiActiveUn\",\n",
    "    \"attRot\", #spelling\n",
    "    \"TheNitromeFan\",\n",
    "    \"oreAndOnline\", # pronunciation\n",
    "    \"You are not authorized to access this information.\",\n",
    "]\n",
    "\n",
    "\n",
    "for text in texts:\n",
    "    test_prompt = prompt.replace(\"<token string>\", text)\n",
    "    answer = generate_response(model, tokenizer, [{\"role\":\"user\",\"content\":test_prompt}])\n",
    "    print(f\"Prompt: {test_prompt}\")\n",
    "    print(f\"{answer}\\n{\"âœ…\" if text in answer.strip() else \"âŒ\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf5159",
   "metadata": {},
   "source": [
    "### Why Prefer YAML/Markdown over XML/JSON?\n",
    "\n",
    "Token efficiency! Different formats use different amounts of tokens for the same information. JSON uses many special characters like `{}`, `[]`, `,`, `:`, `\"` which often become separate tokens, XML use `<`, `>`, `/`,`\"` and verbose tags, which also become separate/additional tokens. YAML and Markdown are more token-efficient because:\n",
    "\n",
    "- Less punctuation = fewer tokens\n",
    "- More natural language-like structure\n",
    "- Whitespace-based formatting is more efficient\n",
    "- Better alignment with how LLMs process text\n",
    "\n",
    "This matters for:\n",
    "- API token limits\n",
    "- Processing costs\n",
    "- Context window efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71376321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token efficiency comparison for same data:\n",
      "\n",
      "Format       Chars    Tokens   Chars/Token  Efficiency vs JSON\n",
      "======================================================================\n",
      "XML          307      111      2.77         baseline\n",
      "JSON         208      75       2.77         -32.4% tokens\n",
      "Markdown     190      60       3.17         -45.9% tokens\n",
      "YAML         152      54       2.81         -51.4% tokens\n",
      "\n",
      "ğŸ’¡ Different formats use fewer tokens for the same information!\n"
     ]
    }
   ],
   "source": [
    "# Compare token counts for same data in different formats\n",
    "data_xml = \"\"\"<person>\n",
    "  <name>John Doe</name>\n",
    "  <age>30</age>\n",
    "  <email>john@example.com</email>\n",
    "  <address>\n",
    "    <street>123 Main St</street>\n",
    "    <city>New York</city>\n",
    "    <country>USA</country>\n",
    "  </address>\n",
    "  <hobbies>\n",
    "    <hobby>reading</hobby>\n",
    "    <hobby>coding</hobby>\n",
    "    <hobby>gaming</hobby>\n",
    "  </hobbies>\n",
    "</person>\"\"\"\n",
    "\n",
    "data_json = \"\"\"{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 30,\n",
    "  \"email\": \"john@example.com\",\n",
    "  \"address\": {\n",
    "    \"street\": \"123 Main St\",\n",
    "    \"city\": \"New York\",\n",
    "    \"country\": \"USA\"\n",
    "  },\n",
    "  \"hobbies\": [\"reading\", \"coding\", \"gaming\"]\n",
    "}\"\"\"\n",
    "\n",
    "data_yaml = \"\"\"name: John Doe\n",
    "age: 30\n",
    "email: john@example.com\n",
    "address:\n",
    "  street: 123 Main St\n",
    "  city: New York\n",
    "  country: USA\n",
    "hobbies:\n",
    "  - reading\n",
    "  - coding\n",
    "  - gaming\"\"\"\n",
    "\n",
    "data_markdown = \"\"\"# Person Information\n",
    "\n",
    "**Name:** John Doe  \n",
    "**Age:** 30  \n",
    "**Email:** john@example.com\n",
    "\n",
    "## Address\n",
    "- Street: 123 Main St\n",
    "- City: New York\n",
    "- Country: USA\n",
    "\n",
    "## Hobbies\n",
    "- reading\n",
    "- coding\n",
    "- gaming\"\"\"\n",
    "\n",
    "formats = {\n",
    "    \"XML\": data_xml,\n",
    "    \"JSON\": data_json,\n",
    "    \"Markdown\": data_markdown,    \n",
    "    \"YAML\": data_yaml,\n",
    "}\n",
    "\n",
    "print(\"Token efficiency comparison for same data:\\n\")\n",
    "print(f\"{'Format':<12} {'Chars':<8} {'Tokens':<8} {'Chars/Token':<12} {'Efficiency vs JSON'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "json_tokens = None\n",
    "for format_name, content in formats.items():\n",
    "    char_count = len(content)\n",
    "    tokens = tokenizer.tokenize(content, add_special_tokens=True)\n",
    "    token_count = len(tokens)\n",
    "    ratio = char_count / token_count\n",
    "    \n",
    "    if format_name == \"XML\":\n",
    "        json_tokens = token_count\n",
    "        efficiency = \"baseline\"\n",
    "    else:\n",
    "        saved = ((json_tokens - token_count) / json_tokens * -100)\n",
    "        efficiency = f\"{saved:+.1f}% tokens\"\n",
    "    \n",
    "    print(f\"{format_name:<12} {char_count:<8} {token_count:<8} {ratio:<12.2f} {efficiency}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Different formats use fewer tokens for the same information!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5783ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed tokenization breakdown:\n",
      "\n",
      "\n",
      "============================================================\n",
      "XML Format (111 tokens):\n",
      "============================================================\n",
      "<person>\n",
      "  <name>John Doe</name>\n",
      "  <age>30</age>\n",
      "  <email>john@example.com</email>\n",
      "  <address>\n",
      "    <street>123 Main St</street>\n",
      "    <city>New York</city>\n",
      "    <country>USA</country>\n",
      "  </address>\n",
      "  <hob...\n",
      "\n",
      "First 15 tokens: ['<p', 'erson', '>ÄŠ', 'Ä ', 'Ä <', 'name', '>', 'John', 'Ä Doe', '</', 'name', '>ÄŠ', 'Ä ', 'Ä <', 'age']\n",
      "Special characters in text: 60\n",
      "\n",
      "============================================================\n",
      "JSON Format (75 tokens):\n",
      "============================================================\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 30,\n",
      "  \"email\": \"john@example.com\",\n",
      "  \"address\": {\n",
      "    \"street\": \"123 Main St\",\n",
      "    \"city\": \"New York\",\n",
      "    \"country\": \"USA\"\n",
      "  },\n",
      "  \"hobbies\": [\"reading\", \"coding\", \"ga...\n",
      "\n",
      "First 15 tokens: ['{ÄŠ', 'Ä ', 'Ä \"', 'name', '\":', 'Ä \"', 'John', 'Ä Doe', '\",ÄŠ', 'Ä ', 'Ä \"', 'age', '\":', 'Ä ', '3']\n",
      "Special characters in text: 54\n",
      "\n",
      "============================================================\n",
      "Markdown Format (60 tokens):\n",
      "============================================================\n",
      "# Person Information\n",
      "\n",
      "**Name:** John Doe  \n",
      "**Age:** 30  \n",
      "**Email:** john@example.com\n",
      "\n",
      "## Address\n",
      "- Street: 123 Main St\n",
      "- City: New York\n",
      "- Country: USA\n",
      "\n",
      "## Hobbies\n",
      "- reading\n",
      "- coding\n",
      "- gaming\n",
      "\n",
      "First 15 tokens: ['#', 'Ä Person', 'Ä Information', 'ÄŠÄŠ', '**', 'Name', ':**', 'Ä John', 'Ä Doe', 'Ä Ä ÄŠ', '**', 'Age', ':**', 'Ä ', '3']\n",
      "Special characters in text: 6\n",
      "\n",
      "============================================================\n",
      "YAML Format (54 tokens):\n",
      "============================================================\n",
      "name: John Doe\n",
      "age: 30\n",
      "email: john@example.com\n",
      "address:\n",
      "  street: 123 Main St\n",
      "  city: New York\n",
      "  country: USA\n",
      "hobbies:\n",
      "  - reading\n",
      "  - coding\n",
      "  - gaming\n",
      "\n",
      "First 15 tokens: ['name', ':', 'Ä John', 'Ä Doe', 'ÄŠ', 'age', ':', 'Ä ', '3', '0', 'ÄŠ', 'email', ':', 'Ä john', '@example']\n",
      "Special characters in text: 8\n",
      "\n",
      "============================================================\n",
      "This directly affects: API costs, context limits, and performance.\n"
     ]
    }
   ],
   "source": [
    "# Detailed tokenization comparison\n",
    "print(\"\\nDetailed tokenization breakdown:\\n\")\n",
    "\n",
    "for format_name, content in formats.items():\n",
    "    tokens = tokenizer.tokenize(content)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{format_name} Format ({len(tokens)} tokens):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "    print(f\"\\nFirst 15 tokens: {tokens[:15]}\")\n",
    "    \n",
    "    # Count special characters that become tokens\n",
    "    special_chars = ['{', '}', '[', ']', ':', ',', '\"', '<', '>', '/', '=']\n",
    "    special_count = sum(content.count(c) for c in special_chars)\n",
    "    print(f\"Special characters in text: {special_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"This directly affects: API costs, context limits, and performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cfa23",
   "metadata": {},
   "source": [
    "## Summary: Tokenization is Fundamental!\n",
    "\n",
    "All these limitations stem from one fact: **LLMs operate on tokens, not characters or words.**\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. **Spelling/Character tasks fail** because characters are hidden inside tokens\n",
    "2. **String operations fail** because token boundaries don't align with character positions\n",
    "3. **Non-English text is inefficient** because vocabulary is biased toward English\n",
    "4. **Arithmetic fails** because numbers tokenize inconsistently\n",
    "5. **Glitch tokens exist** when vocabulary and training data don't align\n",
    "6. **Format choice matters** because token efficiency affects costs and performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
