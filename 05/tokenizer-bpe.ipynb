{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b098edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/_github/massimodipaolo/ai-crash-course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 29567\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 62\n",
      "    })\n",
      "})\n",
      "                                                text\n",
      "0  = Valkyria Chronicles III =\\nSenjÅ no Valkyria...\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
      "3  = Gambia women's national football team =\\nThe...\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...\n"
     ]
    }
   ],
   "source": [
    "# WikiText-103 is a large-scale language modeling dataset extracted from verified Good and Featured articles on Wikipedia\n",
    "# # > 100M words across 29k articles\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "wiki_dataset = load_dataset(\"yehzw/wikitext-103\", \"raw\")\n",
    "print(wiki_dataset)\n",
    "_ = pd.DataFrame.from_dict(wiki_dataset[\"train\"][:5])  \n",
    "print(_.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer \n",
    "from tokenizers.models import BPE # BPE: Byte Pair Encoding model, subword tokenization\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")) # Initialize a BPE tokenizer with a specified unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f8f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BpeTrainer(BpeTrainer(min_frequency=0, vocab_size=30000, show_progress=True, special_tokens=[AddedToken(content=\"[UNK]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[CLS]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[SEP]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[PAD]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[MASK]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True)], limit_alphabet=None, initial_alphabet=[], continuing_subword_prefix=None, end_of_word_suffix=None, max_token_length=None, words={}))\n"
     ]
    }
   ],
   "source": [
    "# trainer instance to train the BPE tokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "# special tokens: unknown, class, separator, padding, mask\n",
    "# unknown: used for out-of-vocabulary tokens\n",
    "# class: used to denote the start of a sequence\n",
    "# separator: used to separate different segments in a sequence\n",
    "# padding: used to pad sequences to a uniform length, used in batching\n",
    "# mask: used in masked language modeling tasks like BERT\n",
    "# The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1...\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization with pre-tokenizers\n",
    "# pre-tokenizer to split input text into words based on whitespace\n",
    "# optimizes the tokenization process by breaking down text into manageable chunks\n",
    "# e.g \"it is\" can be a single token since those two words are often together in the training data, instead -> [\"it\", \"is\"]\n",
    "# ensure that no token is bigger than a word\n",
    "from tokenizers.pre_tokenizers import Split, Digits, Sequence\n",
    "tokenizer.pre_tokenizer = Sequence([\n",
    "    Split(pattern=r\"\\s+\", behavior=\"isolated\"), # split on whitespace, e.g. \"it is\" -> [\"it\", \"is\"], isolated means keep the split tokens, \"it is\" -> [\"it\", \" \", \"is\"]\n",
    "    Digits(individual_digits=False) # split digits from letters, e.g. \"word123\" -> [\"word\", \"123\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7239a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the tokenizer on each split of the WikiText-103 dataset\n",
    "# relative fast training time even on large datasets\n",
    "tokenizer.train_from_iterator(\n",
    "\t(row[\"text\"] for split in [\"train\", \"validation\", \"test\"] for row in wiki_dataset[split]),\n",
    "\ttrainer=trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c09c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./tmp/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "564a21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./tmp/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dc3d506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hell', 'o, ', 'dev', 's', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.']\n",
      "IDs: [14653, 6242, 5598, 89, 0, 7, 8717, 5055, 5132, 5893, 23, 5223, 39, 5509, 16678, 22828, 20]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, devsðŸ˜! Welcome to the 1st AI crash course.\"\n",
    "output = tokenizer.encode(sentence)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5c80b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hell, Offsets: (0, 4)\n",
      "Token: o, , Offsets: (4, 7)\n",
      "Token: dev, Offsets: (7, 10)\n",
      "Token: s, Offsets: (10, 11)\n",
      "Token: [UNK], Offsets: (11, 12)\n",
      "Token: !, Offsets: (12, 13)\n",
      "Token:  W, Offsets: (13, 15)\n",
      "Token: el, Offsets: (15, 17)\n",
      "Token: com, Offsets: (17, 20)\n",
      "Token: e to the , Offsets: (20, 29)\n",
      "Token: 1, Offsets: (29, 30)\n",
      "Token: st , Offsets: (30, 33)\n",
      "Token: A, Offsets: (33, 34)\n",
      "Token: I , Offsets: (34, 36)\n",
      "Token: crash , Offsets: (36, 42)\n",
      "Token: course, Offsets: (42, 48)\n",
      "Token: ., Offsets: (48, 49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ðŸ˜'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# offsets are also tracked to indicate the start and end positions of each token in the original input string\n",
    "for i, token in enumerate(output.tokens):\n",
    "    print(f\"Token: {token}, Offsets: {output.offsets[i]}\")\n",
    "# discovered [UNK] emoji in the original string\n",
    "sentence[11:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6b5fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30000 trainer: 30000\n",
      "Token: [UNK], ID: 0\n",
      "Token: [CLS], ID: 1\n",
      "Token: [SEP], ID: 2\n",
      "Token: [PAD], ID: 3\n",
      "Token: [MASK], ID: 4\n"
     ]
    }
   ],
   "source": [
    "# check vocabulary size, same as trainer.vocab_size\n",
    "print(\"Vocabulary Size:\", tokenizer.get_vocab_size(), \"trainer:\", trainer.vocab_size)\n",
    "# double check special tokens, defined during training\n",
    "for token in [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]:\n",
    "    token_id = tokenizer.token_to_id(token)\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "321d6bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'o, ', 'dev', 's', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '... ', 'second ', 'sent', 'ence', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# pairwise sequence tokenization with type IDs, e.g. for sentence pairs in tasks like QA or NLI (Natural Language Inference)\n",
    "output = tokenizer.encode(sentence, \"... second sentence.\")\n",
    "print(output.tokens)\n",
    "print(output.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75ca9927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'o, ', 'dev', 's', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '... ', 'second ', 'sent', 'ence', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "ðŸ›‘ Hell o,  dev s !  W el com e to the  1 st  A I  crash  course . ...  second  sent ence .\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "output = tokenizer.encode(sentence, \"... second sentence.\")\n",
    "print(output.tokens)\n",
    "print(output.type_ids)\n",
    "decoded = tokenizer.decode(output.ids) #\n",
    "print(\"ðŸ›‘\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd5d0dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'o, ', 'dev', 's', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '... ', 'second ', 'sent', 'ence', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "ðŸ›‘ Hello, devs[UNK]! Welcome to the 1st AI crash course.... second sentence.\n",
      "âœ… Hello, devs! Welcome to the 1st AI crash course.... second sentence.\n"
     ]
    }
   ],
   "source": [
    "# decoder to properly handle BPE tokens\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "output = tokenizer.encode(sentence, \"... second sentence.\")\n",
    "print(output.tokens)\n",
    "print(output.type_ids)\n",
    "# common pitfalls: keep in account special tokens during decoding\n",
    "decoded = tokenizer.decode(output.ids, skip_special_tokens=False)\n",
    "print(\"ðŸ›‘\", decoded)\n",
    "decoded = tokenizer.decode(output.ids, skip_special_tokens=True)\n",
    "print(\"âœ…\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53a6d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply post process, e.g. for BERT-like models\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\", # for single sentence $A\n",
    "    # pairs can be handled too, e.g. sentence similarity tasks\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\", # for pairs of sentences $A and $B, :1 is type IDs for second sentence (omitted in $A)\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), # start of sequence\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), # end of sequence\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8b8c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Hell', 'o, ', 'dev', 's', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '[SEP]', '... ', 'second ', 'sent', 'ence', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "âœ… Hello, devs! Welcome to the 1st AI crash course.... second sentence.\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(sentence, \"... second sentence.\")\n",
    "print(output.tokens)\n",
    "print(output.type_ids)\n",
    "decoded = tokenizer.decode(output.ids, skip_special_tokens=True)\n",
    "print(\"âœ…\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f9046be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Hell', 'o, ', 'dev', 's ', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['[CLS]', 'T', 'ok', 'en', 'iz', 'ers are ', 'really ', 'fast ', 'and ', 'eff', 'icient', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['[CLS]', 'We ', 'can ', 'process ', 'multiple ', 'sent', 'ences ', 'at ', 'onc', 'e.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['[CLS]', 'This ', 'is the ', 'fourth ', 'sent', 'ence', '.', '[SEP]', 'P', 'air ', 'with ', 'another ', 'one ', ' ', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "['[CLS]', 'Fin', 'ally, the ', 'fifth ', 'sentence ', 'complet', 'es ', 'our ', 'bat', 'ch', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# speed up encoding with batch processing\n",
    "sentences = [\n",
    "    \"Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\",\n",
    "    \"Tokenizers are really fast and efficient.\",\n",
    "    \"We can process multiple sentences at once.\",\n",
    "    [\"This is the fourth sentence.\",\"Pair with another one  .\"], #also pairs\n",
    "    \"Finally, the fifth sentence completes our batch.\"\n",
    "    ]\n",
    "outputs = tokenizer.encode_batch(sentences)\n",
    "for output in outputs:\n",
    "    print(output.tokens)\n",
    "    print(output.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12cad436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Tokens: ['[CLS]', 'H', 'ow ', 'are ', 'you ', '[UNK]', ' ', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Padded IDs: [1, 46, 5375, 5232, 7244, 0, 6, 37, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# enable padding: default to right, pad to the longest sequence in the batch\n",
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\")\n",
    "output = tokenizer.encode_batch([\"Hello, y'all! Adding some context to create some padding\", \"How are you ðŸ˜ ?\"]) # the pair sentence\n",
    "print(\"Padded Tokens:\", output[1].tokens)\n",
    "print(\"Padded IDs:\", output[1].ids)\n",
    "# attention mask indicates which tokens should be attended to (1) and which are padding (0)\n",
    "print(output[0].attention_mask)\n",
    "print(output[1].attention_mask) # tokenizer takes the padding into account, e.g. skipping attention on padded tokens\n",
    "tokenizer.save(\"./tmp/tokenizer-wiki-post.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "314c84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BPE ---\n",
      "['[CLS]', 'Hell', 'o, ', 'dev', 's ', '[UNK]', '!', ' W', 'el', 'com', 'e to the ', '1', 'st ', 'A', 'I ', 'crash ', 'course', '.', '[SEP]']\n",
      "Hello, devs ! Welcome to the 1st AI crash course.\n",
      "--- BERT Cased ---\n",
      "['[CLS]', 'Hello', ',', 'de', '##v', '##s', '[UNK]', '!', 'Welcome', 'to', 'the', '1st', 'AI', 'crash', 'course', '.', '[SEP]']\n",
      "Hello, devs! Welcome to the 1st AI crash course.\n",
      "--- BERT Uncased ---\n",
      "['[CLS]', 'hello', ',', 'dev', '##s', '[UNK]', '!', 'welcome', 'to', 'the', '1st', 'ai', 'crash', 'course', '.', '[SEP]']\n",
      "hello, devs! welcome to the 1st ai crash course.\n"
     ]
    }
   ],
   "source": [
    "# BPE vs pretrained WordPiece BERT cased/uncased\n",
    "from tokenizers import Tokenizer\n",
    "bpe = Tokenizer.from_file(\"./tmp/tokenizer-wiki-post.json\")\n",
    "bert_c = Tokenizer.from_pretrained(\"google-bert/bert-base-cased\") \n",
    "bert_u = Tokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "for [tokenizer_name, tokenizer_instance] in [(\"BPE\", bpe), (\"BERT Cased\", bert_c), (\"BERT Uncased\", bert_u)]:\n",
    "    print(f\"--- {tokenizer_name} ---\")\n",
    "    output = tokenizer_instance.encode(sentences[0])\n",
    "    print(output.tokens)\n",
    "    print(tokenizer_instance.decode(output.ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b64892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-07 00:26:49--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.130.248, 16.15.178.253, 16.15.179.50, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.130.248|:443... connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: â€˜./tmp/bert-base-uncased-vocab.txtâ€™\n",
      "\n",
      "./tmp/bert-base-unc 100%[===================>] 226.08K   654KB/s    in 0.3s    \n",
      "\n",
      "2025-11-07 00:26:50 (654 KB/s) - â€˜./tmp/bert-base-uncased-vocab.txtâ€™ saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer card can expose a vocab\n",
    "!wget -O ./tmp/bert-base-uncased-vocab.txt https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
