{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7b95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding name: o200k_base\n",
      "Encoded tokens: [13225, 11, 3947, 82, 22861, 223, 0, 26501, 316, 290, 220, 16, 302, 20837, 20840, 4165, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# load encoding directly\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#or load encoding for a specific model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "print(\"Encoding name:\", encoding.name)\n",
    "# encode text to tokens\n",
    "tokens = encoding.encode(\"Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\")\n",
    "print(\"Encoded tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4037648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in encoding 'o200k_base': 17\n",
      "Number of tokens in encoding 'cl100k_base': 16\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "for encoding_name in [\"o200k_base\", \"cl100k_base\"]:\n",
    "    num_tokens = num_tokens_from_string(\"Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\", encoding_name)\n",
    "    print(f\"Number of tokens in encoding '{encoding_name}':\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee0c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence: \"Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\"\n",
      "\n",
      "r50k_base: 15 tokens\n",
      "token integers: [15496, 11, 41470, 30325, 223, 0, 19134, 284, 262, 352, 301, 9552, 7014, 1781, 13]\n",
      "token bytes: [b'Hello', b',', b' devs', b' \\xf0\\x9f\\x98', b'\\x81', b'!', b' Welcome', b' to', b' the', b' 1', b'st', b' AI', b' crash', b' course', b'.']\n",
      "decoded back: Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\n",
      "----------------------------------------\n",
      "\n",
      "p50k_base: 15 tokens\n",
      "token integers: [15496, 11, 41470, 30325, 223, 0, 19134, 284, 262, 352, 301, 9552, 7014, 1781, 13]\n",
      "token bytes: [b'Hello', b',', b' devs', b' \\xf0\\x9f\\x98', b'\\x81', b'!', b' Welcome', b' to', b' the', b' 1', b'st', b' AI', b' crash', b' course', b'.']\n",
      "decoded back: Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\n",
      "----------------------------------------\n",
      "\n",
      "cl100k_base: 16 tokens\n",
      "token integers: [9906, 11, 81169, 27623, 223, 0, 20776, 311, 279, 220, 16, 267, 15592, 10121, 3388, 13]\n",
      "token bytes: [b'Hello', b',', b' devs', b' \\xf0\\x9f\\x98', b'\\x81', b'!', b' Welcome', b' to', b' the', b' ', b'1', b'st', b' AI', b' crash', b' course', b'.']\n",
      "decoded back: Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\n",
      "----------------------------------------\n",
      "\n",
      "o200k_base: 17 tokens\n",
      "token integers: [13225, 11, 3947, 82, 22861, 223, 0, 26501, 316, 290, 220, 16, 302, 20837, 20840, 4165, 13]\n",
      "token bytes: [b'Hello', b',', b' dev', b's', b' \\xf0\\x9f\\x98', b'\\x81', b'!', b' Welcome', b' to', b' the', b' ', b'1', b'st', b' AI', b' crash', b' course', b'.']\n",
      "decoded back: Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\n",
      "----------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "sentence: \"ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\"\n",
      "\n",
      "r50k_base: 24 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557, 2515, 242, 2515, 244, 18566, 30159, 33623, 171, 120, 223]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86', b'\\xe3\\x81', b'\\x94', b'\\xe3\\x81', b'\\x96', b'\\xe3\\x81\\x84', b'\\xe3\\x81\\xbe', b'\\xe3\\x81\\x99', b'\\xef', b'\\xbc', b'\\x81']\n",
      "decoded back: ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
      "----------------------------------------\n",
      "\n",
      "p50k_base: 24 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557, 2515, 242, 2515, 244, 18566, 30159, 33623, 171, 120, 223]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86', b'\\xe3\\x81', b'\\x94', b'\\xe3\\x81', b'\\x96', b'\\xe3\\x81\\x84', b'\\xe3\\x81\\xbe', b'\\xe3\\x81\\x99', b'\\xef', b'\\xbc', b'\\x81']\n",
      "decoded back: ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
      "----------------------------------------\n",
      "\n",
      "cl100k_base: 12 tokens\n",
      "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699, 77121, 61689, 6447]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86', b'\\xe3\\x81\\x94\\xe3\\x81\\x96', b'\\xe3\\x81\\x84\\xe3\\x81\\xbe\\xe3\\x81\\x99', b'\\xef\\xbc\\x81']\n",
      "decoded back: ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
      "----------------------------------------\n",
      "\n",
      "o200k_base: 10 tokens\n",
      "token integers: [8930, 9697, 243, 128225, 8930, 17693, 4344, 48669, 59809, 3393]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86', b'\\xe3\\x81\\x94\\xe3\\x81\\x96\\xe3\\x81\\x84\\xe3\\x81\\xbe\\xe3\\x81\\x99', b'\\xef\\xbc\\x81']\n",
      "decoded back: ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare different encodings\n",
    "# note: convert single tokens back to bytes for token that are out of utf-8 range\n",
    "def compare_encodings(sentence: str) -> None:\n",
    "    print(f'\\nsentence: \"{sentence}\"')\n",
    "    for encoding_name in [\"r50k_base\", # used by older models like babbage\n",
    "                          \"p50k_base\", # used by codex models like text-davinci-003\n",
    "                          \"cl100k_base\", # used by models like gpt-4, text-embedding-3-small, text-embedding-3-large\n",
    "                          \"o200k_base\" # used by models like gpt-4o-mini\n",
    "                          ]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(sentence)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")\n",
    "        print(f\"decoded back: {encoding.decode(token_integers)}\")\n",
    "        print(\"-\" * 40)\n",
    "compare_encodings(\"Hello, devs ðŸ˜! Welcome to the 1st AI crash course.\")      \n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "compare_encodings(\"ãŠèª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6004b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini\") -> int:\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")    \n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73a19c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n",
      "48 prompt tokens counted by tiktoken.\n",
      "48 prompt tokens counted by the OpenAI API.\n",
      "10 completion tokens counted by the OpenAI API.\n",
      "58 total tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o-mini\n",
      "46 prompt tokens counted by tiktoken.\n",
      "46 prompt tokens counted by the OpenAI API.\n",
      "10 completion tokens counted by the OpenAI API.\n",
      "56 total tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [    \n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o-mini\"\n",
    "     ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by tiktoken.\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(model=model,\n",
    "    messages=example_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=10)\n",
    "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
    "    print(f'{response.usage.completion_tokens} completion tokens counted by the OpenAI API.')\n",
    "    print(f'{response.usage.total_tokens} total tokens counted by the OpenAI API.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea359b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 2063,\n",
       " 'input_tokens_details': InputTokensDetails(cached_tokens=0),\n",
       " 'output_tokens': 147,\n",
       " 'output_tokens_details': OutputTokensDetails(reasoning_tokens=0),\n",
       " 'total_tokens': 2210}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "\n",
    "client = OpenAI() \n",
    "\n",
    "# image generation with text-only input and image output using tool call\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=\"Generate a labeled diagram illustrating the structure of a flower. The diagram shows the internal and external parts of a flower with labels pointing to each part.\",\n",
    "    tools=[{\"type\": \"image_generation\"}]\n",
    ")\n",
    "\n",
    "# save image\n",
    "image_data = [output.result for output in response.output if output.type == \"image_generation_call\"]\n",
    "if image_data:\n",
    "    image_base64 = image_data[0]\n",
    "    with open(\"./tmp/flower_diagram.png\", \"wb\") as f:\n",
    "        f.write(base64.b64decode(image_base64))\n",
    "\n",
    "display(response.usage.__dict__)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848c868",
   "metadata": {},
   "source": [
    "```\n",
    "{'input_tokens': 2063,\n",
    " 'input_tokens_details': InputTokensDetails(cached_tokens=0),\n",
    " 'output_tokens': 147,\n",
    " 'output_tokens_details': OutputTokensDetails(reasoning_tokens=0),\n",
    " 'total_tokens': 2210}\n",
    " ```\n",
    "\n",
    "- Token usage only measures text processed by the model, not the binary data of the image.\n",
    "1. Text in â†’ tokens counted.\n",
    "    prompt text, tool specification, and system context make up the 2,063 input tokens.\n",
    "\n",
    "2. Text out â†’ tokens counted.\n",
    "    modelâ€™s response (which includes the tool call request and metadata, not the base64 image itself) adds 147 output tokens.\n",
    "\n",
    "3. Image bytes â†’ not tokens.\n",
    "    - image generation done by the image_generation tool consumes compute resources (typically billed separately), but its binary output is not part of the token accounting\n",
    "    - tokens apply only to the LLMâ€™s text reasoning and tool-calling layer.\n",
    "\n",
    "- multi-modal flow (may change depending on provider implementation, models, tools, etc.):\n",
    "```\n",
    "prompt â†’ LLM (text tokens)\n",
    "   â†³ tool call instruction â†’ image generator (no tokens)\n",
    "      â†³ image returned as base64 â†’ 1_666_983 bytes delivered: ~1.59 MB\n",
    "```\n",
    "\n",
    "- Key takeaways:\n",
    "\n",
    "    . `Monitor` tokens consumption but also be aware of the media generation costs\n",
    "    \n",
    "    . Tokens count != price"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
