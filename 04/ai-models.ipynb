{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b5ebba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Domain",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Task",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Organization",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Publication date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Reference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Link",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Citations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Notability criteria",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Notability criteria notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Parameters",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Parameters notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training compute (FLOP)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training compute notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training dataset size (gradients)",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Dataset size notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training time (hours)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training time notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training hardware",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Approach",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Confidence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Epochs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WikiText and Penn Treebank data",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Model accessibility",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Country (of organization)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Base model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Finetune compute (FLOP)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Finetune compute notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Hardware quantity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Hardware utilization (MFU)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Last modified",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Training cloud compute vendor",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training data center",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Archived links",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Batch size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Batch size notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Organization categorization",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Foundation model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training compute lower bound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training compute upper bound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training chip-hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training code accessibility",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Accessibility notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Possibly over 1e23 FLOP",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training compute cost (2023 USD)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Utilization notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Numerical format",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Frontier model",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Training power draw (W)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Training compute estimation method",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Hugging Face developer id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Post-training compute (FLOP)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Post-training compute notes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Hardware utilization (HFU)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6d94760d-4f94-431f-b247-98998a0a584e",
       "rows": [
        [
         "0",
         "Emu3.5",
         "Video,Multimodal,Image generation,Vision,Language,Speech",
         "Text-to-video,Image-to-video,Image generation,Text-to-image,Visual question answering,Language modeling/generation,Question answering,Speech recognition (ASR),Video description",
         "Beijing Academy of Artificial Intelligence / BAAI",
         null,
         "2025-10-30",
         "Emu3.5: Native Multimodal Models are World Learners",
         "https://emu.world/Emu35_tech_report.pdf",
         null,
         null,
         null,
         "34100000000.0",
         "\"Overall, the model contains 34.1 billion(B) parameters, including 31.2 B in the transformer layers and 2.9 B in the embedding layers. \"",
         "9.86736e+24",
         "7.084799999999999e+24 FLOP [base model compute] + 2.78256e+24 FLOP = 9.86736e+24 FLOP",
         "13600000000000",
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20× without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration\nand open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image\ngeneration and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support\ncommunity research.",
         null,
         null,
         null,
         "China",
         "Whisper v2,Qwen3-32B",
         null,
         "6 FLOP/parameter/token * 34100000000 parameters * 13600000000000 tokens = 2.78256e+24 FLOP",
         null,
         null,
         "2025-10-30 20:24:20+00:00",
         null,
         null,
         null,
         null,
         null,
         "Academia",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://github.com/baaivision/Emu3.5\n\nHF release is forthcoming\nhttps://huggingface.co/collections/BAAI/emu35",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Tongyi DeepResearch",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Search,System control",
         "Alibaba",
         "Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang",
         "2025-10-28",
         "Tongyi DeepResearch Technical Report",
         "https://arxiv.org/abs/2510.24701",
         null,
         "SOTA improvement",
         "\" achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity’s Last\nExam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510\"",
         "30500000000.0",
         "30.5 billion total parameters,\nwith only 3.3 billion activated per token",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Qwen3-30B-A3B",
         null,
         null,
         null,
         null,
         "2025-10-31 15:50:56+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\nhttps://github.com/Alibaba-NLP/DeepResearch",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Alibaba-NLP",
         null,
         null,
         null
        ],
        [
         "2",
         "MiniMax-M2",
         "Language",
         "Code generation,System control,Search,Language modeling/generation,Question answering",
         "MiniMax",
         null,
         "2025-10-27",
         "MiniMax M2 & Agent: Ingenious in Simplicity",
         "https://www.minimax.io/news/minimax-m2",
         null,
         "Discretionary",
         " high performing open source model",
         "229000000000.0",
         "\"maintaining activations around 10B\"\nsafetensors: 229B params",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "Today, we are officially open-sourcing and launching MiniMax M2, a model born for Agents and code. At only 8% of the price of Claude Sonnet and twice the speed, it's available for free for a limited time!\nTop-tier Coding Capabilities: Built for end-to-end development workflows, it excels in various applications such as Claude Code, Cursor, Cline, Kilo Code, and Droid.\nPowerful Agentic Performance: It demonstrates outstanding planning and stable execution of complex, long-chain tool-calling tasks, coordinating calls to the Shell, Browser, Python code interpreter, and various MCP tools.\nUltimate Cost-Effectiveness & Speed: Through efficient design of activated parameters, we have achieved the optimal balance of intelligence, speed, and cost.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-31 12:15:42+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/MiniMaxAI/MiniMax-M2",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "MiniMaxAI",
         null,
         null,
         null
        ],
        [
         "3",
         "Ring-mini-linear-2.0",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",
         "Ant Group",
         "Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou",
         "2025-10-23",
         "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
         "https://arxiv.org/abs/2510.19338",
         null,
         null,
         null,
         "16400000000.0",
         "Ring-mini-linear-2.0 comprises 16B parameters and 957M activations",
         "1.714452e+23",
         "1.68e+23 FLOP [base model compute] + 3.4452e+21 FLOP = 1.714452e+23 FLOP",
         "600000000000",
         "Continued training Stage: 600B tokens",
         null,
         null,
         "NVIDIA H800 SXM5",
         null,
         "Confident",
         "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Ling-mini-base-2.0-20T",
         "3.4452e+21",
         "6 FLOP / parameter / token * 957 * 10^6 active parameters * 600 * 10^9 tokens = 3.4452e+21 FLOP",
         "288.0",
         null,
         "2025-10-28 20:47:04+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/inclusionAI/Ring-mini-linear-2.0",
         null,
         null,
         null,
         "FP8",
         null,
         "393635.1780890865",
         "Operation counting",
         "inclusionAI",
         null,
         null,
         null
        ],
        [
         "4",
         "Ring-flash-linear-2.0",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",
         "Ant Group",
         "Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou",
         "2025-10-23",
         "Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning",
         "https://arxiv.org/abs/2510.19338",
         null,
         null,
         null,
         "104200000000.0",
         " Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations",
         "7.686e+23",
         "7.32e+23 FLOP [base model compute] + 3.66e+22 FLOP = 7.686e+23 FLOP",
         "1000000000000",
         "Continued training Stage: 1T tokens.",
         null,
         null,
         "NVIDIA H800 SXM5",
         null,
         "Confident",
         "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Ling-flash-base-2.0-20T",
         "3.66e+22",
         "6 FLOP / parameter / token * 6.1 * 10^9 active parameters * 10^12 tokens = 3.66e+22 FLOP",
         "32.0",
         null,
         "2025-10-28 20:47:54+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/inclusionAI/Ring-flash-linear-2.0",
         null,
         null,
         null,
         "FP8",
         null,
         "43737.2420098985",
         "Operation counting",
         "inclusionAI",
         null,
         null,
         null
        ],
        [
         "5",
         "Deepseek OCR",
         "Vision,Language",
         "Character recognition (OCR),Visual question answering",
         "DeepSeek",
         "Haoran Wei, Yaofeng Sun, Yukun Li",
         "2025-10-21",
         "DeepSeek-OCR: Contexts Optical Compression",
         "https://arxiv.org/abs/2510.18234v1",
         null,
         null,
         null,
         "3000000000.0",
         "\"DeepEncoder is approximately 380M in parameters, mainly composed of an 80M SAM-base [17] and a 300M CLIP-large [29] connected in series. The decoder adopts a 3B MoE [19, 20] architecture with 570M activated parameters.\"",
         null,
         null,
         null,
         null,
         null,
         "\"We use 20 nodes (each with 8 A100-40G GPUs) for training\"",
         "NVIDIA A100 SXM4 40 GB",
         null,
         "Confident",
         "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at this http URL.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Segment Anything Model,CLIP (ViT L/14@336px)",
         null,
         null,
         "160.0",
         null,
         "2025-10-30 16:18:36+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://github.com/deepseek-ai/DeepSeek-OCR\nhttps://huggingface.co/deepseek-ai/DeepSeek-OCR",
         null,
         null,
         null,
         null,
         null,
         "124969.11444297624",
         null,
         "deepseek-ai",
         null,
         null,
         null
        ],
        [
         "6",
         "Odyssey 102B",
         "Biology",
         "Protein or nucleotide language model (pLM/nLM),Protein generation",
         "Anthrogen",
         "Ankit Singhal, Shyam Venkatasubramanian, Sean Moushegian, Steven Strutt, Michael Lin, Connor Lee",
         "2025-10-18",
         "Odyssey: reconstructing evolution through emergent consensus in the global proteome",
         "https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1",
         null,
         null,
         null,
         "102000000000.0",
         "102B",
         "1.1e+23",
         "\"trained over 1.1 × 10^23 FLOPs\" from the abstract",
         null,
         "Table 2\n3.662B proteins",
         null,
         null,
         null,
         null,
         "Confident",
         "We present Odyssey, a family of multimodal protein language models for sequence and structure generation, protein editing and design. We scale Odyssey to more than 102 billion parameters, trained over 1.1 × 1023 FLOPs. The Odyssey architecture uses context modalities, categorized as structural cues, semantic descriptions, and orthologous group metadata, and comprises two main components: a finite scalar quantizer for tokenizing continuous atomic coordinates, and a transformer stack for multimodal representation learning. Odyssey is trained via discrete diffusion, and characterizes the generative process as a time-dependent unmasking procedure. The finite scalar quantizer and transformer stack leverage the consensus mechanism, a replacement for attention that uses an iterative propagation scheme informed by local agreements between residues. Across various benchmarks, Odyssey achieves landmark performance for protein generation and protein structure discretization. Our empirical findings are supported by theoretical analysis.",
         null,
         null,
         "Unreleased",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-20 17:25:36+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Reported",
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "Odyssey 12B",
         "Biology",
         "Protein or nucleotide language model (pLM/nLM),Protein generation",
         "Anthrogen",
         "Ankit Singhal, Shyam Venkatasubramanian, Sean Moushegian, Steven Strutt, Michael Lin, Connor Lee",
         "2025-10-18",
         "Odyssey: reconstructing evolution through emergent consensus in the global proteome",
         "https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1",
         null,
         null,
         null,
         "12000000000.0",
         "12B",
         null,
         null,
         null,
         "Table 2\n3.662B proteins",
         null,
         null,
         null,
         null,
         "Confident",
         "We present Odyssey, a family of multimodal protein language models for sequence and structure generation, protein editing and design. We scale Odyssey to more than 102 billion parameters, trained over 1.1 × 1023 FLOPs. The Odyssey architecture uses context modalities, categorized as structural cues, semantic descriptions, and orthologous group metadata, and comprises two main components: a finite scalar quantizer for tokenizing continuous atomic coordinates, and a transformer stack for multimodal representation learning. Odyssey is trained via discrete diffusion, and characterizes the generative process as a time-dependent unmasking procedure. The finite scalar quantizer and transformer stack leverage the consensus mechanism, a replacement for attention that uses an iterative propagation scheme informed by local agreements between residues. Across various benchmarks, Odyssey achieves landmark performance for protein generation and protein structure discretization. Our empirical findings are supported by theoretical analysis.",
         null,
         null,
         "Unreleased",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-20 17:25:24+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Reported",
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "Odyssey 1.2B",
         "Biology",
         "Protein or nucleotide language model (pLM/nLM),Protein generation",
         "Anthrogen",
         "Ankit Singhal, Shyam Venkatasubramanian, Sean Moushegian, Steven Strutt, Michael Lin, Connor Lee",
         "2025-10-18",
         "Odyssey: reconstructing evolution through emergent consensus in the global proteome",
         "https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1",
         null,
         null,
         null,
         "1200000000.0",
         "1.2B",
         null,
         null,
         null,
         "Table 2\n3.662B proteins",
         null,
         null,
         null,
         null,
         "Confident",
         "We present Odyssey, a family of multimodal protein language models for sequence and structure generation, protein editing and design. We scale Odyssey to more than 102 billion parameters, trained over 1.1 × 1023 FLOPs. The Odyssey architecture uses context modalities, categorized as structural cues, semantic descriptions, and orthologous group metadata, and comprises two main components: a finite scalar quantizer for tokenizing continuous atomic coordinates, and a transformer stack for multimodal representation learning. Odyssey is trained via discrete diffusion, and characterizes the generative process as a time-dependent unmasking procedure. The finite scalar quantizer and transformer stack leverage the consensus mechanism, a replacement for attention that uses an iterative propagation scheme informed by local agreements between residues. Across various benchmarks, Odyssey achieves landmark performance for protein generation and protein structure discretization. Our empirical findings are supported by theoretical analysis.",
         null,
         null,
         "Unreleased",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-20 17:25:21+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Reported",
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "Claude Haiku 4.5",
         "Language",
         "Chat,Code generation,Language modeling/generation,Question answering",
         "Anthropic",
         null,
         "2025-10-15",
         "Introducing Claude Haiku 4.5",
         "https://www.anthropic.com/news/claude-haiku-4-5",
         null,
         "Significant use",
         "Likely significant use as one of Anthropic's flagship models",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Claude Haiku 4.5, our latest small model, is available today to all users.\n\nWhat was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4.5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.",
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-27 20:18:46+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "Veo 3.1",
         "Video,Vision",
         "Image-to-video,Video generation,Text-to-video,Audio generation",
         "Google DeepMind",
         null,
         "2025-10-15",
         "Introducing Veo 3.1 and advanced capabilities in Flow",
         "https://blog.google/technology/ai/veo-updates-flow/",
         null,
         "SOTA improvement",
         "\"Veo 3.1 has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models.\"\nhttps://deepmind.google/models/veo/evals/",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "We’re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.",
         null,
         null,
         "API access",
         "United States of America,United Kingdom of Great Britain and Northern Ireland",
         null,
         null,
         null,
         null,
         null,
         "2025-10-16 17:07:34+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API 2 and Vertex AI 3\n\n",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "MAI-Image-1",
         "Image generation",
         "Image generation,Text-to-image",
         "Microsoft",
         null,
         "2025-10-13",
         "Introducing MAI-Image-1, debuting in\nthe top 10 on LMArena",
         "https://microsoft.ai/news/introducing-mai-image-1-debuting-in-the-top-10-on-lmarena/",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "NVIDIA GB200",
         null,
         "Unknown",
         "Today, we’re announcing MAI-Image-1, our first image generation model developed entirely in-house, debuting in the top 10 text-to-image models on LMArena.  \n\nAt Microsoft AI, we’re creating AI for everyone – a supportive, helpful presence always in the service of humanity. We’ve shared how purpose-built models are essential for this mission, and we announced our first two in-house models in August. MAI-Image-1 marks the next step on our journey and paves the way for more immersive, creative and dynamic experiences inside our products.  \n\nWe trained this model with the goal of delivering genuine value for creators, and we put a lot of care into avoiding repetitive or generically-stylized outputs. For example, we prioritized rigorous data selection and nuanced evaluation focused on tasks that closely mirror real-world creative use cases – taking into account feedback from professionals in the creative industries. This model is designed to deliver real flexibility, visual diversity and practical value.\n\nMAI-Image-1 excels at generating photorealistic imagery, like lighting (e.g., bounce light, reflections), landscapes, and much more. This is particularly so when compared to many larger, slower models. Its combination of speed and quality means users can get their ideas on screen faster, iterate through them quickly, and then transfer their work to other tools to continue refining.",
         null,
         null,
         "Hosted access (no API)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-16 16:36:52+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Microsoft is now testing its own in-house AI image generation model.\nYou can test the new model at the LMArena leaderboard site.\nThe new model will soon be available in Copilot and Bing Image Creator.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "C2S-Scale",
         "Language,Biology",
         "Cell Biology,RNA sequence generation,Language modeling/generation,Question answering",
         "Google Research,Yale University,Google DeepMind,Brown University,University of Southern California",
         "Syed Asad Rizvi, Daniel Levine, Aakash Patel, Shiyang Zhang, Eric Wang, Curtis Jamison Perry, Nicole Mayerli Constante, Sizhuang He, David Zhang, Cerise Tang, Zhuoyang Lyu, Rayyan Darji, Chang Li, Emily Sun, David Jeong, Lawrence Zhao, Jennifer Kwan, David Braun, Brian Hafler, Hattie Chung, Rahul M. Dhodapkar, Bryan Perozzi, Jeffrey Ishizuka, Shekoofeh Azizi, David van Dijk",
         "2025-10-11",
         "Scaling Large Language Models for Next-Generation Single-Cell Analysis",
         "https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2",
         null,
         null,
         null,
         "27000000000.0",
         "27B",
         null,
         null,
         "1000000000",
         "\"Trained on a massive dataset of over 57 million cells\"\n\n\" a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata\"",
         null,
         null,
         "Google TPU v4",
         null,
         "Likely",
         "Single-cell RNA sequencing has transformed our understanding of cellular diversity, yet current singlecell foundation models (scFMs) remain limited in their scalability, flexibility across diverse tasks, and ability to natively integrate textual information. In this work, we build upon the Cell2Sentence (C2S) framework, which represents scRNA-seq profiles as textual “cell sentences,” to train Large Language Models (LLMs) on a corpus comprising over one billion tokens of transcriptomic data, biological text, and metadata. Scaling the model to 27 billion parameters yields consistent improvements in predictive and generative capabilities and supports advanced downstream tasks that require synthesis of information across multi-cellular contexts. Targeted fine-tuning with modern reinforcement learning techniques produces strong performance in perturbation response prediction, natural language interpretation, and complex biological reasoning. This predictive strength directly enabled a dualcontext virtual screen that uncovered a striking context split for the kinase inhibitor silmitasertib (CX-4945), suggesting its potential as a synergistic, interferon-conditional amplifier of antigen presentation. Experimental validation in human cell models unseen during training confirmed this hypothesis, demonstrating that C2S-Scale can generate biologically grounded, testable discoveries of context-conditioned biology. C2S-Scale unifies transcriptomic and textual data at unprecedented scales, surpassing both specialized single-cell models and general-purpose LLMs to provide a platform for next-generation single-cell analysis and the development of “virtual cells.”",
         null,
         null,
         "Open weights (unrestricted)",
         "United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
         "Gemma 2 27B",
         "1.62e+20",
         "6 FLOP/parameter/token * 27000000000 parameters * 1000000000 tokens = 1.62e20 FLOP\n\n\"Likely\" confidence because number of epochs is unknown",
         "256.0",
         null,
         "2025-10-16 18:04:34+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry,Academia,Industry,Academia,Academia",
         null,
         null,
         null,
         null,
         "Unreleased",
         "cc-by-4.0\nhttps://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B\n\nAttribution-NonCommercial-NoDerivatives 4.0 International\nhttps://github.com/vandijklab/cell2sentence",
         null,
         null,
         null,
         null,
         null,
         "84997.92421707121",
         "Operation counting",
         "vandijklab",
         null,
         null,
         null
        ],
        [
         "13",
         "Ring-1T",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
         "Ant Group",
         null,
         "2025-10-10",
         "Ring-1T, flow state leads to sudden enlightenment",
         "https://huggingface.co/inclusionAI/Ring-1T",
         null,
         null,
         null,
         "1000000000000.0",
         "1 trillion total parameters with 50 billion activated parameters",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "Today, we officially launch the trillion-parameter thinking model, Ring-1T. It is open-source upon release—developers can download the model weights from Hugging Face and ModelScope, or experience direct chat interactions and API calls via the Ling Chat page and ZenMux (links provided at the end of the article).\n\nBuilding upon the preview version released at the end of last month, Ring-1T has undergone continued scaling with large-scale verifiable reward reinforcement learning (RLVR) training, further unlocking the natural language reasoning capabilities of the trillion-parameter foundation model. Through RLHF training, the model's general abilities have also been refined, making this release of Ring-1T more balanced in performance across various tasks.\n\nRing-1T adopts the Ling 2.0 architecture and is trained on the Ling-1T-base foundation model, which contains 1 trillion total parameters with 50 billion activated parameters, supporting a context window of up to 128K tokens. Leveraging our self-developed icepop reinforcement learning stabilization method and the efficient reinforcement learning system ASystem (whose AReaL framework is already open-source), we have achieved smooth scaling of MoE architecture reinforcement learning—from tens of billions (Ring-mini-2.0) to hundreds of billions (Ring-flash-2.0) to trillions (Ring-1T) of parameters—significantly enhancing the model's deep reasoning and natural language inference capabilities.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Ling-1T",
         null,
         null,
         null,
         null,
         "2025-10-16 15:54:46+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/inclusionAI/Ring-1T",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inclusionAI",
         null,
         null,
         null
        ],
        [
         "14",
         "Ling-1T",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",
         "Ant Group",
         null,
         "2025-10-10",
         "Ling-1T",
         "https://huggingface.co/inclusionAI/Ling-1T",
         null,
         "SOTA improvement",
         "\"We comprehensively evaluated Ling-1T against leading flagship models, including both open-source giants (e.g., DeepSeek-V3.1-Terminus, Kimi-K2-Instruct-0905) and closed-source APIs (GPT-5-main, Gemini-2.5-Pro). Across code generation, software development, competition-level mathematics, professional math, and logical reasoning, Ling-1T consistently demonstrates superior complex reasoning ability and overall advantage.\"",
         "1000000000000.0",
         "1 trillion total parameters with 50 billion activated parameters",
         "6.000001e+24",
         "6 FLOP/parameter/token * 50000000000 active parameters * 20000000000000 tokens = 6e+24 FLOP",
         "20000000000000",
         "\"Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens\"",
         null,
         null,
         null,
         null,
         "Confident",
         "Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with ≈ 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.\n\nPre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the model’s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks—balancing accuracy and efficiency.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-16 15:49:57+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/inclusionAI/Ling-1T",
         null,
         null,
         null,
         "FP8",
         null,
         null,
         "Operation counting",
         "inclusionAI",
         null,
         null,
         null
        ],
        [
         "15",
         "Grok Imagine",
         "Image generation,Video",
         "Image generation,Video generation,Text-to-image,Text-to-video,Image-to-image,Image-to-video",
         "xAI",
         null,
         "2025-10-08",
         "Turn wild ideas into viral content with Grok Imagine",
         "https://grokimagine.ai/",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "\"trained on billions of examples\"",
         null,
         null,
         null,
         null,
         "Unknown",
         "The AI platform that turns your wildest ideas into trending content. While others learn Photoshop, you're already going viral with Grok Imagine. Experience Grok Spicy for advanced video and image generation.\nAdvanced AI Image & Video Generation\nGrok Imagine revolutionizes content creation with advanced AI technology. Experience Grok Imagine's ability to deliver high-quality images across multiple domains and videos of various lengths with sound, making Grok Imagine the ultimate creative platform.\nText-to-Image Generation\nCreate photorealistic images from text prompts with Aurora engine. Generate high-quality portraits, logos, artwork, and visual content across multiple domains with precise detail rendering.\nAI Video Creation\nTransform text descriptions into dynamic videos of various lengths with synchronized audio. Perfect for social media content and longer-form video creation with AI-powered generation.\nMulti-Domain Image Quality\nGenerate high-resolution images up to 1024×1024 pixels across various domains where other models struggle. Create realistic portraits, detailed logos, branded content, and artistic styles.\nPhotorealistic Rendering\nExperience Aurora engine's autoregressive approach for precise visual detail control. Generate coherent, high-quality images with accurate text rendering, logos, and realistic human portraits.\nAurora Engine Technology\nGrok Imagine leverages cutting-edge Aurora technology trained on billions of examples. Discover why Grok Imagine delivers the most precise text-to-image generation with multimodal input support that sets Grok Imagine apart from competitors.",
         null,
         null,
         "Hosted access (no API)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-17 17:05:30+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "GPT-5 Pro",
         "Multimodal,Language,Vision",
         null,
         "OpenAI",
         null,
         "2025-10-07",
         "GPT-5 System Card",
         "https://cdn.openai.com/gpt-5-system-card.pdf",
         null,
         "Significant use,Historical significance",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         null,
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-15 00:03:42+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "Gemini 2.5 Computer Use",
         "Language,Vision,Video,Multimodal,Speech",
         "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR),System control,Search",
         "Google",
         null,
         "2025-10-07",
         "Introducing the Gemini 2.5 Computer Use model",
         "https://blog.google/technology/google-deepmind/gemini-computer-use-model/",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Today, we are releasing the Gemini 2.5 Computer Use model, our new specialized model built on Gemini 2.5 Pro’s visual understanding and reasoning capabilities that powers agents capable of interacting with user interfaces (UIs). It outperforms leading alternatives on multiple web and mobile control benchmarks, all with lower latency. Developers can access these capabilities via the Gemini API in Google AI Studio and Vertex AI.",
         null,
         null,
         "API access",
         "United States of America",
         "Gemini 2.5 Pro",
         null,
         null,
         null,
         null,
         "2025-10-16 17:20:17+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Google AI Studio\nGemini API\nVertex AI",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "Tiny Recursive Model (TRM-Att)",
         "Language,Vision,Multimodal",
         "Language modeling/generation,Question answering,Visual puzzles",
         "Samsung SAIT AI Lab",
         "Alexia Jolicoeur-Martineau",
         "2025-10-06",
         "Less is More: Recursive Reasoning with Tiny Networks",
         "https://arxiv.org/abs/2510.04871",
         null,
         null,
         null,
         "7000000.0",
         "7M",
         "3.07742976e+20",
         "989400000000000 FLOP/GPU/sec * 72 hours * 3600 sec / hour * 4 GPUs * 0.3 [assumed utilization] = 3.07742976e20 FLOP",
         null,
         null,
         "72.0",
         "\"Experiments on ARC-AGI were ran for around 3 days with 4 H100 with 80Gb of RAM.\"",
         "NVIDIA H100 SXM5 80GB",
         null,
         "Confident",
         "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
         null,
         null,
         "Unreleased",
         "Korea (Republic of)",
         null,
         null,
         null,
         "4.0",
         null,
         "2025-10-20 19:53:06+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Open source",
         "MIT license\nhttps://github.com/SamsungSAILMontreal/TinyRecursiveModels",
         null,
         null,
         null,
         null,
         null,
         "5469.225394941245",
         "Hardware",
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "Granite-4.0-H-Tiny",
         "Language",
         "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
         "IBM",
         null,
         "2025-10-02",
         "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
         "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
         null,
         null,
         null,
         "7000000000.0",
         "a hybrid MoE with 7B total parameters (1B active)",
         "1.35e+23",
         "6 FLOP/parameter/token * 1000000000 active parameters * 22500000000000 tokens = 1.35e+23 FLOP",
         null,
         "\"all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.\"\n\n4 training stages: 15T+5T+2T+0.5T = 22.5T",
         null,
         null,
         "NVIDIA GB200",
         null,
         "Confident",
         "We’re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.\nThese new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the world’s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.\nGranite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",
         null,
         null,
         "Open weights (unrestricted)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-08 21:01:36+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/ibm-granite/granite-4.0-h-tiny",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "ibm-granite",
         null,
         null,
         null
        ],
        [
         "20",
         "Granite-4.0-H-Micro",
         "Language",
         "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
         "IBM",
         null,
         "2025-10-02",
         "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
         "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
         null,
         null,
         null,
         "3000000000.0",
         " dense hybrid model with 3B parameters.",
         "3.15e+23",
         "6 FLOP/parameter/token * 3000000000 parameters * 17500000000000 tokens = 3.15e+23 FLOP",
         null,
         "\"all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.\"\n\n4 training stages: 10T+5T+2T+0.5T = 17.5T",
         null,
         null,
         "NVIDIA GB200",
         null,
         "Confident",
         "We’re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.\nThese new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the world’s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.\nGranite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",
         null,
         null,
         "Open weights (unrestricted)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-08 21:01:53+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/ibm-granite/granite-4.0-h-micro",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "ibm-granite",
         null,
         null,
         null
        ],
        [
         "21",
         "Granite-4.0-H-Small",
         "Language",
         "Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",
         "IBM",
         null,
         "2025-10-02",
         "IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",
         "https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",
         null,
         null,
         null,
         "32000000000.0",
         "a hybrid mixture of experts (MoE) model with 32B total parameters (9B active)",
         "1.215e+24",
         "6 FLOP/parameter/token * 9000000000 active parameters * 22500000000000 tokens = 1.215e+24 FLOP",
         null,
         "\"all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.\"\n\n4 training stages: 15T+5T+2T+0.5T = 22.5T",
         null,
         null,
         "NVIDIA GB200",
         null,
         "Confident",
         "We’re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.\nThese new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the world’s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.\nGranite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",
         null,
         null,
         "Open weights (unrestricted)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-08 21:01:02+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/ibm-granite/granite-4.0-h-small",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "ibm-granite",
         null,
         null,
         null
        ],
        [
         "22",
         "Octave 2",
         "Speech",
         "Speech synthesis,Text-to-speech (TTS)",
         "Hume",
         null,
         "2025-10-01",
         "Octave 2: next-generation multilingual voice AI",
         "https://www.hume.ai/blog/octave-2-launch",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Today we’re launching Octave (Omni capable text and voice engine) 2, the second generation of our frontier voice AI model for text-to-speech. We just made a preview of Octave 2 available on our platform and through our API.\n\nOctave 2: \nMore deeply understands the emotional tone of speech.\n\nExtends our text-to-speech system to 11 languages.\n\nIs 40% faster and more efficient, generating audio in under 200ms. \n\nOffers new first-of-their-kind features for a speech-language model, including voice conversion and direct phoneme editing.\n\nPronounces uncommon words, repeated words, numbers, and symbols more reliably.\n\nIs half the price of Octave 1.",
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-09 16:48:08+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "EVI 4 mini",
         "Speech",
         "Speech-to-speech,Audio question answering",
         "Hume",
         null,
         "2025-10-01",
         "Lightweight model tuned for emotive multilingual experiences",
         "https://www.hume.ai/blog/octave-2-launch",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Finally, we're launching EVI 4 mini, which brings all of the capabilities of Octave 2 to our speech-to-speech API. Now, you can build faster, smoother interactive experiences in 11 languages. For example, we built a translator app using EVI 4 mini with just a few voice samples and a prompt.\n\nEVI 4 mini doesn't yet generate its own language natively, so you'll need to pair it with an external LLM through our API until we launch the full version.\n\nAccess Octave 2 and EVI 4 mini\nToday, we're rolling out access to Octave 2 on our text-to-speech playground and API, and to EVI 4 mini on our speech-to-speech playground and API.",
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-09 16:51:46+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "GLM 4.6",
         "Language",
         "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,System control",
         "Zhipu AI,Tsinghua University",
         null,
         "2025-09-30",
         "GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities",
         "https://z.ai/blog/glm-4.6",
         null,
         "Discretionary",
         "\"We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4.\"",
         "357000000000.0",
         "Similarly to GLM 4.5: 355 billion total parameters (reported) with 32 billion active parameters (assumed)",
         "4.42e+24",
         "6 FLOP/parameter/token * 32000000000 active parameters [very likely assumption - everything else is reported to be same as at GLM 4.5] * 23000000000000 tokens = 4.42e24 FLOP",
         "23000000000000",
         "23T tokens (from Jaime's correspondence with the GLM team)",
         "2880.0",
         "4 months (from Jaime's correspondence with the GLM team)\n\n4 months * 30 days * 24 hours = 2880 hours",
         null,
         null,
         "Likely",
         "Today, we are releasing the latest version of our flagship model: GLM-4.6. Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code、Cline、Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.2-Exp and Claude Sonnet 4, but still lags behind Claude Sonnet 4.5 in coding ability.",
         null,
         null,
         "Open weights (unrestricted)",
         "China,China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-24 17:51:50+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry,Academia",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/zai-org/GLM-4.6",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "zai-org",
         null,
         null,
         null
        ],
        [
         "25",
         "Sora 2.0",
         "Video",
         "Video generation,Text-to-video",
         "OpenAI",
         null,
         "2025-09-30",
         "Sora 2 is here",
         "https://openai.com/index/sora-2/",
         null,
         "Significant use",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.\n\nToday we’re releasing Sora 2, our flagship video and audio generation model.\n\nThe original Sora model⁠ from February 2024 was in many ways the GPT‑1 moment for video—the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.",
         null,
         null,
         "Hosted access (no API)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-08 21:29:54+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "Kandinsky 5.0 Video Lite",
         "Video",
         "Video generation,Text-to-video",
         "Sber",
         "Project Leader: Denis Dimitrov\n\nTeam Leads: Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko, Denis Parkhomenko\n\nCore Contributors: Alexey Letunovskiy, Maria Kovaleva, Ivan Kirillov, Lev Novitskiy, Denis Koposov, Dmitrii Mikhailov, Anna Averchenkova, Andrey Shutkin, Julia Agafonova, Olga Kim, Anastasiia Kargapoltseva, Nikita Kiselev\n\nContributors: Anna Dmitrienko, Anastasia Maltseva, Kirill Chernyshev, Ilia Vasiliev, Viacheslav Vasilev, Vladimir Polovnikov, Yury Kolabushin, Alexander Belykh, Mikhail Mamaev, Anastasia Aliaskina, Tatiana Nikulina, Polina Gavrilova",
         "2025-09-30",
         "Kandinsky 5.0 Video Lite is the best open-source high-quality video generator in the lightweight class.",
         "https://habr.com/ru/companies/sberbank/articles/951800/",
         null,
         null,
         null,
         "2000000000.0",
         "2B",
         null,
         null,
         null,
         "To create the pretrain dataset, we collected a massive dataset of 6 billion images and 35 million videos, which we then sliced ​​(using the pyscenedetect scene change detector) into 1.5 billion short scenes ranging from 2 to 60 seconds. We then filtered out samples that:\n\nwere too low-resolution: up to 256 pixels on the shortest side;\n\nwere duplicates and very similar;\n\nwere watermarked;\n\nwere overloaded with text (document photos, etc.);\n\nwere not dynamic enough.\n\nFrom the remaining data, we selected 124 million scenes and 520 million images that were the most aesthetically pleasing and technically sound.",
         null,
         null,
         null,
         null,
         "Confident",
         "We are releasing Kandinsky 5.0 Video Lite, the first model in the new Kandinsky 5 series. The model runs at 768x512 resolution and, despite its compact size of just 2 billion parameters, demonstrates quality superior to previous versions of Kandinsky and most current open-source state-of-the-art solutions. A key focus is efficiency: the model is compact, requires fewer resources, and generates faster. This result was achieved through a comprehensive approach—from data collection and preparation to pretraining and fine-tuning. We explored modern architecture optimization methods and applied our own developments to balance quality and speed.",
         null,
         null,
         "Open weights (unrestricted)",
         "Russia",
         null,
         null,
         null,
         null,
         null,
         "2025-10-16 21:12:35+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry,Government",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://github.com/ai-forever/Kandinsky-5\n\nhttps://huggingface.co/ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-5s",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "ai-forever",
         null,
         null,
         null
        ],
        [
         "27",
         "Claude Sonnet 4.5",
         "Language,Vision,Multimodal",
         "Language modeling/generation,Code generation,System control,Question answering,Quantitative reasoning,Mathematical reasoning,Visual question answering",
         "Anthropic",
         null,
         "2025-09-29",
         "Introducing Claude Sonnet 4.5",
         "https://www.anthropic.com/news/claude-sonnet-4-5",
         null,
         "SOTA improvement",
         "\"Claude Sonnet 4.5 is the best coding model in the world\"\n\n\"Claude Sonnet 4.5 is state-of-the-art on the SWE-bench Verified evaluation\"",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math.\nCode is everywhere. It runs every application, spreadsheet, and software tool you use. Being able to use those tools and reason through hard problems is how modern work gets done.\nClaude Sonnet 4.5 makes this possible. We're releasing it along with a set of major upgrades to our products. In Claude Code, we've added checkpoints—one of our most requested features—that save your progress and allow you to roll back instantly to a previous state. We've refreshed the terminal interface and shipped a native VS Code extension. We've added a new context editing feature and memory tool to the Claude API that lets agents run even longer and handle even greater complexity. In the Claude apps, we've brought code execution and file creation (spreadsheets, slides, and documents) directly into the conversation. And we've made the Claude for Chrome extension available to Max users who joined the waitlist last month.",
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-28 10:12:41+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "NVIDIA Isaac GR00T N1.6",
         "Robotics,Vision,Language",
         "Robotic manipulation,Animal (human/non-human) imitation,Instruction interpretation",
         "NVIDIA",
         null,
         "2025-09-29",
         "NVIDIA Accelerates Robotics Research and Development With New Open Models and Simulation Libraries",
         "https://nvidianews.nvidia.com/news/nvidia-accelerates-robotics-research-and-development-with-new-open-models-and-simulation-libraries",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Isaac GR00T features robot foundation models for cognition and control, simulation frameworks built on NVIDIA Omniverse™ and Cosmos™, data pipelines for generating synthetic data and environments, and a computer in the robot—Jetson AGX Thor™—to run the entire robot stack.",
         null,
         null,
         "Unreleased",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-09 16:36:25+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "\"available soon on Hugging Face\"",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "Cosmos-Transfer2.5-2B",
         "Video,Image generation",
         "Video generation,Text-to-video,Image-to-video,Image generation,Text-to-image,Video-to-video",
         "NVIDIA",
         null,
         "2025-09-29",
         "World Generation with Adaptive Multimodal Control",
         "https://research.nvidia.com/labs/dir/cosmos-transfer2.5/\n\nhttps://research.nvidia.com/publication/2025-09_world-simulation-video-foundation-models-physical-ai",
         null,
         null,
         null,
         "2000000000.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI visionlanguage model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning–based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5× smaller than [Cosmos-Transfer1], it delivers higher\nfidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks\nunder the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.\n",
         null,
         null,
         "Open weights (restricted use)",
         "United States of America",
         "Cosmos-Predict2.5 2B",
         null,
         null,
         null,
         null,
         "2025-10-09 18:32:56+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Nvidia open model license\nhttps://huggingface.co/nvidia/Cosmos-Transfer2.5-2B",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "nvidia",
         null,
         null,
         null
        ],
        [
         "30",
         "Cosmos-Predict2.5-14B",
         "Video",
         "Video generation,Text-to-video,Image-to-video,Video-to-video",
         "NVIDIA",
         null,
         "2025-09-29",
         "World Simulation with Video Foundation Models for Physical AI",
         "https://research.nvidia.com/publication/2025-09_world-simulation-video-foundation-models-physical-ai",
         null,
         null,
         null,
         "14000000000.0",
         null,
         null,
         null,
         null,
         "\"Trained on 200M curated video clips and refined with reinforcement learning–based post-training\"\n\n\"we apply the same 1 × 2 × 2 patchification strategy to compress latent features further. We train our model to generate 93 frames, which corresponds to 24 latent frames, at a time using 16 fps videos. Each of the generated videos is about 5.8 seconds long.\"\n\n200*10^6 * 5.8 / 3600 = 322222 hours of video",
         null,
         null,
         "NVIDIA H100 SXM5 80GB",
         null,
         "Confident",
         "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI visionlanguage model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning–based post-training, [Cosmos-Predict2.5]\nachieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5× smaller than [Cosmos-Transfer1], it delivers higher\nfidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks\nunder the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.\n",
         null,
         null,
         "Unreleased",
         "United States of America",
         null,
         null,
         null,
         "4096.0",
         "0.3308",
         "2025-10-09 18:32:56+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         "5601359.907709977",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "Cosmos-Predict2.5 2B",
         "Video",
         "Video generation,Text-to-video,Image-to-video,Video-to-video",
         "NVIDIA",
         null,
         "2025-09-29",
         "World Simulation with Video Foundation Models for Physical AI",
         "https://research.nvidia.com/publication/2025-09_world-simulation-video-foundation-models-physical-ai",
         null,
         null,
         null,
         "2000000000.0",
         null,
         null,
         null,
         null,
         "\"Trained on 200M curated video clips and refined with reinforcement learning–based post-training\"\n\n\"we apply the same 1 × 2 × 2 patchification strategy to compress latent features further. We train our model to generate 93 frames, which corresponds to 24 latent frames, at a time using 16 fps videos. Each of the generated videos is about 5.8 seconds long.\"\n\n200*10^6 * 5.8 / 3600 = 322222 hours of video",
         null,
         null,
         "NVIDIA H100 SXM5 80GB",
         null,
         "Confident",
         "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI visionlanguage model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning–based post-training, [Cosmos-Predict2.5]\nachieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5× smaller than [Cosmos-Transfer1], it delivers higher\nfidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks\nunder the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.\n",
         null,
         null,
         "Open weights (restricted use)",
         "United States of America",
         null,
         null,
         null,
         "4096.0",
         "0.3649",
         "2025-10-09 18:32:54+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Nvidia open license\nhttps://huggingface.co/nvidia/Cosmos-Predict2.5-2B",
         null,
         null,
         null,
         null,
         null,
         "5601359.907709977",
         null,
         "nvidia",
         null,
         null,
         null
        ],
        [
         "32",
         "DeepSeek-V3.2-Exp",
         "Language",
         "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",
         "DeepSeek",
         null,
         "2025-09-29",
         "Introducing DeepSeek-V3.2-Exp",
         "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf",
         null,
         null,
         null,
         "671000000000.0",
         "671B total, 37B active (assuming same as in the base model)",
         "3.8035594e+24",
         "3.594058e+24 FLOP [base model] + 2.095014e+23 FLOP = 3.8035594e+24 FLOP",
         "943700000000",
         "\"We train both the main model and the indexer for 15000 steps, with each step consisting of 480 sequences of 128K tokens, resulting in a total of 943.7B tokens.\"",
         null,
         null,
         null,
         null,
         "Confident",
         "We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "DeepSeek-V3.1-Terminus",
         "2.095014e+23",
         "6 FLOP/parameter/token * 37000000000 parameters * 943700000000 tokens = 2.095014e+23 FLOP",
         null,
         null,
         "2025-10-16 16:58:23+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "deepseek-ai",
         null,
         null,
         null
        ],
        [
         "33",
         "MinerU2.5",
         "Language,Vision,Multimodal",
         "Visual question answering,Character recognition (OCR)",
         "Shanghai AI Lab,Peking University,Shanghai Jiao Tong University",
         "Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, Conghui He",
         "2025-09-29",
         "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing",
         "https://arxiv.org/abs/2509.22186",
         null,
         null,
         "\"SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.\"",
         "1200000000.0",
         null,
         null,
         null,
         null,
         "558K 665K 6.9M 630K - amount of samples on each stage",
         null,
         null,
         null,
         null,
         "Confident",
         "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead.",
         null,
         null,
         "Open weights (unrestricted)",
         "China,China,China",
         "Qwen2-VL-2B,Qwen2-0.5B",
         null,
         null,
         null,
         null,
         "2025-10-16 18:48:46+00:00",
         null,
         null,
         null,
         null,
         null,
         "Academia,Academia,Academia",
         null,
         null,
         null,
         null,
         "Unreleased",
         "agpl-3.0\nhttps://huggingface.co/opendatalab/MinerU2.5-2509-1.2B\nhttps://github.com/opendatalab/MinerU",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "opendatalab",
         null,
         null,
         null
        ],
        [
         "34",
         "Wan 2.5",
         "Video,Vision",
         "Video generation,Audio generation,Text-to-video,Image-to-video",
         "Alibaba",
         null,
         "2025-09-29",
         "Wan 2.5 AI Video Generator with Audio Sync",
         "https://www.wan-ai.co/wan-2-5",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Wan 2.5 is a state-of-the-art AI video generation model by Alibaba, available on DashScope. It transforms text or images into high-quality 480p/720p/1080p videos with perfectly synchronized audio.\n\nCreate professional, audio-synced videos from a single prompt. Wan 2.5 generates voice, music, and perfectly matched lip-sync in one pass.\n\nwan 2.5 turns a clear, well-structured prompt into a complete talking video—voiceover, music, and precise lip-sync included. With wan 2.5, there's no separate voice recording, no manual timeline nudging, and no third-party tools. One pass, one file, done. Teams using wan 2.5 move faster and publish more consistently.\n\nWhether it's subtle facial micro-expressions or large, dynamic gestures, wan 2.5 keeps motion natural and steady. A wide dynamic range helps wan 2.5 avoid jitter, stutter, and uncanny artifacts, so footage looks polished end-to-end. Longer clips remain stable too—wan 2.5 is built for reliability.\n\nPrompts in Chinese or other minor languages stay A/V-synchronized with wan 2.5. Where Veo 3 may surface \"unknown language\" on mixed-language inputs, wan 2.5 maintains clear alignment and pronunciation. For cross-border campaigns and global classrooms, wan 2.5 makes multilingual production practical.",
         null,
         null,
         "Unreleased",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-16 20:43:33+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Wan 2.5 is available as an open-source model, and it’s also accessible via Alibaba Cloud’s DashScope platform for enterprise use.\n\n[10/16/25 - not yet released]",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "Seedream 4.0",
         "Image generation",
         "Image generation,Text-to-image",
         "ByteDance",
         "Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, Wenjia Zhu",
         "2025-09-28",
         "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
         "https://arxiv.org/abs/2509.20427",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on this https URL.",
         null,
         null,
         "API access",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-17 19:17:46+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "Kling 2.5 Turbo",
         "Video,Vision",
         "Image-to-video,Video generation",
         "Kuaishou Technology",
         null,
         "2025-09-26",
         "Kling AI Launches 2.5 Turbo Video Model: Industry-Leading Performance, Now More Affordable",
         "https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-launches-25-turbo-video-model-industry-leading",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "HONG KONG, September 26, 2025 (GLOBE NEWSWIRE) -- Kuaishou Technology (“Kuaishou” or the “Company”; HKD Counter Stock Code: 01024 / RMB Counter Stock Code: 81024), a leading content community and social platform, announced the launch of its Kling AI 2.5 Turbo Video Model on September 23. The latest model features major upgrades to its text-to-video and image-to-video capabilities, significantly enhancing generation quality with industry-leading performance, while further reducing utilization costs for greater affordability.\n\nIndustry-Leading Evaluation Results\n\nIn a blind test with professionals evaluating model performance, Kling AI’s 2.5 Turbo Video Model outperformed competitors across both text-to-video and image-to-video tasks.   In text-to-video, it achieved win-loss ratios of 285%, 212% and 160% compared with Seedance 1.0 mini, Veo3-fast and Seedance 1.0, respectively. In image-to-video, the model secured win-loss ratios of 208%, 289% and 164% against the same respective benchmarks.",
         null,
         null,
         "API access",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-17 18:48:13+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "Suno v5",
         "Audio",
         "Audio generation",
         "Suno",
         null,
         "2025-09-25",
         "Introducing v5",
         "https://help.suno.com/en/articles/8105153",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "v5 is officially here and available to our Pro and Premier users. Suno v5 represents our most significant technological leap yet:\n\n• Intelligent Composition Architecture: v5 maintains flawless structural coherence across any format—from punchy 30-second hooks to expansive 8-minute epics. Every section flows naturally into the next, with professional transitions and dynamic progression.\n\n• Adaptive Creative Intelligence: The system learns your style preferences and adjusts in real-time, offering suggestions that align with your creative vision while pushing boundaries when you want to explore.\n\n• Streamlined Creative Workflow: Redesigned interface eliminates friction between idea and execution. Complex operations now happen with single actions, making professional production accessible to creators at every level.\n\n• Persistent Voice & Instrument Memory: Characters and instrumental signatures remain consistent throughout your project. Build layered arrangements where each element maintains its unique identity across multiple generations.\n\n• Professional Control Suite: Access granular parameters for tempo, key, dynamics, and arrangement without overwhelming complexity. Fine-tune every aspect or let v5 handle the technical details.\n\n• Zero-Compromise Performance: Generate quality tracks in seconds, not minutes. v5 delivers 10x faster processing while producing significantly higher quality output than any previous version.\n\n• Intelligent Arrangement Engine: Automatically structures your composition with verses, choruses, bridges, and outros that make musical sense. Or take manual control and craft unconventional structures that break the rules.",
         null,
         null,
         "Hosted access (no API)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-03 17:59:43+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "Gemini Robotics 1.5 ",
         "Robotics,Vision,Language",
         "Robotic manipulation,Instruction interpretation",
         "Google DeepMind",
         "Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Peter Pastor Sampedro, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Li Yang Ku, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou and Yuxiang Zhou",
         "2025-09-25",
         "Gemini Robotics 1.5 brings AI agents into the physical world",
         "https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/\n\nhttps://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Google TPU v4,Google TPU v5e,Google TPU v6e Trillium",
         null,
         "Unknown",
         "Our most capable vision-language-action (VLA) model turns visual information and instructions into motor commands for a robot to perform a task. This model thinks before taking action and shows its process, helping robots assess and complete complex tasks more transparently. It also learns across embodiments, accelerating skill learning.",
         null,
         null,
         "Unreleased",
         "United States of America,United Kingdom of Great Britain and Northern Ireland",
         null,
         null,
         null,
         null,
         null,
         "2025-10-06 18:43:24+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Gemini Robotics 1.5 is currently available to select partners.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "Gemini Robotics-ER 1.5",
         "Vision,Language,Speech",
         "Instruction interpretation,Robotic manipulation,Image captioning,Object detection,Search,Language modeling/generation,Question answering,Speech recognition (ASR)",
         "Google DeepMind",
         "Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Peter Pastor Sampedro, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, René Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Li Yang Ku, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou and Yuxiang Zhou",
         "2025-09-25",
         "Gemini Robotics 1.5 brings AI agents into the physical world",
         "https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/\n\nhttps://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf",
         null,
         "SOTA improvement",
         "Table 19\n\"Our model achieves the highest aggregated performance on 15 academic embodied reasoning benchmarks, including Point-Bench, RefSpatial, RoboSpatial-Pointing, Where2Place, BLINK, CV-Bench, ERQA, EmbSpatial, MindCube, RoboSpatial-VQA, SAT, Cosmos-Reason1, Min Video Pairs, OpenEQA and VSI-Bench.\"",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Google TPU v4,Google TPU v5e,Google TPU v6e Trillium",
         null,
         "Unknown",
         "Our most capable vision-language model (VLM) reasons about the physical world, natively calls digital tools and creates detailed, multi-step plans to complete a mission. This model now achieves state-of-the-art performance across spatial understanding benchmarks.",
         null,
         null,
         "API access",
         "United States of America,United Kingdom of Great Britain and Northern Ireland",
         null,
         null,
         null,
         null,
         null,
         "2025-10-06 18:43:24+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Starting today, we’re making Gemini Robotics-ER 1.5 available to developers via the Gemini API in Google AI Studio.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "GigaEmbeddings",
         "Language",
         "Semantic embedding",
         "Sber,Moscow Institute of Physics and Technology",
         "Egor Kolodin, Daria Khomich, Nikita Savushkin, Anastasia Ianina, Fyodor Minkin",
         "2025-09-25",
         "GigaEmbeddings — Efficient Russian Language Embedding Model",
         "https://aclanthology.org/2025.bsnlp-1.3/",
         null,
         null,
         "\"Evaluated on the ruMTEB benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves state-of-the-art results (69.1 avg. score), outperforming strong baselines with a larger number of parameters.\"",
         "3000000000.0",
         "3B",
         null,
         null,
         "49000000000",
         "batch size 16K\nmax steps 6000\nmax length 512\n\n16000 * 6000 * 512 = 49B",
         null,
         null,
         null,
         null,
         "Confident",
         "We introduce GigaEmbeddings, a novel framework for training high-performance Russian-focused text embeddings through hierarchical instruction tuning of the decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our three-stage pipeline, comprising large-scale contrastive pre-training in web-scale corpora, fine-tuning with hard negatives, and multitask generalization across retrieval, classification, and clustering tasks, addresses key limitations of existing methods by unifying diverse objectives and leveraging synthetic data generation. Architectural innovations include bidirectional attention for contextual modeling, latent attention pooling for robust sequence aggregation, and strategic pruning of 25% of transformer layers to enhance efficiency without compromising performance. Evaluated on the ruMTEB benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves state-of-the-art results (69.1 avg. score), outperforming strong baselines with a larger number of parameters.",
         null,
         null,
         "Open weights (unrestricted)",
         "Russia,Russia",
         "GigaChat Lite (GigaChat-20B-A3B)",
         "8.82e+20",
         "6 FLOP/parameter/token * 3000000000 parameters * 49000000000 tokens = 882000000000000000000 FLOP",
         null,
         null,
         "2025-10-16 21:17:36+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry,Government,Academia",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/ai-sage/Giga-Embeddings-instruct",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "ai-sage",
         null,
         null,
         null
        ],
        [
         "41",
         "SimpleFold",
         "Biology",
         "Protein folding prediction",
         "Apple",
         "Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh Susskind, Miguel Angel Bautista",
         "2025-09-23",
         "SimpleFold: Folding Proteins is Simpler than You Think",
         "https://arxiv.org/abs/2509.18480v1",
         null,
         null,
         null,
         "3000000000.0",
         "3B",
         "2e+21",
         "6 FLOP/parameter/token * 3000000000 parameters * 2227200000 amino acids [see dataset size notes] * 141 epochs = 5.6526336e+21 FLOP\n\nfrom figure 4 ~2e+12 GFLOPs = 2e+21 FLOP",
         "2227200000",
         "\"8.7M [data structures] for the 3B model\"\n\"We set the maximal amino acid sequence length to 256\"\n\"except for 1.6B and 3B models which are trained with batch size 1024 and 3072, respectively\"\nfrom figure 4 ~400k training steps\n\n8.7*10^6 * 256 = 2,227,200,000 max gradient updates per epoch\n3072*256*400,000 = 314,572,800,000 max total gradient updates\n~141 epochs",
         null,
         null,
         null,
         null,
         "Confident",
         "Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.",
         "141.0",
         null,
         "Open weights (unrestricted)",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-17 18:20:45+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "https://github.com/apple/ml-simplefold",
         null,
         null,
         null,
         null,
         null,
         null,
         "Reported,Operation counting",
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "DeepSeek-V3.1-Terminus",
         "Language",
         "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",
         "DeepSeek",
         null,
         "2025-09-22",
         "The latest update builds on V3.1’s strengths while addressing key user feedback.",
         "https://api-docs.deepseek.com/news/news250922",
         null,
         null,
         null,
         "671000000000.0",
         "671B total, 37B active",
         "3.594058e+24",
         "3.407799999999999e+24 FLOP [base model] + 1.86258e+23 FLOP = 3.594058e+24 FLOP",
         "839000000000",
         "From V3.1 training dataset information:\n\"840B tokens continued pretraining for long context extension on top of V3\"\n\n\"The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\"\n\n839B in total",
         null,
         null,
         null,
         null,
         "Likely",
         " What’s improved?\n Language consistency: fewer CN/EN mix-ups & no more random chars.\n Agent upgrades: stronger Code Agent & Search Agent performance.\n DeepSeek-V3.1-Terminus delivers more stable & reliable outputs across benchmarks compared to the previous version.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "DeepSeek-V3",
         "1.86258e+23",
         "6 FLOP/parameter/token * 671000000000 parameters * 839000000000 tokens = 3.377814e+24 FLOP\n\n",
         null,
         null,
         "2025-10-16 16:58:23+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "MIT license\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "deepseek-ai",
         null,
         null,
         null
        ],
        [
         "43",
         "Qwen3-Omni-Flash",
         "Multimodal,Language,Vision,Speech,Video",
         "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Speech recognition (ASR),Speech synthesis,Speech-to-text,Text-to-speech (TTS)",
         "Alibaba",
         "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
         "2025-09-22",
         "Qwen3-Omni Technical Report",
         "https://arxiv.org/abs/2509.17765",
         null,
         null,
         null,
         "35300000000.0",
         "Audio Encoder AuT 650M \nVision Encoder SigLIP2-So400M 540M \nThinker MoE Transformer 30B-A3B\nTalker MoE Transformer 3B-A0.3B\nMTP Dense Transformer 80M\nCode2wav ConvNet 200M",
         "3.6e+22",
         "6 FLOP/parameter/token * 3000000000 active parameters * 2000000000000 tokens = 3.6e+22 FLOP",
         null,
         "\"our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio\"\n\n\"The second phase of pretraining utilizes a large-scale dataset containing\napproximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion)\"",
         null,
         null,
         null,
         null,
         "Confident",
         "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
         null,
         null,
         "Unreleased",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-03 19:31:18+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "Qwen3-Omni-30B-A3B",
         "Multimodal,Language,Vision,Speech,Video",
         "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Speech recognition (ASR),Speech synthesis,Speech-to-text,Text-to-speech (TTS)",
         "Alibaba",
         "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
         "2025-09-22",
         "Qwen3-Omni Technical Report",
         "https://arxiv.org/abs/2509.17765",
         null,
         "SOTA improvement",
         "\"Qwen3-Omni achieves SOTA on 32 benchmarks and overall SOTA on 22 across 36 audio and audio-visual benchmarks, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\"",
         "35300000000.0",
         "Audio Encoder AuT 650M \nVision Encoder SigLIP2-So400M 540M \nThinker MoE Transformer 30B-A3B\nTalker MoE Transformer 3B-A0.3B\nMTP Dense Transformer 80M\nCode2wav ConvNet 200M",
         "3.6e+22",
         "6 FLOP/parameter/token * 3000000000 active parameters * 2000000000000 tokens = 3.6e+22 FLOP",
         "2000000000000",
         "\"our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio\"\n\n\"The second phase of pretraining utilizes a large-scale dataset containing\napproximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion)\"",
         null,
         null,
         null,
         null,
         "Confident",
         "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         null,
         null,
         null,
         null,
         null,
         "2025-10-03 19:30:25+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "Qwen",
         null,
         null,
         null
        ],
        [
         "45",
         "Grok 4 Fast",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Search,Code generation,Mathematical reasoning",
         "xAI",
         null,
         "2025-09-19",
         "Pushing the Frontier of Cost-Efficient Intelligence",
         "https://x.ai/news/grok-4-fast",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "We're thrilled to present Grok 4 Fast, our latest advancement in cost-efficient reasoning models. Built on xAI’s learnings from Grok 4, Grok 4 Fast delivers frontier-level performance across Enterprise and Consumer domains—with exceptional token efficiency. This model pushes the boundaries for smaller and faster AI, making high-quality reasoning accessible to more users and developers. Grok 4 Fast features state-of-the-art (SOTA) cost-efficiency, cutting-edge web and X search capabilities, a 2M token context window, and a unified architecture that blends reasoning and non-reasoning modes in one model.",
         null,
         null,
         "API access",
         "United States of America",
         null,
         null,
         null,
         null,
         null,
         "2025-10-01 23:33:54+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "Magistral Small 1.2",
         "Language,Vision,Multimodal",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation,Visual question answering,Image captioning",
         "Mistral AI",
         null,
         "2025-09-18",
         "Introducing Magistral Small 1.2 & Magistral Medium 1.2, minor updates to our Magistral 1.1 models!",
         "https://huggingface.co/mistralai/Magistral-Small-2509",
         null,
         null,
         null,
         "24000000000.0",
         "24B",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Confident",
         "Introducing Magistral Small 1.2 & Magistral Medium 1.2, minor updates to our Magistral 1.1 models!\n\n- Multimodality: Now equipped with a vision encoder, these models handle both text and images seamlessly.\n- Performance Boost: 15% improvements on math and coding benchmarks such as AIME 24/25 and LiveCodeBench v5/v6.\n- Smarter Tool Use: Better tool usage with web search, code interpreter, and image generation.\n- Better Tone & Persona: Responses are clearer, more natural, and better formatted for you.\n",
         null,
         null,
         "Open weights (unrestricted)",
         "France",
         "Mistral Small 3.2",
         null,
         null,
         null,
         null,
         "2025-09-28 01:00:10+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/mistralai/Magistral-Small-2509",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "mistralai",
         null,
         null,
         null
        ],
        [
         "47",
         "Magistral Medium 1.2",
         "Language",
         "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",
         "Mistral AI",
         null,
         "2025-09-18",
         "Introducing Magistral Small 1.2 & Magistral Medium 1.2, minor updates to our Magistral 1.1 models!",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Unknown",
         "Introducing Magistral Small 1.2 & Magistral Medium 1.2, minor updates to our Magistral 1.1 models!\n\n- Multimodality: Now equipped with a vision encoder, these models handle both text and images seamlessly.\n- Performance Boost: 15% improvements on math and coding benchmarks such as AIME 24/25 and LiveCodeBench v5/v6.\n- Smarter Tool Use: Better tool usage with web search, code interpreter, and image generation.\n- Better Tone & Persona: Responses are clearer, more natural, and better formatted for you.\n",
         null,
         null,
         "API access",
         "France",
         null,
         null,
         null,
         null,
         null,
         "2025-09-28 01:00:10+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "Granite-Docling",
         "Vision,Multimodal,Language",
         "Visual question answering,Character recognition (OCR),Retrieval-augmented generation",
         "IBM",
         null,
         "2025-09-17",
         "IBM Granite-Docling: End-to-end document understanding with one tiny model",
         "https://www.ibm.com/new/announcements/granite-docling-end-to-end-document-conversion",
         null,
         null,
         null,
         "258000000.0",
         "258M",
         null,
         null,
         null,
         null,
         null,
         "\"We train granite-docling-258m using IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\"",
         "NVIDIA H100 SXM5 80GB",
         null,
         "Confident",
         "Today, IBM is releasing Granite-Docling-258M, an ultra-compact and cutting-edge open-source vision-language model (VLM) for converting documents to machine-readable formats while fully preserving their layout, tables, equations, lists and more. It’s now available on Hugging Face through a standard Apache 2.0 license.\n\nGranite-Docling is purpose-built for accurate and efficient document conversion, unlike most VLM-based approaches to optical character recognition (OCR) that aim to adapt large, general-purpose models to the task. Even at an ultra-compact 258M parameters, Granite-Docling’s capabilities rival those of systems several times its size, making it extremely cost-effective. The model goes well beyond mere text extraction: it handles both inline and floating math and code, excels at recognizing table structure and preserves the layout and structure of the original document. Whereas conventional OCR models convert documents directly to Markdown and lose connection to the source content, Granite-Docling’s unique method of faithfully translating complex structural elements makes its output ideal for downstream RAG applications.",
         null,
         null,
         "Open weights (unrestricted)",
         "United States of America",
         "SigLIP 2",
         null,
         null,
         null,
         null,
         "2025-09-28 01:00:10+00:00",
         "IBM",
         "\"We train granite-docling-258m using IBM's super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\"",
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/ibm-granite/granite-docling-258M",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "ibm-granite",
         null,
         null,
         null
        ],
        [
         "49",
         "AgentFounder-30B",
         "Language",
         "Language modeling/generation,Question answering,System control,Quantitative reasoning,Mathematical reasoning,Code generation,Search",
         "Alibaba",
         "Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
         "2025-09-16",
         "Scaling Agents via Continual Pre-training",
         "https://arxiv.org/abs/2509.13310",
         null,
         "SOTA improvement",
         "\"We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.\"",
         "30000000000.0",
         "30B-A3B",
         "6.5367e+23",
         "6.48e+23 FLOP [base model compute] + 5.67e+21 FLOP = 6.5367e+23 FLOP",
         "315000000000",
         "We train AgentFounder models with data volumes ranging from 0B to 315B tokens\n\nAgentic CPT Stage 1: We process approximately 200B tokens\nAgentic CPT Stage 2: We further refine these capabilities using 100B tokens",
         null,
         null,
         null,
         null,
         "Confident",
         "Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
         null,
         null,
         "Open weights (unrestricted)",
         "China",
         "Qwen3-30B-A3B",
         "5.67e+21",
         "6 FLOP/parameter/token * 3000000000 active parameters * 315000000000 tokens = 5.67e+21 FLOP",
         null,
         null,
         "2025-10-16 16:30:09+00:00",
         null,
         null,
         null,
         null,
         null,
         "Industry",
         null,
         null,
         null,
         null,
         "Unreleased",
         "Apache 2.0\nhttps://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\n\nhttps://github.com/Alibaba-NLP/DeepResearch",
         null,
         null,
         null,
         null,
         null,
         null,
         "Operation counting",
         "Alibaba-NLP",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 56,
        "rows": 50
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Task</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Link</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Notability criteria</th>\n",
       "      <th>...</th>\n",
       "      <th>Training compute cost (2023 USD)</th>\n",
       "      <th>Utilization notes</th>\n",
       "      <th>Numerical format</th>\n",
       "      <th>Frontier model</th>\n",
       "      <th>Training power draw (W)</th>\n",
       "      <th>Training compute estimation method</th>\n",
       "      <th>Hugging Face developer id</th>\n",
       "      <th>Post-training compute (FLOP)</th>\n",
       "      <th>Post-training compute notes</th>\n",
       "      <th>Hardware utilization (HFU)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emu3.5</td>\n",
       "      <td>Video,Multimodal,Image generation,Vision,Langu...</td>\n",
       "      <td>Text-to-video,Image-to-video,Image generation,...</td>\n",
       "      <td>Beijing Academy of Artificial Intelligence / BAAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>Emu3.5: Native Multimodal Models are World Lea...</td>\n",
       "      <td>https://emu.world/Emu35_tech_report.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tongyi DeepResearch</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang...</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>Tongyi DeepResearch Technical Report</td>\n",
       "      <td>https://arxiv.org/abs/2510.24701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alibaba-NLP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MiniMax-M2</td>\n",
       "      <td>Language</td>\n",
       "      <td>Code generation,System control,Search,Language...</td>\n",
       "      <td>MiniMax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>MiniMax M2 &amp; Agent: Ingenious in Simplicity</td>\n",
       "      <td>https://www.minimax.io/news/minimax-m2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discretionary</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MiniMaxAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ring-mini-linear-2.0</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Ant Group</td>\n",
       "      <td>Bin Han, Caizhi Tang, Chen Liang, Donghao Zhan...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Every Attention Matters: An Efficient Hybrid A...</td>\n",
       "      <td>https://arxiv.org/abs/2510.19338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FP8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.936352e+05</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>inclusionAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ring-flash-linear-2.0</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Ant Group</td>\n",
       "      <td>Bin Han, Caizhi Tang, Chen Liang, Donghao Zhan...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Every Attention Matters: An Efficient Hybrid A...</td>\n",
       "      <td>https://arxiv.org/abs/2510.19338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FP8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.373724e+04</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>inclusionAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepseek OCR</td>\n",
       "      <td>Vision,Language</td>\n",
       "      <td>Character recognition (OCR),Visual question an...</td>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>Haoran Wei, Yaofeng Sun, Yukun Li</td>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>DeepSeek-OCR: Contexts Optical Compression</td>\n",
       "      <td>https://arxiv.org/abs/2510.18234v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.249691e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deepseek-ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Odyssey 102B</td>\n",
       "      <td>Biology</td>\n",
       "      <td>Protein or nucleotide language model (pLM/nLM)...</td>\n",
       "      <td>Anthrogen</td>\n",
       "      <td>Ankit Singhal, Shyam Venkatasubramanian, Sean ...</td>\n",
       "      <td>2025-10-18</td>\n",
       "      <td>Odyssey: reconstructing evolution through emer...</td>\n",
       "      <td>https://www.biorxiv.org/content/10.1101/2025.1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reported</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Odyssey 12B</td>\n",
       "      <td>Biology</td>\n",
       "      <td>Protein or nucleotide language model (pLM/nLM)...</td>\n",
       "      <td>Anthrogen</td>\n",
       "      <td>Ankit Singhal, Shyam Venkatasubramanian, Sean ...</td>\n",
       "      <td>2025-10-18</td>\n",
       "      <td>Odyssey: reconstructing evolution through emer...</td>\n",
       "      <td>https://www.biorxiv.org/content/10.1101/2025.1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reported</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Odyssey 1.2B</td>\n",
       "      <td>Biology</td>\n",
       "      <td>Protein or nucleotide language model (pLM/nLM)...</td>\n",
       "      <td>Anthrogen</td>\n",
       "      <td>Ankit Singhal, Shyam Venkatasubramanian, Sean ...</td>\n",
       "      <td>2025-10-18</td>\n",
       "      <td>Odyssey: reconstructing evolution through emer...</td>\n",
       "      <td>https://www.biorxiv.org/content/10.1101/2025.1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reported</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Claude Haiku 4.5</td>\n",
       "      <td>Language</td>\n",
       "      <td>Chat,Code generation,Language modeling/generat...</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>Introducing Claude Haiku 4.5</td>\n",
       "      <td>https://www.anthropic.com/news/claude-haiku-4-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Veo 3.1</td>\n",
       "      <td>Video,Vision</td>\n",
       "      <td>Image-to-video,Video generation,Text-to-video,...</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>Introducing Veo 3.1 and advanced capabilities ...</td>\n",
       "      <td>https://blog.google/technology/ai/veo-updates-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MAI-Image-1</td>\n",
       "      <td>Image generation</td>\n",
       "      <td>Image generation,Text-to-image</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-13</td>\n",
       "      <td>Introducing MAI-Image-1, debuting in\\nthe top ...</td>\n",
       "      <td>https://microsoft.ai/news/introducing-mai-imag...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C2S-Scale</td>\n",
       "      <td>Language,Biology</td>\n",
       "      <td>Cell Biology,RNA sequence generation,Language ...</td>\n",
       "      <td>Google Research,Yale University,Google DeepMin...</td>\n",
       "      <td>Syed Asad Rizvi, Daniel Levine, Aakash Patel, ...</td>\n",
       "      <td>2025-10-11</td>\n",
       "      <td>Scaling Large Language Models for Next-Generat...</td>\n",
       "      <td>https://www.biorxiv.org/content/10.1101/2025.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.499792e+04</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>vandijklab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ring-1T</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Ant Group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-10</td>\n",
       "      <td>Ring-1T, flow state leads to sudden enlightenment</td>\n",
       "      <td>https://huggingface.co/inclusionAI/Ring-1T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inclusionAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ling-1T</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Ant Group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-10</td>\n",
       "      <td>Ling-1T</td>\n",
       "      <td>https://huggingface.co/inclusionAI/Ling-1T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FP8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>inclusionAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grok Imagine</td>\n",
       "      <td>Image generation,Video</td>\n",
       "      <td>Image generation,Video generation,Text-to-imag...</td>\n",
       "      <td>xAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-08</td>\n",
       "      <td>Turn wild ideas into viral content with Grok I...</td>\n",
       "      <td>https://grokimagine.ai/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GPT-5 Pro</td>\n",
       "      <td>Multimodal,Language,Vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-07</td>\n",
       "      <td>GPT-5 System Card</td>\n",
       "      <td>https://cdn.openai.com/gpt-5-system-card.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Significant use,Historical significance</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gemini 2.5 Computer Use</td>\n",
       "      <td>Language,Vision,Video,Multimodal,Speech</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-07</td>\n",
       "      <td>Introducing the Gemini 2.5 Computer Use model</td>\n",
       "      <td>https://blog.google/technology/google-deepmind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tiny Recursive Model (TRM-Att)</td>\n",
       "      <td>Language,Vision,Multimodal</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Samsung SAIT AI Lab</td>\n",
       "      <td>Alexia Jolicoeur-Martineau</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>Less is More: Recursive Reasoning with Tiny Ne...</td>\n",
       "      <td>https://arxiv.org/abs/2510.04871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.469225e+03</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Granite-4.0-H-Tiny</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>IBM Granite 4.0: hyper-efficient, high perform...</td>\n",
       "      <td>https://www.ibm.com/new/announcements/ibm-gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>ibm-granite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Granite-4.0-H-Micro</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>IBM Granite 4.0: hyper-efficient, high perform...</td>\n",
       "      <td>https://www.ibm.com/new/announcements/ibm-gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>ibm-granite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Granite-4.0-H-Small</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-02</td>\n",
       "      <td>IBM Granite 4.0: hyper-efficient, high perform...</td>\n",
       "      <td>https://www.ibm.com/new/announcements/ibm-gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>ibm-granite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Octave 2</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech synthesis,Text-to-speech (TTS)</td>\n",
       "      <td>Hume</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>Octave 2: next-generation multilingual voice AI</td>\n",
       "      <td>https://www.hume.ai/blog/octave-2-launch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EVI 4 mini</td>\n",
       "      <td>Speech</td>\n",
       "      <td>Speech-to-speech,Audio question answering</td>\n",
       "      <td>Hume</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>Lightweight model tuned for emotive multilingu...</td>\n",
       "      <td>https://www.hume.ai/blog/octave-2-launch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GLM 4.6</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Zhipu AI,Tsinghua University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>GLM-4.6: Advanced Agentic, Reasoning and Codin...</td>\n",
       "      <td>https://z.ai/blog/glm-4.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discretionary</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>zai-org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sora 2.0</td>\n",
       "      <td>Video</td>\n",
       "      <td>Video generation,Text-to-video</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>Sora 2 is here</td>\n",
       "      <td>https://openai.com/index/sora-2/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Significant use</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kandinsky 5.0 Video Lite</td>\n",
       "      <td>Video</td>\n",
       "      <td>Video generation,Text-to-video</td>\n",
       "      <td>Sber</td>\n",
       "      <td>Project Leader: Denis Dimitrov\\n\\nTeam Leads: ...</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>Kandinsky 5.0 Video Lite is the best open-sour...</td>\n",
       "      <td>https://habr.com/ru/companies/sberbank/article...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ai-forever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Claude Sonnet 4.5</td>\n",
       "      <td>Language,Vision,Multimodal</td>\n",
       "      <td>Language modeling/generation,Code generation,S...</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Introducing Claude Sonnet 4.5</td>\n",
       "      <td>https://www.anthropic.com/news/claude-sonnet-4-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NVIDIA Isaac GR00T N1.6</td>\n",
       "      <td>Robotics,Vision,Language</td>\n",
       "      <td>Robotic manipulation,Animal (human/non-human) ...</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>NVIDIA Accelerates Robotics Research and Devel...</td>\n",
       "      <td>https://nvidianews.nvidia.com/news/nvidia-acce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cosmos-Transfer2.5-2B</td>\n",
       "      <td>Video,Image generation</td>\n",
       "      <td>Video generation,Text-to-video,Image-to-video,...</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>World Generation with Adaptive Multimodal Control</td>\n",
       "      <td>https://research.nvidia.com/labs/dir/cosmos-tr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Cosmos-Predict2.5-14B</td>\n",
       "      <td>Video</td>\n",
       "      <td>Video generation,Text-to-video,Image-to-video,...</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>World Simulation with Video Foundation Models ...</td>\n",
       "      <td>https://research.nvidia.com/publication/2025-0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.601360e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Cosmos-Predict2.5 2B</td>\n",
       "      <td>Video</td>\n",
       "      <td>Video generation,Text-to-video,Image-to-video,...</td>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>World Simulation with Video Foundation Models ...</td>\n",
       "      <td>https://research.nvidia.com/publication/2025-0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.601360e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nvidia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>DeepSeek-V3.2-Exp</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Code generation,Q...</td>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Introducing DeepSeek-V3.2-Exp</td>\n",
       "      <td>https://github.com/deepseek-ai/DeepSeek-V3.2-E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>deepseek-ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MinerU2.5</td>\n",
       "      <td>Language,Vision,Multimodal</td>\n",
       "      <td>Visual question answering,Character recognitio...</td>\n",
       "      <td>Shanghai AI Lab,Peking University,Shanghai Jia...</td>\n",
       "      <td>Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang...</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>MinerU2.5: A Decoupled Vision-Language Model f...</td>\n",
       "      <td>https://arxiv.org/abs/2509.22186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>opendatalab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Wan 2.5</td>\n",
       "      <td>Video,Vision</td>\n",
       "      <td>Video generation,Audio generation,Text-to-vide...</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Wan 2.5 AI Video Generator with Audio Sync</td>\n",
       "      <td>https://www.wan-ai.co/wan-2-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Seedream 4.0</td>\n",
       "      <td>Image generation</td>\n",
       "      <td>Image generation,Text-to-image</td>\n",
       "      <td>ByteDance</td>\n",
       "      <td>Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qi...</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>Seedream 4.0: Toward Next-generation Multimoda...</td>\n",
       "      <td>https://arxiv.org/abs/2509.20427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Kling 2.5 Turbo</td>\n",
       "      <td>Video,Vision</td>\n",
       "      <td>Image-to-video,Video generation</td>\n",
       "      <td>Kuaishou Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>Kling AI Launches 2.5 Turbo Video Model: Indus...</td>\n",
       "      <td>https://ir.kuaishou.com/news-releases/news-rel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Suno v5</td>\n",
       "      <td>Audio</td>\n",
       "      <td>Audio generation</td>\n",
       "      <td>Suno</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>Introducing v5</td>\n",
       "      <td>https://help.suno.com/en/articles/8105153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Gemini Robotics 1.5</td>\n",
       "      <td>Robotics,Vision,Language</td>\n",
       "      <td>Robotic manipulation,Instruction interpretation</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>Abbas Abdolmaleki, Saminda Abeyruwan, Joshua A...</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>Gemini Robotics 1.5 brings AI agents into the ...</td>\n",
       "      <td>https://deepmind.google/discover/blog/gemini-r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Gemini Robotics-ER 1.5</td>\n",
       "      <td>Vision,Language,Speech</td>\n",
       "      <td>Instruction interpretation,Robotic manipulatio...</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>Abbas Abdolmaleki, Saminda Abeyruwan, Joshua A...</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>Gemini Robotics 1.5 brings AI agents into the ...</td>\n",
       "      <td>https://deepmind.google/discover/blog/gemini-r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GigaEmbeddings</td>\n",
       "      <td>Language</td>\n",
       "      <td>Semantic embedding</td>\n",
       "      <td>Sber,Moscow Institute of Physics and Technology</td>\n",
       "      <td>Egor Kolodin, Daria Khomich, Nikita Savushkin,...</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>GigaEmbeddings — Efficient Russian Language Em...</td>\n",
       "      <td>https://aclanthology.org/2025.bsnlp-1.3/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>ai-sage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SimpleFold</td>\n",
       "      <td>Biology</td>\n",
       "      <td>Protein folding prediction</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh S...</td>\n",
       "      <td>2025-09-23</td>\n",
       "      <td>SimpleFold: Folding Proteins is Simpler than Y...</td>\n",
       "      <td>https://arxiv.org/abs/2509.18480v1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reported,Operation counting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>DeepSeek-V3.1-Terminus</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Code generation,Q...</td>\n",
       "      <td>DeepSeek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>The latest update builds on V3.1’s strengths w...</td>\n",
       "      <td>https://api-docs.deepseek.com/news/news250922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>deepseek-ai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Qwen3-Omni-Flash</td>\n",
       "      <td>Multimodal,Language,Vision,Speech,Video</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, X...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>Qwen3-Omni Technical Report</td>\n",
       "      <td>https://arxiv.org/abs/2509.17765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Qwen3-Omni-30B-A3B</td>\n",
       "      <td>Multimodal,Language,Vision,Speech,Video</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, X...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>Qwen3-Omni Technical Report</td>\n",
       "      <td>https://arxiv.org/abs/2509.17765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Grok 4 Fast</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>xAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>Pushing the Frontier of Cost-Efficient Intelli...</td>\n",
       "      <td>https://x.ai/news/grok-4-fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Magistral Small 1.2</td>\n",
       "      <td>Language,Vision,Multimodal</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Introducing Magistral Small 1.2 &amp; Magistral Me...</td>\n",
       "      <td>https://huggingface.co/mistralai/Magistral-Sma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mistralai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Magistral Medium 1.2</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Introducing Magistral Small 1.2 &amp; Magistral Me...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Granite-Docling</td>\n",
       "      <td>Vision,Multimodal,Language</td>\n",
       "      <td>Visual question answering,Character recognitio...</td>\n",
       "      <td>IBM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>IBM Granite-Docling: End-to-end document under...</td>\n",
       "      <td>https://www.ibm.com/new/announcements/granite-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ibm-granite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AgentFounder-30B</td>\n",
       "      <td>Language</td>\n",
       "      <td>Language modeling/generation,Question answerin...</td>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen...</td>\n",
       "      <td>2025-09-16</td>\n",
       "      <td>Scaling Agents via Continual Pre-training</td>\n",
       "      <td>https://arxiv.org/abs/2509.13310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOTA improvement</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operation counting</td>\n",
       "      <td>Alibaba-NLP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  \\\n",
       "0                           Emu3.5   \n",
       "1              Tongyi DeepResearch   \n",
       "2                       MiniMax-M2   \n",
       "3             Ring-mini-linear-2.0   \n",
       "4            Ring-flash-linear-2.0   \n",
       "5                     Deepseek OCR   \n",
       "6                     Odyssey 102B   \n",
       "7                      Odyssey 12B   \n",
       "8                     Odyssey 1.2B   \n",
       "9                 Claude Haiku 4.5   \n",
       "10                         Veo 3.1   \n",
       "11                     MAI-Image-1   \n",
       "12                       C2S-Scale   \n",
       "13                         Ring-1T   \n",
       "14                         Ling-1T   \n",
       "15                    Grok Imagine   \n",
       "16                       GPT-5 Pro   \n",
       "17         Gemini 2.5 Computer Use   \n",
       "18  Tiny Recursive Model (TRM-Att)   \n",
       "19              Granite-4.0-H-Tiny   \n",
       "20             Granite-4.0-H-Micro   \n",
       "21             Granite-4.0-H-Small   \n",
       "22                        Octave 2   \n",
       "23                      EVI 4 mini   \n",
       "24                         GLM 4.6   \n",
       "25                        Sora 2.0   \n",
       "26        Kandinsky 5.0 Video Lite   \n",
       "27               Claude Sonnet 4.5   \n",
       "28         NVIDIA Isaac GR00T N1.6   \n",
       "29           Cosmos-Transfer2.5-2B   \n",
       "30           Cosmos-Predict2.5-14B   \n",
       "31            Cosmos-Predict2.5 2B   \n",
       "32               DeepSeek-V3.2-Exp   \n",
       "33                       MinerU2.5   \n",
       "34                         Wan 2.5   \n",
       "35                    Seedream 4.0   \n",
       "36                 Kling 2.5 Turbo   \n",
       "37                         Suno v5   \n",
       "38            Gemini Robotics 1.5    \n",
       "39          Gemini Robotics-ER 1.5   \n",
       "40                  GigaEmbeddings   \n",
       "41                      SimpleFold   \n",
       "42          DeepSeek-V3.1-Terminus   \n",
       "43                Qwen3-Omni-Flash   \n",
       "44              Qwen3-Omni-30B-A3B   \n",
       "45                     Grok 4 Fast   \n",
       "46             Magistral Small 1.2   \n",
       "47            Magistral Medium 1.2   \n",
       "48                 Granite-Docling   \n",
       "49                AgentFounder-30B   \n",
       "\n",
       "                                               Domain  \\\n",
       "0   Video,Multimodal,Image generation,Vision,Langu...   \n",
       "1                                            Language   \n",
       "2                                            Language   \n",
       "3                                            Language   \n",
       "4                                            Language   \n",
       "5                                     Vision,Language   \n",
       "6                                             Biology   \n",
       "7                                             Biology   \n",
       "8                                             Biology   \n",
       "9                                            Language   \n",
       "10                                       Video,Vision   \n",
       "11                                   Image generation   \n",
       "12                                   Language,Biology   \n",
       "13                                           Language   \n",
       "14                                           Language   \n",
       "15                             Image generation,Video   \n",
       "16                         Multimodal,Language,Vision   \n",
       "17            Language,Vision,Video,Multimodal,Speech   \n",
       "18                         Language,Vision,Multimodal   \n",
       "19                                           Language   \n",
       "20                                           Language   \n",
       "21                                           Language   \n",
       "22                                             Speech   \n",
       "23                                             Speech   \n",
       "24                                           Language   \n",
       "25                                              Video   \n",
       "26                                              Video   \n",
       "27                         Language,Vision,Multimodal   \n",
       "28                           Robotics,Vision,Language   \n",
       "29                             Video,Image generation   \n",
       "30                                              Video   \n",
       "31                                              Video   \n",
       "32                                           Language   \n",
       "33                         Language,Vision,Multimodal   \n",
       "34                                       Video,Vision   \n",
       "35                                   Image generation   \n",
       "36                                       Video,Vision   \n",
       "37                                              Audio   \n",
       "38                           Robotics,Vision,Language   \n",
       "39                             Vision,Language,Speech   \n",
       "40                                           Language   \n",
       "41                                            Biology   \n",
       "42                                           Language   \n",
       "43            Multimodal,Language,Vision,Speech,Video   \n",
       "44            Multimodal,Language,Vision,Speech,Video   \n",
       "45                                           Language   \n",
       "46                         Language,Vision,Multimodal   \n",
       "47                                           Language   \n",
       "48                         Vision,Multimodal,Language   \n",
       "49                                           Language   \n",
       "\n",
       "                                                 Task  \\\n",
       "0   Text-to-video,Image-to-video,Image generation,...   \n",
       "1   Language modeling/generation,Question answerin...   \n",
       "2   Code generation,System control,Search,Language...   \n",
       "3   Language modeling/generation,Question answerin...   \n",
       "4   Language modeling/generation,Question answerin...   \n",
       "5   Character recognition (OCR),Visual question an...   \n",
       "6   Protein or nucleotide language model (pLM/nLM)...   \n",
       "7   Protein or nucleotide language model (pLM/nLM)...   \n",
       "8   Protein or nucleotide language model (pLM/nLM)...   \n",
       "9   Chat,Code generation,Language modeling/generat...   \n",
       "10  Image-to-video,Video generation,Text-to-video,...   \n",
       "11                     Image generation,Text-to-image   \n",
       "12  Cell Biology,RNA sequence generation,Language ...   \n",
       "13  Language modeling/generation,Question answerin...   \n",
       "14  Language modeling/generation,Question answerin...   \n",
       "15  Image generation,Video generation,Text-to-imag...   \n",
       "16                                                NaN   \n",
       "17  Language modeling/generation,Question answerin...   \n",
       "18  Language modeling/generation,Question answerin...   \n",
       "19  Language modeling/generation,Question answerin...   \n",
       "20  Language modeling/generation,Question answerin...   \n",
       "21  Language modeling/generation,Question answerin...   \n",
       "22              Speech synthesis,Text-to-speech (TTS)   \n",
       "23          Speech-to-speech,Audio question answering   \n",
       "24  Language modeling/generation,Question answerin...   \n",
       "25                     Video generation,Text-to-video   \n",
       "26                     Video generation,Text-to-video   \n",
       "27  Language modeling/generation,Code generation,S...   \n",
       "28  Robotic manipulation,Animal (human/non-human) ...   \n",
       "29  Video generation,Text-to-video,Image-to-video,...   \n",
       "30  Video generation,Text-to-video,Image-to-video,...   \n",
       "31  Video generation,Text-to-video,Image-to-video,...   \n",
       "32  Language modeling/generation,Code generation,Q...   \n",
       "33  Visual question answering,Character recognitio...   \n",
       "34  Video generation,Audio generation,Text-to-vide...   \n",
       "35                     Image generation,Text-to-image   \n",
       "36                    Image-to-video,Video generation   \n",
       "37                                   Audio generation   \n",
       "38    Robotic manipulation,Instruction interpretation   \n",
       "39  Instruction interpretation,Robotic manipulatio...   \n",
       "40                                 Semantic embedding   \n",
       "41                         Protein folding prediction   \n",
       "42  Language modeling/generation,Code generation,Q...   \n",
       "43  Language modeling/generation,Question answerin...   \n",
       "44  Language modeling/generation,Question answerin...   \n",
       "45  Language modeling/generation,Question answerin...   \n",
       "46  Language modeling/generation,Question answerin...   \n",
       "47  Language modeling/generation,Question answerin...   \n",
       "48  Visual question answering,Character recognitio...   \n",
       "49  Language modeling/generation,Question answerin...   \n",
       "\n",
       "                                         Organization  \\\n",
       "0   Beijing Academy of Artificial Intelligence / BAAI   \n",
       "1                                             Alibaba   \n",
       "2                                             MiniMax   \n",
       "3                                           Ant Group   \n",
       "4                                           Ant Group   \n",
       "5                                            DeepSeek   \n",
       "6                                           Anthrogen   \n",
       "7                                           Anthrogen   \n",
       "8                                           Anthrogen   \n",
       "9                                           Anthropic   \n",
       "10                                    Google DeepMind   \n",
       "11                                          Microsoft   \n",
       "12  Google Research,Yale University,Google DeepMin...   \n",
       "13                                          Ant Group   \n",
       "14                                          Ant Group   \n",
       "15                                                xAI   \n",
       "16                                             OpenAI   \n",
       "17                                             Google   \n",
       "18                                Samsung SAIT AI Lab   \n",
       "19                                                IBM   \n",
       "20                                                IBM   \n",
       "21                                                IBM   \n",
       "22                                               Hume   \n",
       "23                                               Hume   \n",
       "24                       Zhipu AI,Tsinghua University   \n",
       "25                                             OpenAI   \n",
       "26                                               Sber   \n",
       "27                                          Anthropic   \n",
       "28                                             NVIDIA   \n",
       "29                                             NVIDIA   \n",
       "30                                             NVIDIA   \n",
       "31                                             NVIDIA   \n",
       "32                                           DeepSeek   \n",
       "33  Shanghai AI Lab,Peking University,Shanghai Jia...   \n",
       "34                                            Alibaba   \n",
       "35                                          ByteDance   \n",
       "36                                Kuaishou Technology   \n",
       "37                                               Suno   \n",
       "38                                    Google DeepMind   \n",
       "39                                    Google DeepMind   \n",
       "40    Sber,Moscow Institute of Physics and Technology   \n",
       "41                                              Apple   \n",
       "42                                           DeepSeek   \n",
       "43                                            Alibaba   \n",
       "44                                            Alibaba   \n",
       "45                                                xAI   \n",
       "46                                         Mistral AI   \n",
       "47                                         Mistral AI   \n",
       "48                                                IBM   \n",
       "49                                            Alibaba   \n",
       "\n",
       "                                              Authors Publication date  \\\n",
       "0                                                 NaN       2025-10-30   \n",
       "1   Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang...       2025-10-28   \n",
       "2                                                 NaN       2025-10-27   \n",
       "3   Bin Han, Caizhi Tang, Chen Liang, Donghao Zhan...       2025-10-23   \n",
       "4   Bin Han, Caizhi Tang, Chen Liang, Donghao Zhan...       2025-10-23   \n",
       "5                   Haoran Wei, Yaofeng Sun, Yukun Li       2025-10-21   \n",
       "6   Ankit Singhal, Shyam Venkatasubramanian, Sean ...       2025-10-18   \n",
       "7   Ankit Singhal, Shyam Venkatasubramanian, Sean ...       2025-10-18   \n",
       "8   Ankit Singhal, Shyam Venkatasubramanian, Sean ...       2025-10-18   \n",
       "9                                                 NaN       2025-10-15   \n",
       "10                                                NaN       2025-10-15   \n",
       "11                                                NaN       2025-10-13   \n",
       "12  Syed Asad Rizvi, Daniel Levine, Aakash Patel, ...       2025-10-11   \n",
       "13                                                NaN       2025-10-10   \n",
       "14                                                NaN       2025-10-10   \n",
       "15                                                NaN       2025-10-08   \n",
       "16                                                NaN       2025-10-07   \n",
       "17                                                NaN       2025-10-07   \n",
       "18                         Alexia Jolicoeur-Martineau       2025-10-06   \n",
       "19                                                NaN       2025-10-02   \n",
       "20                                                NaN       2025-10-02   \n",
       "21                                                NaN       2025-10-02   \n",
       "22                                                NaN       2025-10-01   \n",
       "23                                                NaN       2025-10-01   \n",
       "24                                                NaN       2025-09-30   \n",
       "25                                                NaN       2025-09-30   \n",
       "26  Project Leader: Denis Dimitrov\\n\\nTeam Leads: ...       2025-09-30   \n",
       "27                                                NaN       2025-09-29   \n",
       "28                                                NaN       2025-09-29   \n",
       "29                                                NaN       2025-09-29   \n",
       "30                                                NaN       2025-09-29   \n",
       "31                                                NaN       2025-09-29   \n",
       "32                                                NaN       2025-09-29   \n",
       "33  Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang...       2025-09-29   \n",
       "34                                                NaN       2025-09-29   \n",
       "35  Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qi...       2025-09-28   \n",
       "36                                                NaN       2025-09-26   \n",
       "37                                                NaN       2025-09-25   \n",
       "38  Abbas Abdolmaleki, Saminda Abeyruwan, Joshua A...       2025-09-25   \n",
       "39  Abbas Abdolmaleki, Saminda Abeyruwan, Joshua A...       2025-09-25   \n",
       "40  Egor Kolodin, Daria Khomich, Nikita Savushkin,...       2025-09-25   \n",
       "41  Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh S...       2025-09-23   \n",
       "42                                                NaN       2025-09-22   \n",
       "43  Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, X...       2025-09-22   \n",
       "44  Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, X...       2025-09-22   \n",
       "45                                                NaN       2025-09-19   \n",
       "46                                                NaN       2025-09-18   \n",
       "47                                                NaN       2025-09-18   \n",
       "48                                                NaN       2025-09-17   \n",
       "49  Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen...       2025-09-16   \n",
       "\n",
       "                                            Reference  \\\n",
       "0   Emu3.5: Native Multimodal Models are World Lea...   \n",
       "1                Tongyi DeepResearch Technical Report   \n",
       "2         MiniMax M2 & Agent: Ingenious in Simplicity   \n",
       "3   Every Attention Matters: An Efficient Hybrid A...   \n",
       "4   Every Attention Matters: An Efficient Hybrid A...   \n",
       "5          DeepSeek-OCR: Contexts Optical Compression   \n",
       "6   Odyssey: reconstructing evolution through emer...   \n",
       "7   Odyssey: reconstructing evolution through emer...   \n",
       "8   Odyssey: reconstructing evolution through emer...   \n",
       "9                        Introducing Claude Haiku 4.5   \n",
       "10  Introducing Veo 3.1 and advanced capabilities ...   \n",
       "11  Introducing MAI-Image-1, debuting in\\nthe top ...   \n",
       "12  Scaling Large Language Models for Next-Generat...   \n",
       "13  Ring-1T, flow state leads to sudden enlightenment   \n",
       "14                                            Ling-1T   \n",
       "15  Turn wild ideas into viral content with Grok I...   \n",
       "16                                  GPT-5 System Card   \n",
       "17      Introducing the Gemini 2.5 Computer Use model   \n",
       "18  Less is More: Recursive Reasoning with Tiny Ne...   \n",
       "19  IBM Granite 4.0: hyper-efficient, high perform...   \n",
       "20  IBM Granite 4.0: hyper-efficient, high perform...   \n",
       "21  IBM Granite 4.0: hyper-efficient, high perform...   \n",
       "22    Octave 2: next-generation multilingual voice AI   \n",
       "23  Lightweight model tuned for emotive multilingu...   \n",
       "24  GLM-4.6: Advanced Agentic, Reasoning and Codin...   \n",
       "25                                     Sora 2 is here   \n",
       "26  Kandinsky 5.0 Video Lite is the best open-sour...   \n",
       "27                      Introducing Claude Sonnet 4.5   \n",
       "28  NVIDIA Accelerates Robotics Research and Devel...   \n",
       "29  World Generation with Adaptive Multimodal Control   \n",
       "30  World Simulation with Video Foundation Models ...   \n",
       "31  World Simulation with Video Foundation Models ...   \n",
       "32                      Introducing DeepSeek-V3.2-Exp   \n",
       "33  MinerU2.5: A Decoupled Vision-Language Model f...   \n",
       "34         Wan 2.5 AI Video Generator with Audio Sync   \n",
       "35  Seedream 4.0: Toward Next-generation Multimoda...   \n",
       "36  Kling AI Launches 2.5 Turbo Video Model: Indus...   \n",
       "37                                     Introducing v5   \n",
       "38  Gemini Robotics 1.5 brings AI agents into the ...   \n",
       "39  Gemini Robotics 1.5 brings AI agents into the ...   \n",
       "40  GigaEmbeddings — Efficient Russian Language Em...   \n",
       "41  SimpleFold: Folding Proteins is Simpler than Y...   \n",
       "42  The latest update builds on V3.1’s strengths w...   \n",
       "43                        Qwen3-Omni Technical Report   \n",
       "44                        Qwen3-Omni Technical Report   \n",
       "45  Pushing the Frontier of Cost-Efficient Intelli...   \n",
       "46  Introducing Magistral Small 1.2 & Magistral Me...   \n",
       "47  Introducing Magistral Small 1.2 & Magistral Me...   \n",
       "48  IBM Granite-Docling: End-to-end document under...   \n",
       "49          Scaling Agents via Continual Pre-training   \n",
       "\n",
       "                                                 Link  Citations  \\\n",
       "0             https://emu.world/Emu35_tech_report.pdf        NaN   \n",
       "1                    https://arxiv.org/abs/2510.24701        NaN   \n",
       "2              https://www.minimax.io/news/minimax-m2        NaN   \n",
       "3                    https://arxiv.org/abs/2510.19338        NaN   \n",
       "4                    https://arxiv.org/abs/2510.19338        NaN   \n",
       "5                  https://arxiv.org/abs/2510.18234v1        NaN   \n",
       "6   https://www.biorxiv.org/content/10.1101/2025.1...        NaN   \n",
       "7   https://www.biorxiv.org/content/10.1101/2025.1...        NaN   \n",
       "8   https://www.biorxiv.org/content/10.1101/2025.1...        NaN   \n",
       "9     https://www.anthropic.com/news/claude-haiku-4-5        NaN   \n",
       "10  https://blog.google/technology/ai/veo-updates-...        NaN   \n",
       "11  https://microsoft.ai/news/introducing-mai-imag...        NaN   \n",
       "12  https://www.biorxiv.org/content/10.1101/2025.0...        NaN   \n",
       "13         https://huggingface.co/inclusionAI/Ring-1T        NaN   \n",
       "14         https://huggingface.co/inclusionAI/Ling-1T        NaN   \n",
       "15                            https://grokimagine.ai/        NaN   \n",
       "16       https://cdn.openai.com/gpt-5-system-card.pdf        NaN   \n",
       "17  https://blog.google/technology/google-deepmind...        NaN   \n",
       "18                   https://arxiv.org/abs/2510.04871        NaN   \n",
       "19  https://www.ibm.com/new/announcements/ibm-gran...        NaN   \n",
       "20  https://www.ibm.com/new/announcements/ibm-gran...        NaN   \n",
       "21  https://www.ibm.com/new/announcements/ibm-gran...        NaN   \n",
       "22           https://www.hume.ai/blog/octave-2-launch        NaN   \n",
       "23           https://www.hume.ai/blog/octave-2-launch        NaN   \n",
       "24                          https://z.ai/blog/glm-4.6        NaN   \n",
       "25                   https://openai.com/index/sora-2/        NaN   \n",
       "26  https://habr.com/ru/companies/sberbank/article...        NaN   \n",
       "27   https://www.anthropic.com/news/claude-sonnet-4-5        NaN   \n",
       "28  https://nvidianews.nvidia.com/news/nvidia-acce...        NaN   \n",
       "29  https://research.nvidia.com/labs/dir/cosmos-tr...        NaN   \n",
       "30  https://research.nvidia.com/publication/2025-0...        NaN   \n",
       "31  https://research.nvidia.com/publication/2025-0...        NaN   \n",
       "32  https://github.com/deepseek-ai/DeepSeek-V3.2-E...        NaN   \n",
       "33                   https://arxiv.org/abs/2509.22186        NaN   \n",
       "34                      https://www.wan-ai.co/wan-2-5        NaN   \n",
       "35                   https://arxiv.org/abs/2509.20427        NaN   \n",
       "36  https://ir.kuaishou.com/news-releases/news-rel...        NaN   \n",
       "37          https://help.suno.com/en/articles/8105153        NaN   \n",
       "38  https://deepmind.google/discover/blog/gemini-r...        NaN   \n",
       "39  https://deepmind.google/discover/blog/gemini-r...        NaN   \n",
       "40           https://aclanthology.org/2025.bsnlp-1.3/        NaN   \n",
       "41                 https://arxiv.org/abs/2509.18480v1        NaN   \n",
       "42      https://api-docs.deepseek.com/news/news250922        NaN   \n",
       "43                   https://arxiv.org/abs/2509.17765        NaN   \n",
       "44                   https://arxiv.org/abs/2509.17765        NaN   \n",
       "45                      https://x.ai/news/grok-4-fast        NaN   \n",
       "46  https://huggingface.co/mistralai/Magistral-Sma...        NaN   \n",
       "47                                                NaN        NaN   \n",
       "48  https://www.ibm.com/new/announcements/granite-...        NaN   \n",
       "49                   https://arxiv.org/abs/2509.13310        NaN   \n",
       "\n",
       "                        Notability criteria  ...  \\\n",
       "0                                       NaN  ...   \n",
       "1                          SOTA improvement  ...   \n",
       "2                             Discretionary  ...   \n",
       "3                                       NaN  ...   \n",
       "4                                       NaN  ...   \n",
       "5                                       NaN  ...   \n",
       "6                                       NaN  ...   \n",
       "7                                       NaN  ...   \n",
       "8                                       NaN  ...   \n",
       "9                           Significant use  ...   \n",
       "10                         SOTA improvement  ...   \n",
       "11                                      NaN  ...   \n",
       "12                                      NaN  ...   \n",
       "13                                      NaN  ...   \n",
       "14                         SOTA improvement  ...   \n",
       "15                                      NaN  ...   \n",
       "16  Significant use,Historical significance  ...   \n",
       "17                                      NaN  ...   \n",
       "18                                      NaN  ...   \n",
       "19                                      NaN  ...   \n",
       "20                                      NaN  ...   \n",
       "21                                      NaN  ...   \n",
       "22                                      NaN  ...   \n",
       "23                                      NaN  ...   \n",
       "24                            Discretionary  ...   \n",
       "25                          Significant use  ...   \n",
       "26                                      NaN  ...   \n",
       "27                         SOTA improvement  ...   \n",
       "28                                      NaN  ...   \n",
       "29                                      NaN  ...   \n",
       "30                                      NaN  ...   \n",
       "31                                      NaN  ...   \n",
       "32                                      NaN  ...   \n",
       "33                                      NaN  ...   \n",
       "34                                      NaN  ...   \n",
       "35                                      NaN  ...   \n",
       "36                                      NaN  ...   \n",
       "37                                      NaN  ...   \n",
       "38                                      NaN  ...   \n",
       "39                         SOTA improvement  ...   \n",
       "40                                      NaN  ...   \n",
       "41                                      NaN  ...   \n",
       "42                                      NaN  ...   \n",
       "43                                      NaN  ...   \n",
       "44                         SOTA improvement  ...   \n",
       "45                                      NaN  ...   \n",
       "46                                      NaN  ...   \n",
       "47                                      NaN  ...   \n",
       "48                                      NaN  ...   \n",
       "49                         SOTA improvement  ...   \n",
       "\n",
       "   Training compute cost (2023 USD)  Utilization notes Numerical format  \\\n",
       "0                               NaN                NaN              NaN   \n",
       "1                               NaN                NaN              NaN   \n",
       "2                               NaN                NaN              NaN   \n",
       "3                               NaN                NaN              FP8   \n",
       "4                               NaN                NaN              FP8   \n",
       "5                               NaN                NaN              NaN   \n",
       "6                               NaN                NaN              NaN   \n",
       "7                               NaN                NaN              NaN   \n",
       "8                               NaN                NaN              NaN   \n",
       "9                               NaN                NaN              NaN   \n",
       "10                              NaN                NaN              NaN   \n",
       "11                              NaN                NaN              NaN   \n",
       "12                              NaN                NaN              NaN   \n",
       "13                              NaN                NaN              NaN   \n",
       "14                              NaN                NaN              FP8   \n",
       "15                              NaN                NaN              NaN   \n",
       "16                              NaN                NaN              NaN   \n",
       "17                              NaN                NaN              NaN   \n",
       "18                              NaN                NaN              NaN   \n",
       "19                              NaN                NaN              NaN   \n",
       "20                              NaN                NaN              NaN   \n",
       "21                              NaN                NaN              NaN   \n",
       "22                              NaN                NaN              NaN   \n",
       "23                              NaN                NaN              NaN   \n",
       "24                              NaN                NaN              NaN   \n",
       "25                              NaN                NaN              NaN   \n",
       "26                              NaN                NaN              NaN   \n",
       "27                              NaN                NaN              NaN   \n",
       "28                              NaN                NaN              NaN   \n",
       "29                              NaN                NaN              NaN   \n",
       "30                              NaN                NaN              NaN   \n",
       "31                              NaN                NaN              NaN   \n",
       "32                              NaN                NaN              NaN   \n",
       "33                              NaN                NaN              NaN   \n",
       "34                              NaN                NaN              NaN   \n",
       "35                              NaN                NaN              NaN   \n",
       "36                              NaN                NaN              NaN   \n",
       "37                              NaN                NaN              NaN   \n",
       "38                              NaN                NaN              NaN   \n",
       "39                              NaN                NaN              NaN   \n",
       "40                              NaN                NaN              NaN   \n",
       "41                              NaN                NaN              NaN   \n",
       "42                              NaN                NaN              NaN   \n",
       "43                              NaN                NaN              NaN   \n",
       "44                              NaN                NaN              NaN   \n",
       "45                              NaN                NaN              NaN   \n",
       "46                              NaN                NaN              NaN   \n",
       "47                              NaN                NaN              NaN   \n",
       "48                              NaN                NaN              NaN   \n",
       "49                              NaN                NaN              NaN   \n",
       "\n",
       "    Frontier model Training power draw (W) Training compute estimation method  \\\n",
       "0              NaN                     NaN                 Operation counting   \n",
       "1              NaN                     NaN                                NaN   \n",
       "2              NaN                     NaN                                NaN   \n",
       "3              NaN            3.936352e+05                 Operation counting   \n",
       "4              NaN            4.373724e+04                 Operation counting   \n",
       "5              NaN            1.249691e+05                                NaN   \n",
       "6              NaN                     NaN                           Reported   \n",
       "7              NaN                     NaN                           Reported   \n",
       "8              NaN                     NaN                           Reported   \n",
       "9              NaN                     NaN                                NaN   \n",
       "10             NaN                     NaN                                NaN   \n",
       "11             NaN                     NaN                                NaN   \n",
       "12             NaN            8.499792e+04                 Operation counting   \n",
       "13             NaN                     NaN                                NaN   \n",
       "14             NaN                     NaN                 Operation counting   \n",
       "15             NaN                     NaN                                NaN   \n",
       "16             NaN                     NaN                                NaN   \n",
       "17             NaN                     NaN                                NaN   \n",
       "18             NaN            5.469225e+03                           Hardware   \n",
       "19             NaN                     NaN                 Operation counting   \n",
       "20             NaN                     NaN                 Operation counting   \n",
       "21             NaN                     NaN                 Operation counting   \n",
       "22             NaN                     NaN                                NaN   \n",
       "23             NaN                     NaN                                NaN   \n",
       "24             NaN                     NaN                 Operation counting   \n",
       "25             NaN                     NaN                                NaN   \n",
       "26             NaN                     NaN                                NaN   \n",
       "27             NaN                     NaN                                NaN   \n",
       "28             NaN                     NaN                                NaN   \n",
       "29             NaN                     NaN                                NaN   \n",
       "30             NaN            5.601360e+06                                NaN   \n",
       "31             NaN            5.601360e+06                                NaN   \n",
       "32             NaN                     NaN                 Operation counting   \n",
       "33             NaN                     NaN                                NaN   \n",
       "34             NaN                     NaN                                NaN   \n",
       "35             NaN                     NaN                                NaN   \n",
       "36             NaN                     NaN                                NaN   \n",
       "37             NaN                     NaN                                NaN   \n",
       "38             NaN                     NaN                                NaN   \n",
       "39             NaN                     NaN                                NaN   \n",
       "40             NaN                     NaN                 Operation counting   \n",
       "41             NaN                     NaN        Reported,Operation counting   \n",
       "42             NaN                     NaN                 Operation counting   \n",
       "43             NaN                     NaN                 Operation counting   \n",
       "44             NaN                     NaN                 Operation counting   \n",
       "45             NaN                     NaN                                NaN   \n",
       "46             NaN                     NaN                                NaN   \n",
       "47             NaN                     NaN                                NaN   \n",
       "48             NaN                     NaN                                NaN   \n",
       "49             NaN                     NaN                 Operation counting   \n",
       "\n",
       "   Hugging Face developer id  Post-training compute (FLOP)  \\\n",
       "0                        NaN                           NaN   \n",
       "1                Alibaba-NLP                           NaN   \n",
       "2                  MiniMaxAI                           NaN   \n",
       "3                inclusionAI                           NaN   \n",
       "4                inclusionAI                           NaN   \n",
       "5                deepseek-ai                           NaN   \n",
       "6                        NaN                           NaN   \n",
       "7                        NaN                           NaN   \n",
       "8                        NaN                           NaN   \n",
       "9                        NaN                           NaN   \n",
       "10                       NaN                           NaN   \n",
       "11                       NaN                           NaN   \n",
       "12                vandijklab                           NaN   \n",
       "13               inclusionAI                           NaN   \n",
       "14               inclusionAI                           NaN   \n",
       "15                       NaN                           NaN   \n",
       "16                       NaN                           NaN   \n",
       "17                       NaN                           NaN   \n",
       "18                       NaN                           NaN   \n",
       "19               ibm-granite                           NaN   \n",
       "20               ibm-granite                           NaN   \n",
       "21               ibm-granite                           NaN   \n",
       "22                       NaN                           NaN   \n",
       "23                       NaN                           NaN   \n",
       "24                   zai-org                           NaN   \n",
       "25                       NaN                           NaN   \n",
       "26                ai-forever                           NaN   \n",
       "27                       NaN                           NaN   \n",
       "28                       NaN                           NaN   \n",
       "29                    nvidia                           NaN   \n",
       "30                       NaN                           NaN   \n",
       "31                    nvidia                           NaN   \n",
       "32               deepseek-ai                           NaN   \n",
       "33               opendatalab                           NaN   \n",
       "34                       NaN                           NaN   \n",
       "35                       NaN                           NaN   \n",
       "36                       NaN                           NaN   \n",
       "37                       NaN                           NaN   \n",
       "38                       NaN                           NaN   \n",
       "39                       NaN                           NaN   \n",
       "40                   ai-sage                           NaN   \n",
       "41                       NaN                           NaN   \n",
       "42               deepseek-ai                           NaN   \n",
       "43                       NaN                           NaN   \n",
       "44                      Qwen                           NaN   \n",
       "45                       NaN                           NaN   \n",
       "46                 mistralai                           NaN   \n",
       "47                       NaN                           NaN   \n",
       "48               ibm-granite                           NaN   \n",
       "49               Alibaba-NLP                           NaN   \n",
       "\n",
       "   Post-training compute notes Hardware utilization (HFU)  \n",
       "0                          NaN                        NaN  \n",
       "1                          NaN                        NaN  \n",
       "2                          NaN                        NaN  \n",
       "3                          NaN                        NaN  \n",
       "4                          NaN                        NaN  \n",
       "5                          NaN                        NaN  \n",
       "6                          NaN                        NaN  \n",
       "7                          NaN                        NaN  \n",
       "8                          NaN                        NaN  \n",
       "9                          NaN                        NaN  \n",
       "10                         NaN                        NaN  \n",
       "11                         NaN                        NaN  \n",
       "12                         NaN                        NaN  \n",
       "13                         NaN                        NaN  \n",
       "14                         NaN                        NaN  \n",
       "15                         NaN                        NaN  \n",
       "16                         NaN                        NaN  \n",
       "17                         NaN                        NaN  \n",
       "18                         NaN                        NaN  \n",
       "19                         NaN                        NaN  \n",
       "20                         NaN                        NaN  \n",
       "21                         NaN                        NaN  \n",
       "22                         NaN                        NaN  \n",
       "23                         NaN                        NaN  \n",
       "24                         NaN                        NaN  \n",
       "25                         NaN                        NaN  \n",
       "26                         NaN                        NaN  \n",
       "27                         NaN                        NaN  \n",
       "28                         NaN                        NaN  \n",
       "29                         NaN                        NaN  \n",
       "30                         NaN                        NaN  \n",
       "31                         NaN                        NaN  \n",
       "32                         NaN                        NaN  \n",
       "33                         NaN                        NaN  \n",
       "34                         NaN                        NaN  \n",
       "35                         NaN                        NaN  \n",
       "36                         NaN                        NaN  \n",
       "37                         NaN                        NaN  \n",
       "38                         NaN                        NaN  \n",
       "39                         NaN                        NaN  \n",
       "40                         NaN                        NaN  \n",
       "41                         NaN                        NaN  \n",
       "42                         NaN                        NaN  \n",
       "43                         NaN                        NaN  \n",
       "44                         NaN                        NaN  \n",
       "45                         NaN                        NaN  \n",
       "46                         NaN                        NaN  \n",
       "47                         NaN                        NaN  \n",
       "48                         NaN                        NaN  \n",
       "49                         NaN                        NaN  \n",
       "\n",
       "[50 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_url = \"https://epoch.ai/data/all_ai_models.csv\"\n",
    "models_df = pd.read_csv(data_url, engine='python')\n",
    "models_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
