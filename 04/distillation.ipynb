{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196a02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (120000, 28, 28, 1)\n",
      "120000 train samples\n",
      "20000 test samples\n",
      "number of classes: 20\n",
      "After augmentation:\n",
      "x_train shape: (480000, 28, 28, 1)\n",
      "480000 train samples\n",
      "80000 test samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "0 0-9 digits\n",
    "\"\"\"\n",
    "#(x_handwritten_mnist_train, y_handwritten_mnist_train), (x_handwritten_mnist_test, y_handwritten_mnist_test) = keras.datasets.mnist.load_data()\n",
    "handwritten_mnist = keras.datasets.mnist.load_data()\n",
    "\"\"\"\n",
    "0\tT-shirt/top\n",
    "1\tTrouser\n",
    "2\tPullover\n",
    "3\tDress\n",
    "4\tCoat\n",
    "5\tSandal\n",
    "6\tShirt\n",
    "7\tSneaker\n",
    "8\tBag\n",
    "9\tAnkle boot\n",
    "\"\"\"\n",
    "# (x_fashion_mnist_train, y_fashion_mnist_train), (x_fashion_mnist_test, y_fashion_mnist_test) = keras.datasets.fashion_mnist.load_data()\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# combine the two datasets, changing the labels of the fashion MNIST +10 (num_handwritten_classes) to avoid overlap\n",
    "num_handwritten_classes = len(np.unique(handwritten_mnist[0][1]))\n",
    "x_mnist_train = np.concatenate((handwritten_mnist[0][0], fashion_mnist[0][0]), axis=0) # image: shape (60000, 28, 28)\n",
    "y_mnist_train = np.concatenate((handwritten_mnist[0][1], fashion_mnist[0][1] + num_handwritten_classes), axis=0) # label: shape (60000,)\n",
    "x_mnist_test = np.concatenate((handwritten_mnist[1][0], fashion_mnist[1][0]), axis=0) # image: shape (10000, 28, 28)\n",
    "y_mnist_test = np.concatenate((handwritten_mnist[1][1], fashion_mnist[1][1] + num_handwritten_classes), axis=0) # label: shape (10000,)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_mnist_train = x_mnist_train.astype(\"float32\") / 255\n",
    "x_mnist_test = x_mnist_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_mnist_train = np.expand_dims(x_mnist_train, -1)\n",
    "x_mnist_test = np.expand_dims(x_mnist_test, -1)\n",
    "print(\"x_train shape:\", x_mnist_train.shape)\n",
    "print(x_mnist_train.shape[0], \"train samples\")\n",
    "print(x_mnist_test.shape[0], \"test samples\")\n",
    "\n",
    "# find number of classes\n",
    "num_classes = len(np.unique(np.concatenate((y_mnist_train, y_mnist_test))))\n",
    "print(\"number of classes:\", num_classes)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_mnist_train = keras.utils.to_categorical(y_mnist_train, num_classes) \n",
    "y_mnist_test = keras.utils.to_categorical(y_mnist_test, num_classes)\n",
    "\n",
    "# increase dataset size (not necessarily complexity) by rotating images by 90,180,270 degrees\n",
    "def augment_data(x_data, y_data):\n",
    "    x_augmented = []\n",
    "    y_augmented = []\n",
    "    for i in range(x_data.shape[0]):\n",
    "        x_augmented.append(x_data[i])\n",
    "        y_augmented.append(y_data[i])\n",
    "        for angle in [90, 180, 270]:\n",
    "            x_rotated = np.rot90(x_data[i], k=angle//90)\n",
    "            x_augmented.append(x_rotated)\n",
    "            y_augmented.append(y_data[i])\n",
    "    return np.array(x_augmented), np.array(y_augmented)\n",
    "x_mnist_train, y_mnist_train = augment_data(x_mnist_train, y_mnist_train)\n",
    "x_mnist_test, y_mnist_test = augment_data(x_mnist_test, y_mnist_test)\n",
    "print(\"After augmentation:\")\n",
    "print(\"x_train shape:\", x_mnist_train.shape)\n",
    "print(x_mnist_train.shape[0], \"train samples\")\n",
    "print(x_mnist_test.shape[0], \"test samples\")\n",
    "\n",
    "# shuffle the combined dataset\n",
    "rng = np.random.default_rng(seed=42)\n",
    "train_indices = rng.permutation(x_mnist_train.shape[0])\n",
    "x_mnist_train = x_mnist_train[train_indices]\n",
    "y_mnist_train = y_mnist_train[train_indices]\n",
    "# same for test set\n",
    "test_indices = rng.permutation(x_mnist_test.shape[0])\n",
    "x_mnist_test = x_mnist_test[test_indices]\n",
    "y_mnist_test = y_mnist_test[test_indices]\n",
    "\n",
    "# show some data\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_mnist_train[i].reshape(28,28)\n",
    "               #, cmap=plt.cm.binary\n",
    "               )\n",
    "    plt.xlabel(np.argmax(y_mnist_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b2cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "\n",
    "teacher = keras.models.load_model('mnist-cnn-model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2591568b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mnist-cnn-model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mnist-cnn-model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
       "\n",
       " max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> \n",
       "\n",
       " max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,020</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m320\u001b[0m \n",
       "\n",
       " max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m18,496\u001b[0m \n",
       "\n",
       " max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                     \u001b[38;5;34m32,020\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152,510</span> (595.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m152,510\u001b[0m (595.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,836</span> (198.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,836\u001b[0m (198.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,674</span> (397.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m101,674\u001b[0m (397.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mnist-cnn-student\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mnist-cnn-student\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> \n",
       "\n",
       " max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> \n",
       "\n",
       " max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,020</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m16\u001b[0m)                \u001b[38;5;34m160\u001b[0m \n",
       "\n",
       " max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)              \u001b[38;5;34m2,320\u001b[0m \n",
       "\n",
       " max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                      \u001b[38;5;34m8,020\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,500</span> (41.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,500\u001b[0m (41.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,500</span> (41.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,500\u001b[0m (41.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import layers\n",
    "# note: student model is smaller than teacher model (fewer parameters and lower computational cost)\n",
    "num_classes = 20\n",
    "input_shape = (28, 28, 1)\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"), #16 vs 64\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"), #16 vs 32\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5), \n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ],name=\"mnist-cnn-student\"\n",
    ")\n",
    "\n",
    "# clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)\n",
    "\n",
    "teacher.summary()\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66dc8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import ops\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1, # 0.1 means 10% learning from ground-truth, 90% from teacher\n",
    "        # low temperature (1) makes the softmax sharper [0.01, 0.01, 0.97, ...], high temperature (10) makes it softer (more spread out) [0.1, 0.2, 0.3, ...]\n",
    "        temperature=10, # high temperature reveals more of the teacher's knowledge\n",
    "    ):\n",
    "        \"\"\"Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions. \n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "    # loss: numerical measure of how wrong the predictions are, it's a 'penality score' where lower is better predictions\n",
    "    # during training, the model adjust its weights to minimize this loss, 'learning' to make better predictions\n",
    "    def compute_loss(\n",
    "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
    "    ):\n",
    "        teacher_pred = self.teacher(x, training=False) # here the teacher doesn't just say the right answer, it provides its full prediction distribution\n",
    "        student_loss = self.student_loss_fn(y, y_pred) #cross-entropy loss between ground-truth and student predictions\n",
    "        # Kullback-Leibler divergence loss between teacher and student predictions (complete distribution among classes): this is WHERE the knowledge distillation happens\n",
    "        # the student doesn't just learn to predict the right class (doesn't copy the teacher pred!), it learns to mimic the teacher's full prediction distribution\n",
    "        # if the student simply copied the teacher's predictions:\n",
    "        #   1. it would not learn to generalize from the ground-truth labels (unseen images)\n",
    "        #   2. not learn the underlying patterns (needs to develop its own 'understanding')\n",
    "        #   3. the student has fewer parameters, so it cannot exactly replicate the teacher's behavior but approximates it adjustingly its own weights\n",
    "        #instead the student learns the function, not the outputs\n",
    "        distillation_loss = self.distillation_loss_fn(\n",
    "            ops.softmax(teacher_pred / self.temperature, axis=1),\n",
    "            ops.softmax(y_pred / self.temperature, axis=1),\n",
    "        ) * (self.temperature**2) # scales the loss to keep it balanced with the student loss\n",
    "        # finally combine the two losses: the student improves both on ground-truth and on teacher predictions\n",
    "        # student loss only: the image is labeled as \"7\", so the student should predict class 7 with high confidence\n",
    "        # with distillation loss: the image is labeled as \"7\" with 90% confidence, but also class 2 and 9 with lower confidence, so the student should mimic this behavior\n",
    "        # instead of forcing the student to be 100% certain, it learns the teacher's \"wisdom\" about uncertainty\n",
    "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss # 10% from ground-truth, 90% from distillation teacher\n",
    "        return loss\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.student(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a72b0033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 9ms/step - accuracy: 0.8072 - loss: 0.0593\n",
      "Epoch 2/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 9ms/step - accuracy: 0.8661 - loss: 0.0408\n",
      "Epoch 3/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 9ms/step - accuracy: 0.8776 - loss: 0.0373\n"
     ]
    }
   ],
   "source": [
    "# distill teacher to student\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(), #cross-entropy loss between ground-truth and student predictions\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(), # KL divergence between teacher and student predictions\n",
    "    alpha=0.1, # 0.1 means 10% learning from ground-truth, 90% from teacher\n",
    "    temperature=10, #  # high temperature reveals more of the teacher's knowledge: low temperature (1) makes the softmax sharper [0.01, 0.01, 0.97, ...], high temperature (10) makes it softer (more spread out) [0.1, 0.2, 0.3, ...]\n",
    ")\n",
    "# train student through distillation\n",
    "distiller.fit(x_mnist_train, y_mnist_train, epochs=3, verbose=1)\n",
    "distiller.save('mnist-cnn-distilled.keras')\n",
    "distiller.student.save('mnist-cnn-distilled-student.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f6a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 5ms/step - accuracy: 0.7036 - loss: 0.8733\n",
      "Epoch 2/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 5ms/step - accuracy: 0.7628 - loss: 0.6917\n",
      "Epoch 3/3\n",
      "\u001b[1m15000/15000\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 5ms/step - accuracy: 0.7741 - loss: 0.6581\n"
     ]
    }
   ],
   "source": [
    "# train student from scratch for comparison, as usually done\n",
    "student_scratch.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "student_scratch.fit(x_mnist_train, y_mnist_train, epochs=3, verbose=1)\n",
    "student_scratch.save('mnist-cnn-student.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793595b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 - 8s - 3ms/step - accuracy: 0.9137 - loss: 0.2519\n",
      "2500/2500 - 10s - 4ms/step - accuracy: 0.8741 - loss: 0.0388\n",
      "2500/2500 - 7s - 3ms/step - accuracy: 0.8492 - loss: 0.4476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4476426839828491, 0.8492000102996826]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = keras.models.load_model('mnist-cnn-model.keras')      \n",
    "distilled_student = keras.models.load_model('mnist-cnn-distilled-student.keras',compile=False)\n",
    "# recompile distilled model\n",
    "distilled = Distiller(student=distilled_student, teacher=teacher)\n",
    "distilled.compile(\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "student_scratch = keras.models.load_model('mnist-cnn-student.keras')\n",
    "# Evaluate student on test dataset\n",
    "teacher.evaluate(x_mnist_test, y_mnist_test, verbose=2)\n",
    "distilled.evaluate(x_mnist_test, y_mnist_test, verbose=2)\n",
    "student_scratch.evaluate(x_mnist_test, y_mnist_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d8cc095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher - Loss: 0.2519, Accuracy: 0.9137\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled - Loss: 0.0388, Accuracy: 0.8741\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Scratch - Loss: 0.4476, Accuracy: 0.8492\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x3000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = range(10) #np.random.choice(x_mnist_test.shape[0], 10, replace=False)\n",
    "\n",
    "def evaluate_model(q_model: keras.Model, model_name=\"Model\"):\n",
    "    score = q_model.evaluate(x_mnist_test, y_mnist_test, verbose=0)\n",
    "    print(f\"{model_name} - Loss: {score[0]:.4f}, Accuracy: {score[1]:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(20,30))    \n",
    "    x_mnist_rnd, y_mnist_rnd = x_mnist_test[idx], y_mnist_test[idx]\n",
    "    for i in range(10):    \n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(\n",
    "            x_mnist_rnd[i].reshape(28,28), \n",
    "            #cmap=plt.cm.binary\n",
    "            )\n",
    "        _preds = q_model.predict(x_mnist_rnd[i:i+1], verbose=0)[0]\n",
    "        #print(f\"Image {i} predictions: {_preds.round(3)}\")\n",
    "        plt.xlabel(f\"{np.argmax(_preds)==np.argmax(y_mnist_rnd[i])}\\n{np.max(_preds).round(3).astype(str)}\")\n",
    "    plt.show()\n",
    "\n",
    "for model, name in zip(\n",
    "    [teacher, distilled, student_scratch],\n",
    "    [\"Teacher\", \"Distilled\", \"Student Scratch\"]\n",
    "):\n",
    "    evaluate_model(model, model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.dipaolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Setup the Student (local model) \n",
    "model_base = \"Qwen/Qwen3-0.6B\"\n",
    "model_id = os.path.join(os.environ['HF_HOME'], 'hub', f'models--{model_base.replace(\"/\", \"--\")}-q8')\n",
    "\n",
    "# Configure 4-bit quantization to save memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Enable 4-bit quantization (reduces memory)\n",
    "    bnb_4bit_quant_type=\"nf4\", # NormalFloat4: optimized 4-bit format, preserving model quality better than naive 4bit \"fp4\"\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computations (faster on modern GPUs)\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config, # Apply 4-bit quantization (model weights stored in 4-bit format => less memory)\n",
    "    device_map=\"auto\", # Automatically use the GPU if available\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bcdc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This system prompt is the \"new ability\" we want to teach.\n",
    "# It's very strict.\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert text classifier. You MUST respond with ONLY a single, minified JSON object in the format: {\"classification\": \"...\", \"tone\": \"...\", \"alert\": \"...\" }.\n",
    "\n",
    "Valid classifications are: BUG, FEATURE_REQUEST, INQUIRY\n",
    "Valid tones are: POSITIVE, NEUTRAL, ANGRY\n",
    "Valid alerts are: , , \n",
    "\n",
    "Alert must match the tone. Do not add any other text, explanations, or markdown formatting.\"\"\"\n",
    "\n",
    "def create_prompt_messages(ticket_text):\n",
    "    \"\"\"Helper function to format the chat prompt\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the following support ticket:\\n\\n{ticket_text}\"}\n",
    "    ]\n",
    "\n",
    "# Helper function to run inference\n",
    "def generate_response(model_to_test, test_messages):\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        test_messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_to_test.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    outputs = model_to_test.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output, skipping special tokens and the prompt itself\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eab6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Testing UNTRAINED Student Model ---\n",
      "Ticket: Your app crashed again! This is the third time. I can't even open my files! This is unacceptable.\n",
      "\n",
      "Response from base model:\n",
      "{\"classification\": \"BUG\", \"tone\": \"ANGER\", \"alert\": \"\"}\n",
      " Valid JSON\n",
      " invalid tone\n",
      " FAILED. JSON fields have invalid values.\n"
     ]
    }
   ],
   "source": [
    "print(\"---  Testing UNTRAINED Student Model ---\")\n",
    "\n",
    "test_ticket = \"Your app crashed again! This is the third time. I can't even open my files! This is unacceptable.\"\n",
    "test_messages = create_prompt_messages(test_ticket)\n",
    "\n",
    "# Test the base model\n",
    "base_model_response = generate_response(model, test_messages)\n",
    "\n",
    "print(f\"Ticket: {test_ticket}\\n\")\n",
    "print(f\"Response from base model:\\n{base_model_response}\")\n",
    "\n",
    "# Check if it's valid JSON\n",
    "try:\n",
    "    rs = json.loads(base_model_response)\n",
    "    print(\" Valid JSON\")\n",
    "    try:\n",
    "        assert rs[\"classification\"] in [\"BUG\", \"FEATURE_REQUEST\", \"INQUIRY\"], print(\" invalid classification\")\n",
    "        assert rs[\"tone\"] in [\"POSITIVE\", \"NEUTRAL\", \"ANGRY\"], print(\" invalid tone\")\n",
    "        assert rs[\"alert\"] in [\"\", \"\", \"\"], print(\" invalid alert\")\n",
    "        assert rs[\"alert\"] == (\"\" if rs[\"tone\"] == \"POSITIVE\" else \"\" if rs[\"tone\"] == \"NEUTRAL\" else \"\"), print(\" alert does not match tone\")\n",
    "        print(\" SUCCESS. well done!\")\n",
    "    except AssertionError:\n",
    "        print(\" FAILED. JSON fields have invalid values.\")    \n",
    "except json.JSONDecodeError:\n",
    "    print(\" FAILED. Not valid JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Calling Teacher API to generate dataset ---\n",
      "Ticket: I love the new update! The int... -> Response: {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: My password reset link isn't w... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: This app is unusable! It just ... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: It would be really cool if you... -> Response: {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: I'm not sure how to find the s... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: The new export feature is amaz... -> Response: {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: Why does it keep logging me ou... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: Requesting an integration with... -> Response: {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: Just writing to say thanks for... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: The billing page is confusing.... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: It's broken. The main dashboar... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: I'd like to see a 'project tem... -> Response: {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: Your support team is the best.... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"POSITIVE\",\"alert\":\"\"}\n",
      "Ticket: The app is slow today. Is ther... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: THIS IS AWFUL. I'M SO MAD. FIX... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: Can you help me with my accoun... -> Response: {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: The app keeps crashing on star... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: I'm getting a 500 error when I... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: The new analytics dashboard is... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: Can you assist me with a bug i... -> Response: {\"classification\":\"BUG\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: I'm having trouble with the AP... -> Response: {\"classification\":\"BUG\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}\n",
      "Ticket: THIS IS UNACCEPTABLE! I DEMAND... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "Ticket: I CAN'T TAKE THIS ANYMORE! FIX... -> Response: {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}\n",
      "\n",
      "---  Dataset creation complete. Total examples: 23 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d7373ff1a04b2e804b6d6f0d3676ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16344"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# A small dataset of raw prompts\n",
    "raw_tickets = [\n",
    "    \"I love the new update! The interface is so clean.\",\n",
    "    \"My password reset link isn't working. Can you help?\",\n",
    "    \"This app is unusable! It just deleted all my data!\",\n",
    "    \"It would be really cool if you could add a dark mode.\",\n",
    "    \"I'm not sure how to find the settings menu.\",\n",
    "    \"The new export feature is amazing, thank you!\",\n",
    "    \"Why does it keep logging me out? It's so annoying.\",\n",
    "    \"Requesting an integration with Google Calendar.\",\n",
    "    \"Just writing to say thanks for this great tool!\",\n",
    "    \"The billing page is confusing. Where's my last invoice?\",\n",
    "    \"It's broken. The main dashboard won't load at all.\",\n",
    "    \"I'd like to see a 'project templates' feature.\",\n",
    "    \"Your support team is the best. Quick and helpful.\",\n",
    "    \"The app is slow today. Is there an outage?\",\n",
    "    \"THIS IS AWFUL. I'M SO MAD. FIX IT.\",\n",
    "    \"Can you help me with my account issues?\",\n",
    "    \"The app keeps crashing on startup.\",\n",
    "    \"I'm getting a 500 error when I try to upload files.\",\n",
    "    \"The new analytics dashboard is not loading.\",\n",
    "    \"Can you assist me with a bug in the app?\",\n",
    "    \"I'm having trouble with the API integration.\",\n",
    "    \"THIS IS UNACCEPTABLE! I DEMAND A FIX NOW!\",\n",
    "    \"I CAN'T TAKE THIS ANYMORE! FIX IT NOW!\",\n",
    "]\n",
    "\n",
    "def get_teacher_response(ticket_text):\n",
    "    \"\"\"Calls the OpenAI API to get a perfect response.\"\"\"\n",
    "    messages = create_prompt_messages(ticket_text)\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", # Teacher model\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Create the dataset ---\n",
    "distilled_data = []\n",
    "print(\"---  Calling Teacher API to generate dataset ---\")\n",
    "\n",
    "for ticket in raw_tickets:\n",
    "    # 1. Get the \"perfect\" JSON response from the teacher\n",
    "    teacher_response = get_teacher_response(ticket)\n",
    "    \n",
    "    if teacher_response:\n",
    "        print(f\"Ticket: {ticket[:30]}... -> Response: {teacher_response}\")\n",
    "        \n",
    "        # 2. Format it as a \"messages\" list\n",
    "        distilled_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the following support ticket:\\n\\n{ticket}\"},\n",
    "                {\"role\": \"assistant\", \"content\": teacher_response} # This is the \"label\"\n",
    "            ]\n",
    "        })\n",
    "\n",
    "print(f\"\\n---  Dataset creation complete. Total examples: {len(distilled_data)} ---\")\n",
    "\n",
    "# Convert to Hugging Face Dataset object\n",
    "hf_dataset = Dataset.from_list(distilled_data)\n",
    "hf_dataset.save_to_disk(\"./tmp/hf_distilled_dataset\")\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(hf_dataset)   \n",
    "df.to_json(\"./tmp/distilled_dataset.json\")\n",
    "df.to_csv(\"./tmp/distilled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c076854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Model prepared for LoRA fine-tuning ---\n",
      "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepares quantized models for training. Original weights are frozen, only LoRA adapters are ADDED and trainable\n",
    "model = prepare_model_for_kbit_training(model) # Prepare model for LoRA fine-tuning (essential for quantized models)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # Rank/bottleneck dimension (controls capacity vs efficiency tradeoff), higher = more parameters, more capacity (e.g. for complex reasoning tasks), lower = less parameters, more efficient\n",
    "    lora_alpha=32, # Scaling factor for LoRA weights (high = more influence on the model's behavior), typically 2r\n",
    "    # target_modules: which layers/modules to apply LoRA adapters to\n",
    "    target_modules=[\n",
    "        \"q_proj\",      # Query projection in multi-head attention: transforms input to query vectors\n",
    "        \"k_proj\",      # Key projection in multi-head attention: transforms input to key vectors\n",
    "        \"v_proj\",      # Value projection in multi-head attention: transforms input to value vectors\n",
    "        \"o_proj\",      # Output projection after attention: combines attention outputs\n",
    "        \"gate_proj\",   # Gating mechanism in feed-forward network (SwiGLU activation)\n",
    "        \"up_proj\",     # First layer of FFN, expands dimension (up)\n",
    "        \"down_proj\"    # Second layer of FFN, reduces dimension back (down)\n",
    "    ], \n",
    "    lora_dropout=0.05, # regularization to prevent overfitting on small datasets, e.g 5% of LoRA neurons are randomly dropped during training\n",
    "    bias=\"none\", # dont train bias parameters (most parameter-efficient)\n",
    "    task_type=\"CAUSAL_LM\" #  Specifies autoregressive language modeling (for decoder-only models)\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "# results in a model where original weights are frozen, and LoRA adapters are ADDED and trainable\n",
    "# base model weights is in 4-bit quantized format, LoRA adapters are in full precision (float16)\n",
    "model = get_peft_model(model, lora_config) #LoRa on top of quantized weights = QLoRa (Q4 + LoRa)\n",
    "\n",
    "print(\"---  Model prepared for LoRA fine-tuning ---\")\n",
    "# Print trainable parameters to verify only LoRA adapters are trainable (and will be ADDED to the model)\n",
    "model.print_trainable_parameters()\n",
    "# NOTE: this new trainable parameters are FIXED, they do not depend on the dataset size. \n",
    "# LoRA parameter count is determined by MODEL ARCHITECTURE, not dataset size\n",
    "# For each target module in each layer:\n",
    "# Parameters = (input_dim + output_dim)  r\n",
    "# Example with Qwen 3-0.6B:\n",
    "# - 24 layers\n",
    "# - 7 target modules per layer  \n",
    "# - Various dimensions (768, 3072, etc.)\n",
    "# Subsequent training on new tasks will only update these LoRA parameters, preserving the knowledge acquired in previous trainings (and the base model weights remain frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd123bb7",
   "metadata": {},
   "source": [
    "- LoRa (Low-Rank Adaptation) is a technique used to fine-tune large language models (LLMs) efficiently by updating only a small number of parameters, rather than the entire model. This approach significantly reduces the computational resources and time required for training.\n",
    "![LoRa](https://media.licdn.com/dms/image/v2/D5612AQHfM42NWm0L3Q/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1729439416136?e=2147483647&v=beta&t=5qVvcfaGBVCa6Fzg9OMxerBvuXqNnJbE_K86ZAz1q4Q)\n",
    "- QLoRa (Quantized Low-Rank Adaptation) is an extension of LoRa that combines low-rank adaptation with model quantization techniques. QLoRa further reduces the memory footprint and computational requirements by quantizing the model weights, allowing for even more efficient fine-tuning of large language models.\n",
    "![LoRa vs QLoRa](https://cdn.prod.website-files.com/62528d398a42420e66390ef9/65fc1abec64c8a1acc9a21c3_image%20(1).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453aca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Starting training... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.dipaolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\m.dipaolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 07:11, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.624800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Training complete! ---\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "hf_dataset = Dataset.load_from_disk(\"./tmp/hf_distilled_dataset\")\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(os.environ['HF_HOME'], 'tmp', f\"{model_id}-distilled\"),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=hf_dataset.num_rows // 2,\n",
    "    eval_strategy=\"no\",\n",
    "    num_train_epochs=2,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=hf_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"---  Starting training... ---\")\n",
    "# Start training\n",
    "trainer.train()\n",
    "print(\"---  Training complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb0d1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(os.environ['HF_HOME'], 'hub', f\"{model_id}-distilled-final\"))\n",
    "model.save_pretrained(os.path.join(os.environ['HF_HOME'], 'hub', f\"{model_id}-distilled-final\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f41d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " INSPECTING LoRA ADAPTERS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LoRA Configuration:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Adapter: default\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.05\n",
      "  Target modules: {'o_proj', 'k_proj', 'gate_proj', 'q_proj', 'v_proj', 'up_proj', 'down_proj'}\n",
      "  Task type: CAUSAL_LM\n",
      "\n",
      "================================================================================\n",
      " EXTRACTING LoRA PARAMETERS\n",
      "================================================================================\n",
      "\n",
      " Found 392 LoRA parameter tensors\n",
      " Total LoRA parameters: 10,092,544\n",
      "\n",
      "================================================================================\n",
      " LoRA STATISTICS BY MODULE TYPE\n",
      "================================================================================\n",
      "            num_params          norm                             std\n",
      "                   sum count    mean     std     min     max    mean\n",
      "module_type                                                         \n",
      "down_proj      1835008    56  1.2065  1.1242  0.0766  2.3315  0.0056\n",
      "gate_proj      1835008    56  1.2381  1.0883  0.1374  2.3321  0.0094\n",
      "k_proj          917504    56  1.2009  1.1239  0.0720  2.3252  0.0094\n",
      "o_proj         1376256    56  1.2054  1.1248  0.0763  2.3310  0.0068\n",
      "q_proj         1376256    56  1.2173  1.1025  0.1014  2.3241  0.0094\n",
      "up_proj        1835008    56  1.2375  1.0881  0.1325  2.3343  0.0094\n",
      "v_proj          917504    56  1.2019  1.1226  0.0739  2.3319  0.0094\n",
      "\n",
      "====================================================================================================\n",
      " LAYER-BY-LAYER ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      " model.embed_tokens\n",
      "   Type: Embedding\n",
      "   Parameters: 155,582,464\n",
      "   Shape: weight: (151936, 1024)\n",
      "\n",
      " model.layers.0.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.0.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.0.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.0.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.0.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.0.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.0.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.0.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.0.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.0.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.0.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.0.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.0.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.0.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.0.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.0.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.0.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.0.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.0.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.0.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.0.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.0.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.0.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.0.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.0.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.1.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.1.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.1.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.1.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.1.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.1.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.1.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.1.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.1.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.1.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.1.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.1.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.1.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.1.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.1.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.1.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.1.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.1.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.1.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.1.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.1.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.1.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.1.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.1.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.1.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.2.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.2.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.2.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.2.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.2.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.2.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.2.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.2.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.2.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.2.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.2.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.2.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.2.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.2.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.2.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.2.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.2.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.2.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.2.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.2.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.2.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.2.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.2.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.2.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.2.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.3.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.3.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.3.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.3.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.3.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.3.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.3.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.3.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.3.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.3.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.3.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.3.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.3.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.3.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.3.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.3.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.3.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.3.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.3.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.3.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.3.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.3.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.3.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.3.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.3.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.4.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.4.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.4.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.4.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.4.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.4.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.4.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.4.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.4.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.4.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.4.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.4.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.4.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.4.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.4.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.4.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.4.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.4.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.4.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.4.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.4.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.4.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.4.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.4.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.4.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.5.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.5.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.5.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.5.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.5.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.5.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.5.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.5.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.5.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.5.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.5.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.5.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.5.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.5.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.5.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.5.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.5.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.5.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.5.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.5.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.5.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.5.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.5.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.5.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.5.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.6.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.6.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.6.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.6.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.6.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.6.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.6.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.6.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.6.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.6.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.6.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.6.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.6.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.6.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.6.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.6.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.6.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.6.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.6.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.6.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.6.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.6.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.6.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.6.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.6.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.7.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.7.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.7.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.7.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.7.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.7.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.7.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.7.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.7.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.7.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.7.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.7.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.7.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.7.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.7.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.7.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.7.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.7.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.7.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.7.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.7.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.7.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.7.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.7.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.7.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.8.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.8.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.8.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.8.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.8.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.8.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.8.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.8.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.8.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.8.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.8.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.8.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.8.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.8.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.8.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.8.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.8.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.8.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.8.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.8.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.8.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.8.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.8.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.8.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.8.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.9.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.9.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.9.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.9.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.9.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.9.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.9.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.9.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.9.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.9.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.9.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.9.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.9.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.9.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.9.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.9.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.9.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.9.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.9.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.9.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.9.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.9.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.9.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.9.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.9.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.10.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.10.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.10.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.10.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.10.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.10.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.10.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.10.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.10.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.10.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.10.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.10.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.10.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.10.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.10.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.10.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.10.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.10.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.10.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.10.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.10.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.10.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.10.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.10.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.10.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.11.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.11.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.11.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.11.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.11.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.11.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.11.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.11.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.11.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.11.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.11.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.11.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.11.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.11.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.11.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.11.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.11.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.11.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.11.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.11.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.11.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.11.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.11.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.11.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.11.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.12.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.12.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.12.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.12.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.12.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.12.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.12.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.12.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.12.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.12.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.12.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.12.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.12.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.12.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.12.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.12.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.12.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.12.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.12.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.12.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.12.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.12.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.12.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.12.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.12.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.13.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.13.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.13.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.13.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.13.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.13.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.13.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.13.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.13.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.13.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.13.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.13.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.13.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.13.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.13.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.13.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.13.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.13.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.13.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.13.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.13.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.13.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.13.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.13.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.13.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.14.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.14.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.14.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.14.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.14.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.14.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.14.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.14.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.14.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.14.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.14.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.14.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.14.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.14.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.14.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.14.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.14.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.14.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.14.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.14.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.14.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.14.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.14.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.14.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.14.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.15.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.15.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.15.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.15.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.15.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.15.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.15.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.15.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.15.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.15.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.15.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.15.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.15.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.15.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.15.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.15.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.15.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.15.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.15.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.15.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.15.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.15.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.15.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.15.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.15.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.16.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.16.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.16.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.16.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.16.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.16.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.16.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.16.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.16.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.16.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.16.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.16.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.16.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.16.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.16.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.16.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.16.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.16.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.16.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.16.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.16.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.16.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.16.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.16.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.16.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.17.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.17.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.17.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.17.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.17.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.17.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.17.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.17.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.17.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.17.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.17.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.17.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.17.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.17.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.17.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.17.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.17.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.17.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.17.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.17.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.17.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.17.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.17.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.17.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.17.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.18.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.18.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.18.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.18.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.18.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.18.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.18.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.18.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.18.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.18.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.18.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.18.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.18.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.18.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.18.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.18.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.18.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.18.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.18.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.18.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.18.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.18.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.18.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.18.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.18.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.19.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.19.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.19.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.19.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.19.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.19.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.19.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.19.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.19.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.19.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.19.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.19.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.19.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.19.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.19.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.19.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.19.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.19.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.19.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.19.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.19.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.19.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.19.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.19.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.19.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.20.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.20.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.20.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.20.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.20.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.20.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.20.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.20.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.20.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.20.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.20.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.20.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.20.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.20.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.20.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.20.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.20.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.20.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.20.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.20.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.20.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.20.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.20.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.20.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.20.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.21.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.21.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.21.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.21.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.21.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.21.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.21.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.21.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.21.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.21.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.21.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.21.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.21.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.21.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.21.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.21.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.21.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.21.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.21.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.21.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.21.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.21.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.21.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.21.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.21.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.22.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.22.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.22.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.22.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.22.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.22.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.22.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.22.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.22.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.22.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.22.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.22.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.22.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.22.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.22.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.22.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.22.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.22.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.22.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.22.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.22.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.22.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.22.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.22.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.22.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.23.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.23.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.23.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.23.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.23.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.23.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.23.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.23.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.23.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.23.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.23.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.23.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.23.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.23.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.23.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.23.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.23.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.23.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.23.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.23.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.23.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.23.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.23.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.23.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.23.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.24.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.24.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.24.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.24.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.24.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.24.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.24.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.24.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.24.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.24.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.24.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.24.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.24.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.24.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.24.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.24.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.24.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.24.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.24.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.24.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.24.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.24.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.24.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.24.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.24.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.25.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.25.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.25.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.25.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.25.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.25.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.25.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.25.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.25.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.25.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.25.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.25.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.25.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.25.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.25.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.25.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.25.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.25.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.25.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.25.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.25.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.25.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.25.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.25.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.25.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.26.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.26.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.26.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.26.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.26.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.26.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.26.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.26.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.26.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.26.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.26.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.26.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.26.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.26.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.26.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.26.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.26.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.26.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.26.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.26.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.26.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.26.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.26.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.26.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.26.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.27.self_attn.q_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (2048, 1024)\n",
      "\n",
      " model.layers.27.self_attn.q_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.27.self_attn.q_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (2048, 16)\n",
      "\n",
      " model.layers.27.self_attn.k_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.27.self_attn.k_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.27.self_attn.k_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.27.self_attn.v_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 1,048,576\n",
      "   Shape: weight: (1024, 1024)\n",
      "\n",
      " model.layers.27.self_attn.v_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.27.self_attn.v_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.27.self_attn.o_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 2,097,152\n",
      "   Shape: weight: (1024, 2048)\n",
      "\n",
      " model.layers.27.self_attn.o_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 32,768\n",
      "   Shape: weight: (16, 2048)\n",
      "\n",
      " model.layers.27.self_attn.o_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.27.self_attn.q_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.27.self_attn.k_norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 128\n",
      "   Shape: weight: (128,)\n",
      "\n",
      " model.layers.27.mlp.gate_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.27.mlp.gate_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.27.mlp.gate_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.27.mlp.up_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (3072, 1024)\n",
      "\n",
      " model.layers.27.mlp.up_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (16, 1024)\n",
      "\n",
      " model.layers.27.mlp.up_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (3072, 16)\n",
      "\n",
      " model.layers.27.mlp.down_proj.base_layer\n",
      "   Type: Linear8bitLt\n",
      "   Parameters: 3,145,728\n",
      "   Shape: weight: (1024, 3072)\n",
      "\n",
      " model.layers.27.mlp.down_proj.lora_A.default\n",
      "   Type: Linear\n",
      "   Parameters: 49,152\n",
      "   Shape: weight: (16, 3072)\n",
      "\n",
      " model.layers.27.mlp.down_proj.lora_B.default\n",
      "   Type: Linear\n",
      "   Parameters: 16,384\n",
      "   Shape: weight: (1024, 16)\n",
      "\n",
      " model.layers.27.input_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.layers.27.post_attention_layernorm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " model.norm\n",
      "   Type: Qwen3RMSNorm\n",
      "   Parameters: 1,024\n",
      "   Shape: weight: (1024,)\n",
      "\n",
      " lm_head\n",
      "   Type: Linear\n",
      "   Parameters: 155,582,464\n",
      "   Shape: weight: (151936, 1024)\n",
      "\n",
      " Summary:\n",
      "   Total layers with parameters: 703\n",
      "   Total parameters: 761,724,928\n",
      "\n",
      "====================================================================================================\n",
      " MODEL SUMMARY TABLE\n",
      "====================================================================================================\n",
      "\n",
      "Total transformer layers: 28\n",
      "LoRA target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "\n",
      " Module Type Summary:\n",
      "                           param_count           input_dim output_dim\n",
      "                                 count       sum     first      first\n",
      "module_name is_lora_target                                           \n",
      "down_proj   True                    28  88080384      3072       1024\n",
      "gate_proj   True                    28  88080384      1024       3072\n",
      "k_proj      True                    28  29360128      1024       1024\n",
      "o_proj      True                    28  58720256      2048       1024\n",
      "q_proj      True                    28  58720256      1024       2048\n",
      "up_proj     True                    28  88080384      1024       3072\n",
      "v_proj      True                    28  29360128      1024       1024\n",
      "\n",
      " LoRA Target Analysis:\n",
      "   Total LoRA target modules: 196\n",
      "   Estimated LoRA parameters (r=16): 10,092,544\n",
      "     layer_idx   layer_type module_name                         full_path  weight_shape  param_count  is_lora_target  input_dim  output_dim\n",
      "0            0  transformer      q_proj   model.layers.0.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "1            0  transformer      k_proj   model.layers.0.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "2            0  transformer      v_proj   model.layers.0.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "3            0  transformer      o_proj   model.layers.0.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "4            0  transformer   gate_proj      model.layers.0.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "5            0  transformer     up_proj        model.layers.0.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "6            0  transformer   down_proj      model.layers.0.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "7            1  transformer      q_proj   model.layers.1.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "8            1  transformer      k_proj   model.layers.1.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "9            1  transformer      v_proj   model.layers.1.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "10           1  transformer      o_proj   model.layers.1.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "11           1  transformer   gate_proj      model.layers.1.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "12           1  transformer     up_proj        model.layers.1.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "13           1  transformer   down_proj      model.layers.1.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "14           2  transformer      q_proj   model.layers.2.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "15           2  transformer      k_proj   model.layers.2.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "16           2  transformer      v_proj   model.layers.2.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "17           2  transformer      o_proj   model.layers.2.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "18           2  transformer   gate_proj      model.layers.2.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "19           2  transformer     up_proj        model.layers.2.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "20           2  transformer   down_proj      model.layers.2.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "21           3  transformer      q_proj   model.layers.3.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "22           3  transformer      k_proj   model.layers.3.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "23           3  transformer      v_proj   model.layers.3.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "24           3  transformer      o_proj   model.layers.3.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "25           3  transformer   gate_proj      model.layers.3.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "26           3  transformer     up_proj        model.layers.3.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "27           3  transformer   down_proj      model.layers.3.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "28           4  transformer      q_proj   model.layers.4.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "29           4  transformer      k_proj   model.layers.4.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "30           4  transformer      v_proj   model.layers.4.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "31           4  transformer      o_proj   model.layers.4.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "32           4  transformer   gate_proj      model.layers.4.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "33           4  transformer     up_proj        model.layers.4.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "34           4  transformer   down_proj      model.layers.4.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "35           5  transformer      q_proj   model.layers.5.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "36           5  transformer      k_proj   model.layers.5.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "37           5  transformer      v_proj   model.layers.5.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "38           5  transformer      o_proj   model.layers.5.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "39           5  transformer   gate_proj      model.layers.5.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "40           5  transformer     up_proj        model.layers.5.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "41           5  transformer   down_proj      model.layers.5.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "42           6  transformer      q_proj   model.layers.6.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "43           6  transformer      k_proj   model.layers.6.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "44           6  transformer      v_proj   model.layers.6.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "45           6  transformer      o_proj   model.layers.6.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "46           6  transformer   gate_proj      model.layers.6.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "47           6  transformer     up_proj        model.layers.6.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "48           6  transformer   down_proj      model.layers.6.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "49           7  transformer      q_proj   model.layers.7.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "50           7  transformer      k_proj   model.layers.7.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "51           7  transformer      v_proj   model.layers.7.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "52           7  transformer      o_proj   model.layers.7.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "53           7  transformer   gate_proj      model.layers.7.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "54           7  transformer     up_proj        model.layers.7.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "55           7  transformer   down_proj      model.layers.7.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "56           8  transformer      q_proj   model.layers.8.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "57           8  transformer      k_proj   model.layers.8.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "58           8  transformer      v_proj   model.layers.8.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "59           8  transformer      o_proj   model.layers.8.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "60           8  transformer   gate_proj      model.layers.8.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "61           8  transformer     up_proj        model.layers.8.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "62           8  transformer   down_proj      model.layers.8.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "63           9  transformer      q_proj   model.layers.9.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "64           9  transformer      k_proj   model.layers.9.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "65           9  transformer      v_proj   model.layers.9.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "66           9  transformer      o_proj   model.layers.9.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "67           9  transformer   gate_proj      model.layers.9.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "68           9  transformer     up_proj        model.layers.9.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "69           9  transformer   down_proj      model.layers.9.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "70          10  transformer      q_proj  model.layers.10.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "71          10  transformer      k_proj  model.layers.10.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "72          10  transformer      v_proj  model.layers.10.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "73          10  transformer      o_proj  model.layers.10.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "74          10  transformer   gate_proj     model.layers.10.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "75          10  transformer     up_proj       model.layers.10.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "76          10  transformer   down_proj     model.layers.10.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "77          11  transformer      q_proj  model.layers.11.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "78          11  transformer      k_proj  model.layers.11.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "79          11  transformer      v_proj  model.layers.11.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "80          11  transformer      o_proj  model.layers.11.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "81          11  transformer   gate_proj     model.layers.11.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "82          11  transformer     up_proj       model.layers.11.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "83          11  transformer   down_proj     model.layers.11.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "84          12  transformer      q_proj  model.layers.12.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "85          12  transformer      k_proj  model.layers.12.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "86          12  transformer      v_proj  model.layers.12.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "87          12  transformer      o_proj  model.layers.12.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "88          12  transformer   gate_proj     model.layers.12.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "89          12  transformer     up_proj       model.layers.12.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "90          12  transformer   down_proj     model.layers.12.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "91          13  transformer      q_proj  model.layers.13.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "92          13  transformer      k_proj  model.layers.13.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "93          13  transformer      v_proj  model.layers.13.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "94          13  transformer      o_proj  model.layers.13.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "95          13  transformer   gate_proj     model.layers.13.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "96          13  transformer     up_proj       model.layers.13.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "97          13  transformer   down_proj     model.layers.13.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "98          14  transformer      q_proj  model.layers.14.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "99          14  transformer      k_proj  model.layers.14.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "100         14  transformer      v_proj  model.layers.14.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "101         14  transformer      o_proj  model.layers.14.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "102         14  transformer   gate_proj     model.layers.14.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "103         14  transformer     up_proj       model.layers.14.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "104         14  transformer   down_proj     model.layers.14.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "105         15  transformer      q_proj  model.layers.15.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "106         15  transformer      k_proj  model.layers.15.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "107         15  transformer      v_proj  model.layers.15.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "108         15  transformer      o_proj  model.layers.15.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "109         15  transformer   gate_proj     model.layers.15.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "110         15  transformer     up_proj       model.layers.15.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "111         15  transformer   down_proj     model.layers.15.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "112         16  transformer      q_proj  model.layers.16.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "113         16  transformer      k_proj  model.layers.16.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "114         16  transformer      v_proj  model.layers.16.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "115         16  transformer      o_proj  model.layers.16.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "116         16  transformer   gate_proj     model.layers.16.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "117         16  transformer     up_proj       model.layers.16.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "118         16  transformer   down_proj     model.layers.16.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "119         17  transformer      q_proj  model.layers.17.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "120         17  transformer      k_proj  model.layers.17.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "121         17  transformer      v_proj  model.layers.17.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "122         17  transformer      o_proj  model.layers.17.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "123         17  transformer   gate_proj     model.layers.17.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "124         17  transformer     up_proj       model.layers.17.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "125         17  transformer   down_proj     model.layers.17.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "126         18  transformer      q_proj  model.layers.18.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "127         18  transformer      k_proj  model.layers.18.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "128         18  transformer      v_proj  model.layers.18.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "129         18  transformer      o_proj  model.layers.18.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "130         18  transformer   gate_proj     model.layers.18.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "131         18  transformer     up_proj       model.layers.18.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "132         18  transformer   down_proj     model.layers.18.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "133         19  transformer      q_proj  model.layers.19.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "134         19  transformer      k_proj  model.layers.19.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "135         19  transformer      v_proj  model.layers.19.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "136         19  transformer      o_proj  model.layers.19.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "137         19  transformer   gate_proj     model.layers.19.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "138         19  transformer     up_proj       model.layers.19.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "139         19  transformer   down_proj     model.layers.19.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "140         20  transformer      q_proj  model.layers.20.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "141         20  transformer      k_proj  model.layers.20.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "142         20  transformer      v_proj  model.layers.20.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "143         20  transformer      o_proj  model.layers.20.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "144         20  transformer   gate_proj     model.layers.20.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "145         20  transformer     up_proj       model.layers.20.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "146         20  transformer   down_proj     model.layers.20.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "147         21  transformer      q_proj  model.layers.21.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "148         21  transformer      k_proj  model.layers.21.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "149         21  transformer      v_proj  model.layers.21.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "150         21  transformer      o_proj  model.layers.21.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "151         21  transformer   gate_proj     model.layers.21.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "152         21  transformer     up_proj       model.layers.21.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "153         21  transformer   down_proj     model.layers.21.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "154         22  transformer      q_proj  model.layers.22.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "155         22  transformer      k_proj  model.layers.22.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "156         22  transformer      v_proj  model.layers.22.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "157         22  transformer      o_proj  model.layers.22.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "158         22  transformer   gate_proj     model.layers.22.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "159         22  transformer     up_proj       model.layers.22.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "160         22  transformer   down_proj     model.layers.22.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "161         23  transformer      q_proj  model.layers.23.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "162         23  transformer      k_proj  model.layers.23.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "163         23  transformer      v_proj  model.layers.23.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "164         23  transformer      o_proj  model.layers.23.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "165         23  transformer   gate_proj     model.layers.23.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "166         23  transformer     up_proj       model.layers.23.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "167         23  transformer   down_proj     model.layers.23.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "168         24  transformer      q_proj  model.layers.24.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "169         24  transformer      k_proj  model.layers.24.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "170         24  transformer      v_proj  model.layers.24.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "171         24  transformer      o_proj  model.layers.24.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "172         24  transformer   gate_proj     model.layers.24.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "173         24  transformer     up_proj       model.layers.24.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "174         24  transformer   down_proj     model.layers.24.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "175         25  transformer      q_proj  model.layers.25.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "176         25  transformer      k_proj  model.layers.25.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "177         25  transformer      v_proj  model.layers.25.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "178         25  transformer      o_proj  model.layers.25.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "179         25  transformer   gate_proj     model.layers.25.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "180         25  transformer     up_proj       model.layers.25.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "181         25  transformer   down_proj     model.layers.25.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "182         26  transformer      q_proj  model.layers.26.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "183         26  transformer      k_proj  model.layers.26.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "184         26  transformer      v_proj  model.layers.26.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "185         26  transformer      o_proj  model.layers.26.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "186         26  transformer   gate_proj     model.layers.26.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "187         26  transformer     up_proj       model.layers.26.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "188         26  transformer   down_proj     model.layers.26.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n",
      "189         27  transformer      q_proj  model.layers.27.self_attn.q_proj  (2048, 1024)      2097152            True       1024        2048\n",
      "190         27  transformer      k_proj  model.layers.27.self_attn.k_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "191         27  transformer      v_proj  model.layers.27.self_attn.v_proj  (1024, 1024)      1048576            True       1024        1024\n",
      "192         27  transformer      o_proj  model.layers.27.self_attn.o_proj  (1024, 2048)      2097152            True       2048        1024\n",
      "193         27  transformer   gate_proj     model.layers.27.mlp.gate_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "194         27  transformer     up_proj       model.layers.27.mlp.up_proj  (3072, 1024)      3145728            True       1024        3072\n",
      "195         27  transformer   down_proj     model.layers.27.mlp.down_proj  (1024, 3072)      3145728            True       3072        1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" INSPECTING LoRA ADAPTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the model with LoRA adapters\n",
    "model_path = 'models--Qwen--Qwen3-0.6B-q8'\n",
    "lora_model_path = os.path.join(os.environ['HF_HOME'], 'hub', f\"{model_path}-distilled-final\")\n",
    "\n",
    "# Load model with LoRA adapters\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    lora_model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"\\n LoRA Configuration:\")\n",
    "print(\"-\" * 80)\n",
    "if hasattr(lora_model, 'peft_config'):\n",
    "    for key, config in lora_model.peft_config.items():\n",
    "        print(f\"\\nAdapter: {key}\")\n",
    "        print(f\"  Rank (r): {config.r}\")\n",
    "        print(f\"  Alpha: {config.lora_alpha}\")\n",
    "        print(f\"  Dropout: {config.lora_dropout}\")\n",
    "        print(f\"  Target modules: {config.target_modules}\")\n",
    "        print(f\"  Task type: {config.task_type}\")\n",
    "\n",
    "# Get all LoRA parameters\n",
    "def extract_lora_params(model):\n",
    "    \"\"\"Extract all LoRA adapter parameters\"\"\"\n",
    "    lora_params = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name.lower():\n",
    "            lora_params[name] = {\n",
    "                'shape': tuple(param.shape),\n",
    "                'num_params': param.numel(),\n",
    "                'mean': param.data.float().mean().item(),\n",
    "                'std': param.data.float().std().item(),\n",
    "                'min': param.data.float().min().item(),\n",
    "                'max': param.data.float().max().item(),\n",
    "                'norm': torch.norm(param.data.float()).item(),\n",
    "                'data': param.data.cpu().float()\n",
    "            }\n",
    "    \n",
    "    return lora_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" EXTRACTING LoRA PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lora_params = extract_lora_params(lora_model)\n",
    "\n",
    "print(f\"\\n Found {len(lora_params)} LoRA parameter tensors\")\n",
    "print(f\" Total LoRA parameters: {sum(p['num_params'] for p in lora_params.values()):,}\")\n",
    "\n",
    "# Create detailed DataFrame\n",
    "lora_data = []\n",
    "for name, stats in lora_params.items():\n",
    "    # Parse layer info\n",
    "    parts = name.split('.')\n",
    "    layer_num = None\n",
    "    module_type = None\n",
    "    lora_type = None\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if part.isdigit():\n",
    "            layer_num = int(part)\n",
    "        if 'lora_A' in part or 'lora_B' in part:\n",
    "            lora_type = part\n",
    "        if any(m in part for m in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):\n",
    "            module_type = part\n",
    "    \n",
    "    lora_data.append({\n",
    "        'name': name,\n",
    "        'layer_num': layer_num,\n",
    "        'module_type': module_type,\n",
    "        'lora_type': lora_type,\n",
    "        'shape': str(stats['shape']),\n",
    "        'num_params': stats['num_params'],\n",
    "        'mean': stats['mean'],\n",
    "        'std': stats['std'],\n",
    "        'min': stats['min'],\n",
    "        'max': stats['max'],\n",
    "        'norm': stats['norm']\n",
    "    })\n",
    "\n",
    "df_lora = pd.DataFrame(lora_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" LoRA STATISTICS BY MODULE TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "module_stats = df_lora.groupby('module_type').agg({\n",
    "    'num_params': ['sum', 'count'],\n",
    "    'norm': ['mean', 'std', 'min', 'max'],\n",
    "    'std': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(module_stats)\n",
    "\n",
    "# Detailed layer inspection with shapes\n",
    "\n",
    "def inspect_model_layers(model, show_details=True):\n",
    "    \"\"\"Inspect all model layers with their shapes and parameter counts\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\" LAYER-BY-LAYER ANALYSIS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    total_params = 0\n",
    "    layer_info = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules only\n",
    "            param_count = sum(p.numel() for p in module.parameters())\n",
    "            if param_count > 0:\n",
    "                # Get shapes of all parameters in this module\n",
    "                param_shapes = []\n",
    "                for param_name, param in module.named_parameters():\n",
    "                    param_shapes.append(f\"{param_name}: {tuple(param.shape)}\")\n",
    "                \n",
    "                layer_info.append({\n",
    "                    'name': name,\n",
    "                    'type': type(module).__name__,\n",
    "                    'param_count': param_count,\n",
    "                    'param_shapes': param_shapes\n",
    "                })\n",
    "                total_params += param_count\n",
    "                \n",
    "                if show_details:\n",
    "                    print(f\"\\n {name}\")\n",
    "                    print(f\"   Type: {type(module).__name__}\")\n",
    "                    print(f\"   Parameters: {param_count:,}\")\n",
    "                    for shape_info in param_shapes:\n",
    "                        print(f\"   Shape: {shape_info}\")\n",
    "    \n",
    "    print(f\"\\n Summary:\")\n",
    "    print(f\"   Total layers with parameters: {len(layer_info)}\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "# Inspect the model\n",
    "layer_details = inspect_model_layers(lora_model, show_details=True)\n",
    "\n",
    "def create_model_summary(model, target_modules):\n",
    "    \"\"\"Create a comprehensive summary DataFrame\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    # Get basic model info\n",
    "    config = model.config\n",
    "    \n",
    "    # Analyze each transformer layer\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        layer_data = {\n",
    "            'layer_idx': i,\n",
    "            'layer_type': 'transformer'\n",
    "        }\n",
    "        \n",
    "        # Attention projections\n",
    "        if hasattr(layer, 'self_attn'):\n",
    "            attn = layer.self_attn\n",
    "            for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:\n",
    "                if hasattr(attn, proj_name):\n",
    "                    proj = getattr(attn, proj_name)\n",
    "                    summary_data.append({\n",
    "                        **layer_data,\n",
    "                        'module_name': proj_name,\n",
    "                        'full_path': f'model.layers.{i}.self_attn.{proj_name}',\n",
    "                        'weight_shape': str(tuple(proj.weight.shape)),\n",
    "                        'param_count': proj.weight.numel(),\n",
    "                        'is_lora_target': proj_name in target_modules,\n",
    "                        'input_dim': proj.weight.shape[1],\n",
    "                        'output_dim': proj.weight.shape[0]\n",
    "                    })\n",
    "        \n",
    "        # MLP projections\n",
    "        if hasattr(layer, 'mlp'):\n",
    "            mlp = layer.mlp\n",
    "            for proj_name in ['gate_proj', 'up_proj', 'down_proj']:\n",
    "                if hasattr(mlp, proj_name):\n",
    "                    proj = getattr(mlp, proj_name)\n",
    "                    summary_data.append({\n",
    "                        **layer_data,\n",
    "                        'module_name': proj_name,\n",
    "                        'full_path': f'model.layers.{i}.mlp.{proj_name}',\n",
    "                        'weight_shape': str(tuple(proj.weight.shape)),\n",
    "                        'param_count': proj.weight.numel(),\n",
    "                        'is_lora_target': proj_name in target_modules,\n",
    "                        'input_dim': proj.weight.shape[1],\n",
    "                        'output_dim': proj.weight.shape[0]\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\" MODEL SUMMARY TABLE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nTotal transformer layers: {len(model.model.layers)}\")\n",
    "    print(f\"LoRA target modules: {target_modules}\")\n",
    "    \n",
    "    # Group by module type\n",
    "    module_summary = df.groupby(['module_name', 'is_lora_target']).agg({\n",
    "        'param_count': ['count', 'sum'],\n",
    "        'input_dim': 'first',\n",
    "        'output_dim': 'first'\n",
    "    }).round(0)\n",
    "    \n",
    "    print(\"\\n Module Type Summary:\")\n",
    "    print(module_summary)\n",
    "    \n",
    "    # LoRA parameter calculation\n",
    "    lora_targets = df[df['is_lora_target'] == True]\n",
    "    print(f\"\\n LoRA Target Analysis:\")\n",
    "    print(f\"   Total LoRA target modules: {len(lora_targets)}\")\n",
    "    \n",
    "    # Calculate LoRA parameters (assuming r=16)\n",
    "    r = 16\n",
    "    total_lora_params = 0\n",
    "    for _, row in lora_targets.iterrows():\n",
    "        lora_params = (row['input_dim'] + row['output_dim']) * r\n",
    "        total_lora_params += lora_params\n",
    "    \n",
    "    print(f\"   Estimated LoRA parameters (r={r}): {total_lora_params:,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary_df = create_model_summary(lora_model, target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "print(summary_df.to_string())\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6694bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Testing TRAINED Student Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The app is fantastic! I love using it every day.... -> {\"classification\":\"FEATURE_REQUEST\",\"tone\":\"POSITIVE\",\"alert\":\"\"}  SUCCESS. well done!\n",
      "I'm having trouble logging in. Please assist.... -> {\"classification\":\"BUG\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}  SUCCESS. well done!\n",
      "The new update is great, but I found a bug.... -> {\"classification\":\"BUG\",\"tone\":\"POSITIVE\",\"alert\":\"\"}  SUCCESS. well done!\n",
      "Can you help me with my account issues?... -> {\"classification\":\"INQUIRY\",\"tone\":\"NEUTRAL\",\"alert\":\"\"}  SUCCESS. well done!\n",
      "THIS IS UNACCEPTABLE! I DEMAND A FIX NOW!... -> {\"classification\":\"BUG\",\"tone\":\"ANGRY\",\"alert\":\"\"}  SUCCESS. well done!\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"Remove invisible characters and normalize Unicode\"\"\"\n",
    "    # Normalize and decompose\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    # Keep only printable ASCII letters, numbers, and spaces\n",
    "    s = ''.join(char for char in s if unicodedata.category(char) not in ['Mn', 'Cf', 'Cc', 'Cs', 'Co'])\n",
    "    # Recompose\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    return s.strip()\n",
    "\n",
    "print(\"---  Testing TRAINED Student Model ---\")\n",
    "\n",
    "# distilled model with LoRA adapters\n",
    "tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.path.join(os.environ['HF_HOME'], 'hub', f\"{model_id}-distilled-final\"),\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tickets = [{\"The app is fantastic! I love using it every day.\"},\n",
    "           {\"I'm having trouble logging in. Please assist.\"},\n",
    "           {\"The new update is great, but I found a bug.\"},\n",
    "           {\"Can you help me with my account issues?\"},\n",
    "           {\"THIS IS UNACCEPTABLE! I DEMAND A FIX NOW!\"}]\n",
    "\n",
    "for idx, ticket_text in enumerate(tickets):\n",
    "    prompt_messages = create_prompt_messages(list(ticket_text)[0])\n",
    "    tuned_model_response = generate_response(tuned_model, prompt_messages)\n",
    "    result = f\"{str(list(ticket_text)[0])[:100]}... -> {tuned_model_response}\"\n",
    "    try:\n",
    "        rs = json.loads(tuned_model_response)\n",
    "        if normalize_string(rs[\"classification\"]) not in [\"BUG\", \"FEATURE_REQUEST\", \"INQUIRY\"]:\n",
    "            result += \"  invalid classification\"\n",
    "        if normalize_string(rs[\"tone\"]) not in [\"POSITIVE\", \"NEUTRAL\", \"ANGRY\"]:\n",
    "            result += \"  invalid tone\"\n",
    "        if normalize_string(rs[\"alert\"]) not in [\"\", \"\", \"\"]:\n",
    "            result += \"  invalid alert\"\n",
    "        if normalize_string(rs[\"alert\"]) != (\"\" if normalize_string(rs[\"tone\"]) == \"POSITIVE\" else \"\" if normalize_string(rs[\"tone\"]) == \"NEUTRAL\" else \"\"):\n",
    "            result += \"  alert does not match tone\"\n",
    "        if (\n",
    "            normalize_string(rs[\"classification\"]) in [\"BUG\", \"FEATURE_REQUEST\", \"INQUIRY\"]\n",
    "            and normalize_string(rs[\"tone\"]) in [\"POSITIVE\", \"NEUTRAL\", \"ANGRY\"]\n",
    "            and normalize_string(rs[\"alert\"]) in [\"\", \"\", \"\"]\n",
    "            and normalize_string(rs[\"alert\"]) == (\"\" if normalize_string(rs[\"tone\"]) == \"POSITIVE\" else \"\" if normalize_string(rs[\"tone\"]) == \"NEUTRAL\" else \"\")\n",
    "        ):\n",
    "            result += \"  SUCCESS. well done!\"\n",
    "        else:\n",
    "            result += \"  FAILED. JSON fields have invalid values.\"\n",
    "    except json.JSONDecodeError:\n",
    "        result += \"  FAILED. Not valid JSON.\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54bcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Response from untrained model ---\n",
      "LLM distillation   \n",
      "A whisper of knowledge,  \n",
      "A shadow of wisdom.\n",
      "\n",
      "\n",
      "--- Response from trained model ---\n",
      "Llama's wisdom streams,  \n",
      "A whisper through data's haze.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write an haiku on LLM distillation\"\n",
    "print(\"--- Response from untrained model ---\")\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "response = generate_response(model, messages)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\\n--- Response from trained model ---\")\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "response = generate_response(tuned_model, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "119ebd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Comparing Base vs Trained Model on Emergent Skills ---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Test 1: What is the customer sentiment about the app? 'The app keeps crashing!'\n",
      "================================================================================\n",
      "\n",
      " Base Model Response:\n",
      "The customer sentiment about the app is that it \"keeps crashing.\"\n",
      "\n",
      " Trained Model Response:\n",
      "The sentiment expressed by the customer is **negative**.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Test 2: What's the sentiment here: 'I've been waiting for support for 3 days now.'\n",
      "================================================================================\n",
      "\n",
      " Base Model Response:\n",
      "The sentiment here is **positive**. The phrase \"I've been waiting for support for 3 days now\" suggests that the speaker is currently in need of assistance and is actively seeking it. The duration of waiting (3 days) adds to the urgency, which reinforces a positive tone.\n",
      "\n",
      " Trained Model Response:\n",
      "The sentiment here is **negative**.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Test 3: What emoji best represents this message: 'This is unacceptable!' Choose:   \n",
      "================================================================================\n",
      "\n",
      " Base Model Response:\n",
      "The emoji that best represents the message \"This is unacceptable!\" is ****.\n",
      "\n",
      " Trained Model Response:\n",
      "The emoji that best represents the message \"This is unacceptable!\" is:\n",
      "\n",
      "****\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Test 4: A customer wrote: 'Can you add a dark mode?' What kind of ticket request is this?\n",
      "================================================================================\n",
      "\n",
      " Base Model Response:\n",
      "The customer is requesting a **dark mode** for their application. This is a **ticket request** related to the application's theme or configuration.\n",
      "\n",
      " Trained Model Response:\n",
      "The customer is asking for a **dark mode**. This is a **feature request**.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test prompts to reveal emergent behaviors from distillation training\n",
    "test_prompts = [    \n",
    "    #  Related but broader analytical task\n",
    "    \"What is the customer sentiment about the app? 'The app keeps crashing!'\",\n",
    "    \n",
    "    # Implicit tone detection (no explicit anger markers)\n",
    "    \"What's the sentiment here: 'I've been waiting for support for 3 days now.'\",\n",
    "\n",
    "    # Tests emoji understanding (learned from alert field)\n",
    "    \"What emoji best represents this message: 'This is unacceptable!' Choose:   \",\n",
    "\n",
    "    # Classification without explicit request\n",
    "    \"A customer wrote: 'Can you add a dark mode?' What kind of ticket request is this?\",\n",
    "]\n",
    "\n",
    "print(\"---  Comparing Base vs Trained Model on Emergent Skills ---\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: {prompt}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    print(\"\\n Base Model Response:\")\n",
    "    base_response = generate_response(model, messages)\n",
    "    print(base_response)\n",
    "    \n",
    "    print(\"\\n Trained Model Response:\")\n",
    "    trained_response = generate_response(tuned_model, messages)\n",
    "    print(trained_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
